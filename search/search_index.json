{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#neural-ordinary-differential-equations","title":"Neural Ordinary Differential Equations","text":""},{"location":"#description","title":"Description","text":"<p>This project will walk through solving Ordinary Differential Equations (ODEs) within an autograd framework (PyTorch), utilising the inbuilt tools to effectively differentiate the parameters and solutions of them, and finally incorporating Neural Networks to demonstrate how to effectively learn dynamics from data.</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<ul> <li>Writing a python module geared towards research that can be used by others</li> <li>How to take research/theoretical concepts and turn them into code</li> <li>How numerical integration works</li> <li>How neural networks work</li> <li>How neural networks and numerical integration can be combined</li> </ul> Task Time Reading 8 hours Running Notebooks 4-12 hours Practising with Own Dynamics 4+ hours"},{"location":"#requirements","title":"Requirements","text":""},{"location":"#academic","title":"Academic","text":"<ul> <li>Knowledge of calculus, specifically in derivatives, integrals and limits.</li> <li>A rudimentary understanding of how floating-point/finite precision algebra works on computers.</li> <li>Basic python programming skills, knowledge of iteration, branching, etc.</li> <li>A bref understanding of vectorised computation. How CPUs/GPUs process different data in parallel</li> </ul>"},{"location":"#system","title":"System","text":"<ul> <li>Python 3.10 or newer</li> <li>Poetry</li> <li>CUDA-capable GPU (for GPU training of networks)</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#setting-up-python-environment","title":"Setting up Python Environment","text":"<ol> <li>Install Python 3.10 or above</li> <li>Install <code>pipx</code> following the instructions here: https://pipx.pypa.io/stable/installation/</li> <li>Install Poetry using the instructions here: https://python-poetry.org/docs/#installing-with-pipx</li> <li>Once Poetry is set up and usable, go to the root directory of this repository and run <code>poetry lock</code> followed by <code>poetry install</code>. This should install the project dependencies into a Poetry managed virtual environment.</li> <li>To run the code, use:</li> <li><code>poetry run [SCRIPT NAME]</code> to run any script in the repository.</li> <li><code>poetry shell</code> to enter a shell with the appropriate <code>python</code> and dependencies set up. From there you can use <code>python [SCRIPT NAME]</code> to run any script.</li> <li><code>poetry run jupyter notebook</code> to start a jupyter notebook in the repository environment from which the notebooks can be run.</li> <li>If using the code as a dependency (i.e. as a module that is imported in your own script), then you'll need to run <code>pip install .</code> which will install the locally available package into the current python environment.</li> </ol>"},{"location":"#how-to-use-this-repository","title":"How to Use this Repository","text":"<ol> <li>Start by reading <code>Chapter 1 - Introduction to Ordinary Differential Equations (ODEs)</code> and refer to the introductory notebooks for the implementation of the concepts.</li> <li>Study the jupyter notebooks on the implementations in further detail: [Fill with notebook names for introductory material]</li> <li>Study Chapter 2 for a walk-through of the module structure</li> <li>Study jupyter notebooks for training scripts as well as visualisation of results</li> </ol>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 neuralode\n\u2502   \u251c\u2500\u2500 integrators\n\u2502   \u251c\u2500\u2500 models\n\u2502   \u251c\u2500\u2500 plot\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 notebooks\n|   \u251c\u2500\u2500 01-simple-integration-routines.ipynb\n|   \u251c\u2500\u2500 02-arbitrary-adaptive-tableaus.ipynb\n|   \u251c\u2500\u2500 03-the-adjoint-method.ipynb\n|   \u251c\u2500\u2500 04-driven-harmonic-oscillator.ipynb\n|   \u2514\u2500\u2500 05-the-inverted-pendulum.ipynb\n\u251c\u2500\u2500 docs\n|   \u2514\u2500\u2500 01-introduction.md\n|   ...\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>"},{"location":"01-introduction/","title":"Introduction","text":""},{"location":"01-introduction/#overview-of-project","title":"Overview of Project","text":"<p>This ReCoDE Project on Neural Ordinary Differential Equations will walk you through the theoretical basics of Ordinary Differential Equations (ODE), specifically in the context of numerical solvers, all the way to Neural ODEs. The topics we'll cover will include answering what ODEs are, how to implement their numerical solution in research code, how to utilise an autograd equipped library to propagate gradients through a integration routine, and, most importantly, how one can then subsequently use these developed tools to tackle problems with Neural Networks. While it looks like a lot of ground to cover, a lot of the mathematical machinery should be familiar to you, and hopefully through this project you will learn how to structure a distributable python research module, how to take theoretical ideas like backpropagation through an integrator and apply it to Neural Networks and most importantly, you will have fun.</p>"},{"location":"01-introduction/#neural-ordinary-differential-equations","title":"Neural Ordinary Differential Equations","text":"<p>Neural ODEs combines Neural Networks with ODEs. Broadly, NNs are a class of models where a (usually statistical) mapping between an input space and an output space can be learned. Differential Equations (DE) relate a function and its derivative(s), and ODEs are a subset of DEs where there is only one independent variable.</p> <p>In less vague terms, Neural Networks (NNs) model functions using a variety of nonlinear transformations, a simple feed-forward model uses affine projections combined with nonlinear transformations whereas a network that processes images may use convolutions. An example of this is a \"Digit Classifier\" that learns the mapping from images of digits (the input space) to the numeric value of the digit (the output space).</p> <p>The input and output space could also be in relation to a sequence of some kind. For example, the input space could be a text fragement and the output space could be the distribution of probable letters that can succeed the input. Given an incomplete sentence \"The dog jumps ove\", a neural network could learn the mapping and predict the letter \"r\" as the output giving \"The dog jumps over\".</p> <p>A more dynamical example is a falling ball. A ball dropped from some height will accelerate towards the ground until impact. Perhaps what we can measure is just the height of the ball at each point in time and we drop many balls from different heights with different masses, maybe even different sizes to create a \"Ball-Drop\" dataset. This dataset would have some input labels (the height from which the balls is dropped, the mass of the ball, the size of the ball) and output labels (the time of impact). We could train a neural network to predict the time of impact directly, but we could also ask the exact state of the ball at any given time between the drop and the impact? We'd need a huge dataset to get a dense sampling, and we would have to hope that the neural network gives reasonable predictions for times that we don't have the data for.</p> <p>Instead, what if we learned the dynamics itself, the mapping between the current state and some future state. We could iteratively estimate the state of the ball by first putting in the initial state, getting the next state (prescribed by the data sampling) and then re-inputting this next state back into the network until the network says the ball hit the ground. But we're still restricting to our data sampling, if we have sampled at 0.1s increments, then our network won't know how to predict at 0.05s increments because there is no data. Ideally we'd be able to sample continuously, but in this scheme each finer increment requires increasingly more data to the point where simply storing this data becomes impractical.</p> <p>This is where DEs come in. DEs can be used to model the dynamics of some kind of system, this could be chemical, physical, etc., but ultimately the DEs we're interested in describe the time-evolution of a time-dependent dynamical system in a continuous fashion. Hence, the goal with Neural ODEs is to learn this dynamical mapping from data and perhaps even control it. We will focus on ODEs since they are the simplest DEs that can be dealt in a time-continuous fashion and do not have the complexity associated with other DEs such as Partial DEs, Stochastic DEs and Differential-Algebraic Equations (DEs but with algebraic constraints).</p>"},{"location":"01-introduction/#what-are-odes","title":"What are ODEs?","text":"<p>ODEs are a class of DEs where the flow is dependent on only a single independent variable<sup>1</sup>. Formally, an ODE is a differential equation of the following form<sup>2</sup>:  <sup>3</sup> This expression looks somewhat daunting given the number of terms so let's simplify the notation slightly. First, let  where we implicitly assume that  is the independent variable. Let's also restrict our attention to explicit ODEs given that many systems commonly take this form and we'll also restrict our attention to first-order systems as higher-order explicit ODEs can be rewritten as first-order systems:  In this form, we can more clearly see what an ODE tells us, if we consider  as a point in , then the ODE gives the flow map . Were we to follow this map we would be given a trajectory in the  space and it is this trajectory that we aim to compute using the methods described in this section.</p>"},{"location":"01-introduction/#example-system-of-a-falling-ball","title":"Example System of a Falling Ball","text":"<p>For example, the equation for a falling ball takes the form  where  is the height of the ball and  is standard gravitational acceleration. We can rewrite this as a first-order equation by introducing velocity :  </p> <p>We can write this more compactly as:  </p> <p>And if we write   and   we can see how this notation is equivalent.</p> <p>You'll note that this information alone is not enough to deduce the values of  and , because we lack boundary conditions, and, more specifically in this case, initial values. The height of the ball will differ depending on if the ball was thrown at the ground and what height it was let go at, namely the values  and .</p>"},{"location":"01-introduction/#initial-value-problems-ivps","title":"Initial Value Problems (IVPs)","text":"<p>Initial Value Problems are a class of problems where the values of the differential equation are defined at , at the \"initiation\" of the system. There are a separate class of problems called Boundary Value Problems (BVPs) where the values of the system are defined at both  and  for some  (if  is the independent variable), but these are not the systems of interest to us as they are solved using methods different to the ones found here<sup>5</sup>. In this work, we are seeking to solve IVPs of the form  </p>"},{"location":"01-introduction/#numerical-integration-techniques","title":"Numerical Integration Techniques","text":"<p>Numerical integration can refer to many different techniques ranging from the Trapezoid Rule (a Quadrature method) to Monte-Carlo methods to Runge-Kutta methods, but broadly, these are all methods to solve integrals of different kinds. Quadrature techniques compute definite integrals where the Boundary Conditions are defined exactly (ie. the values of the function and/or derivative on the integration boundaries are known) by subdividing the domain of integration; unlike ODEs, these integrals are not restricted to one continous dimension. Monte-Carlo methods compute multi-dimensional integrals using a probabilistic approach and solve the issues of poor scaling experienced in Quadrature methods which subdivide -dimensions into  parts leading to  points of evaluation. Here we focus solely on ODEs and IVPs.</p> <p>Many methods exist for numerically integrating DEs, depending on the type some tools are more suitable than others, but we will entirely restrict ourselves to IVPs. First, let's define some more notation. Let  be the interval of integration such that we restrict our interest to only a subset of the values of  <sup>4</sup> where our initial values are defined as . With this machinery setup, we can look at the most simple integration method: The (Explicit/Forward) Euler Method.</p>"},{"location":"01-introduction/#the-explicitforward-euler-method","title":"The (Explicit/Forward) Euler Method","text":"<p>If we recall that , then we could consider rewriting the differentials as:  Now  represents an infinitesimal increment of time and given calculus we know that  represents the infinitesimal change in , so if we could somehow start with  and sum each infinitesimal change, we would be able to estimate the trajectory . But this unfortunately doesn't work because computers do not handle infinities and infinitesimals in a way that would allow us to compute the above sum. So maybe if we relax the equation a little bit<sup>6</sup>, and suppose that actually the infinitesimal change in time is no longer infinitesimal but some measurable delta and make the transformation , then perhaps we could sum the measurable changes  to obtain the trajectory . To simplify the notation, we'll assume that  and this will split our interval into evenly sized chunks in which we can obtain the discrete mapping of  where  and  is our discrete approximation to  <sup>7</sup><sup>15</sup> So now we have defined a very simple integrator where given some initial values and an interval of integration, we can, in principle compute the trajectory of a system. Writing this method out formally, we have that  You will note that the next timestep depends only on the previous timestep in this method and that is why this is called the Explicit or Forward Euler Method. While this method works, an error analysis using Taylor series shows that the error in this approximation is a linear function of  (ie. an  algorithm where ). A computer has finite precision, and so if a step size of  gives the solution to a precision of  then obtaining a solution with a precision that is machine precision ( for 32-bit floats and  for 64-bit floats), then you would require a step size that is 6 orders of magnitude smaller (or 14 orders for 64-bit doubles) to get to such precision. This translates to  more steps and correspondingly,  times more computation. So while Euler's Method is easy to understand from a variety of perspectives, it has terrible scaling with the step size and while it may handle some systems well, it is not a good general purpose integrator for almost any system. Aside from errors, these methods also have stability considerations and many systems can show instability when the approximation is too inaccurate. More formally, instability here refers to a tendency for the approximation and the ideal trajectory diverging<sup>8</sup> Euler's Method is not the most stable method especially for systems which can be described as \"stiff\". While there is no formal mathematical definition that accounts for all stiff systems, the general behaviour of a stiff system is that a method with a finite region of stability takes excessively small steps relative to the smoothness of the underlying solution<sup>17</sup>.</p>"},{"location":"01-introduction/#explicit-vs-implicit-methods","title":"Explicit vs Implicit Methods","text":"<p>You'll recall that the Euler Method above was labelled as being Explicit/Forward, and this was to highlight that there is a variation of Euler's Method where  And this defines an algebraic equation given a general  where the solution  must be found iteratively. This is commonly referred to as the Implicit/Backward Euler Method as the solution  is implicitly defined. This method, while having the same error properties, generally exhibits better stability<sup>16</sup> especially for stiff systems, but requires significantly more computation to solve.</p>"},{"location":"01-introduction/#runge-kutta-methods-and-butcher-tableaus","title":"Runge-Kutta Methods and Butcher Tableaus","text":"<p>While there exist different methods and approaches than the Forward and Backward Euler Methods, such as linear multi-step methods, we will restrict our attention to Runge-Kutta Methods due to their ease of implementation and conceptually straightforward description. Let  be the order of a given Runge-Method and  be the number of steps, the aforementioned Forward Euler method would be described as an order ,  (as there is only one step in the approximation) Explicit Runge-Kutta method. Runge-Kutta methods assume a form where the approximation at each timestep<sup>9</sup>, is given by  where  is henceforth assumed to be in reference to the approximated trajectory (unless stated explicitly), and   For explicit methods , while for implicit methods there is no restriction. Our Explicit Euler Method, in this notation, has only one set of coefficients, namely ,  and . For the Implicit Euler Method, the coefficients are almost the same except now .  As you can see, this is a powerful notation for writing out different integration schemes as it will help appropriately track coefficients when we're writing our own integrators. If we denote the matrix of coefficients , and the vectors of coefficients  and , then we can write a general Runge-Kutta method as follows  referred to as a Butcher Tableau.</p>"},{"location":"01-introduction/#summary","title":"Summary","text":"<p>In summary, we have seen that there are methods of integrating ODEs called Explicit Runge-Kutta methods that approximate the trajectory using local estimates of the state, and we'll be implementing them using their Butcher Tableaus. The goal is to code a pipeline where, starting from data relating to a dynamical system, we can train a neural network to learn the dynamics of a system.</p>"},{"location":"01-introduction/#appendix-1-adaptive-vs-fixed-step","title":"Appendix 1 - Adaptive vs Fixed-Step","text":"<p>Up to this point we've discussed numerical integrators that use a fixed timestep in order to estimate a given trajectory, but let's consider a 1d function where the solution has two timescales, one long and one short such as  where we'll assume  and . The ODE for this system would be  We can see that the solution timescale for the exponential is  whereas the timescale for the oscillation is . If we were to numerically integrate this using a timestep of ; when , we would introduce spurious oscillations in the solution as we'd overestimate the decay in value of , if  then these oscillations would be too small in magnitude and the cosine/sine terms would dominate the solution, and so we would need to choose a timestep in accordance with their oscillation.  </p> <p>Multiple timescales exist in many systems and require careful consideration depending on the system being observed. Ideally, we would resolve all timescales by choosing a \"small enough\" timestep when  is large and \"large enough\" when  is small. The idea of \"small enough\" and \"large enough\" depend strongly on the system in question, and so we must carefully consider how to measure this while integrating a given system.  </p> <p>One idea, commonly used in Runge-Kutta methods, is the introduction of an auxiliary estimate of the trajectory, ideally one with a higher order error than the baseline method used for estimation. Then, assuming that the system is well-resolved (i.e. our timestep hits the \"small/large enough\" criterion), the difference between the two estimates of the trajectory should be similarly \"small\". Thus, one can consider a timestep adaptation strategy that \"controls\" the discrepancy between the two estimates.</p> <p>Consider the following Runge-Kutta method that has both a 4th order and a 5th order estimate of the trajectory:  </p> <p> </p> <p>We see that there are now two rows where before we had one row of  coefficients, the first row is the  () order estimate and the second row is the   () order estimate (where ).<sup>10</sup> <sup>18</sup>.</p> <p>Since the estimate is given by , we can see that the estimated error is given by  where  are the coefficients for the higher order estimate.  </p> <p>A detailed derivation is in the Appendix in Derivation of Error Adaptation, but suffice to say, we can change the timestep based on the error using the equation:</p> <p> </p> <p>where  is the absolute error tolerance and  is the relative error tolerance.</p>"},{"location":"01-introduction/#appendix-2-deriving-the-error-adaptation-equations","title":"Appendix 2 - Deriving the Error Adaptation Equations","text":"<p>The error is for each component of  with a sign that indicates over-/under-estimation relative to the higher-order estimate. Generally, we are concerned with keeping the magnitude of the error small and are not particularly concerned about over-/under-estimation since, if the magnitude is sufficiently small, the sign becomes irrelevant. Noting that the timestep is a scalar quantity, we must derive a scalar measure of the error from this vector estimate. There are several ways of calculating the magnitude of a vector such as the  norm, the  norm and the  norm. By writing these as -norms, we can leave the choice of  until later and first write down how we can adjust the timestep.  </p> <p>Consider a trajectory where a scalar , an error of  is a 1% deviation from the ground truth. If this was an estimate of distance in meters, the magnitude of the error would be on the order of centimeters, a measurable and noteworthy quantity. Now let's consider a change of units from meters to nanometers, our trajectory now becomes . An error of  would now be on the order of one hundredth of a nanometer, for a system where the changes occur on the order of meters, this would be an insignificant change. If our units were kilometers, then  and our error is larger than the quantities we're estimating. If we consider the problem in reverse, targeting an error of  would be unnecessarily precise (except in perhaps rare circumstances) or critical depending on the scale of the problem.  </p> <p>This motivates the introduction of two concepts: absolute tolerance (\\sigma_a) and relative tolerance (\\sigma_r). The absolute tolerance is the maximum deviation we would allow irrespective of the scale of the problem, whereas the relative tolerance is the maximum deviation we would allow normalised to the scale of the problem. <sup>11</sup> </p> <p>Going back to our error estimate, the ideal condition we would like to fulfil is the following (Loosely following the derivation in <sup>19</sup>):</p> <p> </p> <p>where the subscript  denotes the vector components and where we've introduced the scale of the problem through multiplying  by . We can rearrange this equation to obtain the following:  </p> <p> </p> <p>And if we consider that the error  is actually an estimate of the higher-order error term (i.e. the term  in the Taylor expansion of the solution), then we can write the ratio as follows  </p> <p> </p> <p>Where we know that the leading error term is proportional to  and  (which in turn is dependent on the p+1 derivative of the solution). From this we see the relationship between the tolerances we'd like to achieve and the timestep, but there are still unknown terms and an inequality is not easy to solve. </p> <p>Consider now that we change the step by a factor  to , then the error estimate of the step will be:</p> <p> </p> <p>Solving for  (to the leading order term) we get:</p> <p> <sup>12</sup></p> <p>If we assume that the higher order terms, , are similar to the higher order terms of the unaltered timestep , then we can write substitute back our estimate of the error to obtain:</p> <p> </p> <p>Which gives us a way to compute an adjustment to the timestep that obeys our tolerance criteria for each component of our trajectory. In common usage, a safety factor of approximately  is incorporated to get the the stricter inequality:</p> <p> </p> <p>That accounts for any spurious changes in the solution (i.e. mild violations of the equality of the higher order terms between the current step and the new step).</p> <p>Whenever our original error does not satisfy our inequality, we can reject the step and recalculate it more accurately using a timestep  from the following formula. Conversely, whenever the step is accepted, we can use  to increase the step and avoid extra computation in solving the system.</p> <p> </p> <p><sup>13</sup></p> <p>Up to here we have ignored the fact that  is a vector quantity and our inequality is on a per-component basis. Since we cannot take separate steps for each component<sup>14</sup>, we need to make an adjustment that accounts for all our components simultaneously. One way would be to take some kind of average over the different 's for each component such the arithmetic or geometric mean. Another might be to consider the maximum or minimum, but a simple solution is to convert the error into a scalar over all the components:</p> <p> </p> <p>Where we now consider the magnitude of our errors in a vector sense. The p-norm, discussed before, is left up to the developer's choice and can even be exposed as part of the integration interface. A suitable choice would be the -norm for the denominator and the -norm for the numerator as this adjusts the error proportional to the maximum error incurred while considering the overall scale of the problem.</p> <ol> <li> <p>In many systems in physics and chemistry, this is time.\u00a0\u21a9</p> </li> <li> <p>I've used  here since many systems we will look at use time as the independent variable, but this could also be  which usually denotes a spatial dimension meaning that  is some spatially varying function. Mathematically they are equivalent, but I will follow the physics conventions in this project.\u00a0\u21a9</p> </li> <li> <p>This expression encompasses both explicit and implicit ODEs, where it may be the case that  cannot be isolated and thus one cannot find an explicit closed-form expression for it.\u00a0\u21a9</p> </li> <li> <p>Suppose we want to know what a system does in the limit , then a computer cannot numerically represent this interval in a useful fashion. In that case, we can use a suitable change of variables such that  where  and integrate as a function of .\u00a0\u21a9</p> </li> <li> <p>In fact, I've omitted an entire class of problems with mixed boundary conditions where both initial values and boundary values are specified depending on the components of . These are more common in ODE systems derived from Partial Differential Equations (PDE) where, for example, temperature diffusion through a metal plate may occur where one end of the plate is always at a fixed temperature.\u00a0\u21a9</p> </li> <li> <p>And here you should be getting echoes of Riemannian Sums.\u00a0\u21a9</p> </li> <li> <p>If we could somehow take the limit  then we would have an exact solution to our system.\u00a0\u21a9</p> </li> <li> <p>You may ask, how would you measure that if these systems cannot be solved analytically. One method involves linearising some ideal system of equations and looking at how that linear mapping behaves when composed many times while accounting for the error terms introduced by the approximation. In the ideal case, these error terms increase as a linear or sub-linear function of the timestep, but in the unstable case, these may increase in a super-linear or even exponential fashion leading to divergence from the true trajectory.\u00a0\u21a9</p> </li> <li> <p>Timestep will denote steps in  whereas step will denote steps taken to approximate the timestep\u00a0\u21a9</p> </li> <li> <p>The coefficients are normally given as fractions, but for brevity they have been rounded to 4 decimal places or the repeating portion of the decimal has been indicated with a bar.\u00a0\u21a9</p> </li> <li> <p>While we've discussed these tolerances as scalar quantities, one could consider specifying them on a per component basis and we will come back to this at a future point.\u00a0\u21a9</p> </li> <li> <p>This is valid as all quantities are strictly positive and exponentiation is a monotonic function for positive quantities.\u00a0\u21a9</p> </li> <li> <p>It should be noted that there are other methods of choosing the step size adjustment during a numerical integration procedure including estimation of the global error (here we have only considered the local error incurred in taking one step, but not the cumulative error) and the error per unit step.\u00a0\u21a9</p> </li> <li> <p>there are methods that take fractional steps for some quantities, but these are beyond the scope of this work and require significant work/bookkeeping to correctly implement\u00a0\u21a9</p> </li> <li> <p>There are alternative ways of deriving the same result, this is a loose derivation based on intuition, but a more formal approach would be to first take the integral of both sides wrt.  from  to  to obtain  and then approximate the integral on the RHS using known quadrature rules such as the Rectangle Rule.\u00a0\u21a9</p> </li> <li> <p>Butcher, J. C. (John C. (2016). Numerical methods for ordinary differential equations (Third edition.). John Wiley &amp; Sons, Ltd.\u00a0\u21a9</p> </li> <li> <p>Lambert, J. D. (1991). Numerical methods for ordinary differential systems\u202f: the initial value problem. Wiley.\u00a0\u21a9</p> </li> <li> <p>Bogacki, P., &amp; Shampine, L. F. (1996). An efficient Runge-Kutta (4,5) pair. In Computers &amp; Mathematics with Applications (Vol. 32, Issue 6, pp. 15\u201328). Elsevier BV. https://doi.org/10.1016/0898-1221(96)00141-1 \u21a9</p> </li> <li> <p>Shampine, L. F. (2005). Error estimation and control for ODEs. In Journal of Scientific Computing (Vol. 25, Issues 1\u20132, pp. 3\u201316). Springer Science and Business Media LLC. https://doi.org/10.1007/bf02728979 \u21a9</p> </li> </ol>"},{"location":"01-simple-integration-routines/","title":"Simple Integration Routine","text":"In\u00a0[1]: Copied! <pre># For plotting results\nimport matplotlib.pyplot as plt\nimport torch\n</pre> # For plotting results import matplotlib.pyplot as plt import torch In\u00a0[2]: Copied! <pre># For convenience, we define the default tensor device and dtype here\ntorch.set_default_device('cpu')\n# In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training\ntorch.set_default_dtype(torch.float64)\n</pre> # For convenience, we define the default tensor device and dtype here torch.set_default_device('cpu') # In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training torch.set_default_dtype(torch.float64)  In\u00a0[3]: Copied! <pre>def simple_fn(x, t):\n    \"\"\"\n    Derivative of a linear function.\n    :param x: the current state\n    :param t: the current time\n    :return: the gradient at (x,t)\n    \"\"\"\n    # We use torch.ones_like as x may be a multi-dimensional tensor\n    # While PyTorch may correctly handle adding a simple scalar, it is good practice\n    # to use the utilities provided by PyTorch\n    return torch.ones_like(x)\n\ndef simple_fn_solution(initial_x, t):\n    \"\"\"\n    Linear function initial_x+t\n    :param initial_x: the initial state for the linear function\n    :param t: the time at which the solution is desired\n    :return: the state at time t\n    \"\"\"\n    return initial_x + torch.ones_like(initial_x)*t\n</pre> def simple_fn(x, t):     \"\"\"     Derivative of a linear function.     :param x: the current state     :param t: the current time     :return: the gradient at (x,t)     \"\"\"     # We use torch.ones_like as x may be a multi-dimensional tensor     # While PyTorch may correctly handle adding a simple scalar, it is good practice     # to use the utilities provided by PyTorch     return torch.ones_like(x)  def simple_fn_solution(initial_x, t):     \"\"\"     Linear function initial_x+t     :param initial_x: the initial state for the linear function     :param t: the time at which the solution is desired     :return: the state at time t     \"\"\"     return initial_x + torch.ones_like(initial_x)*t In\u00a0[4]: Copied! <pre>x0 = torch.tensor([1.0])\nt0 = torch.tensor(0.0)\nt1 = torch.tensor(1.0)\nN  = 10\ndt = (t1 - t0) / N\n</pre> x0 = torch.tensor([1.0]) t0 = torch.tensor(0.0) t1 = torch.tensor(1.0) N  = 10 dt = (t1 - t0) / N <p>The Euler method is quite simple. At each step, we take the result of the previous step and add to it the product of the timestep and the derivative.</p> <p>$$ x_{n+1} = x_n + \\frac{dx}{dt}(x_n, t_n)\\Delta t \\\\ t_{n+1} = t_n + \\Delta t $$</p> In\u00a0[5]: Copied! <pre># We don't want to modify the pre-existing state variables so we clone them\ncurrent_time = t0.clone()\ncurrent_state = x0.clone()\n\n# We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times\nintermediate_states = [(current_state.clone(), current_time.clone())]\n\n# We integrate as long as the current_time is less than t1, the final time\nwhile torch.any(current_time &lt; t1):\n    current_state = current_state + simple_fn(current_state, current_time)*dt\n    current_time = current_time + dt\n    intermediate_states.append((current_state.clone(), current_time.clone()))\n    \nprint(f\"Final time: {current_time}\")\nprint(f\"Final state: {current_state}\")\n</pre> # We don't want to modify the pre-existing state variables so we clone them current_time = t0.clone() current_state = x0.clone()  # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times intermediate_states = [(current_state.clone(), current_time.clone())]  # We integrate as long as the current_time is less than t1, the final time while torch.any(current_time &lt; t1):     current_state = current_state + simple_fn(current_state, current_time)*dt     current_time = current_time + dt     intermediate_states.append((current_state.clone(), current_time.clone()))      print(f\"Final time: {current_time}\") print(f\"Final state: {current_state}\") <pre>Final time: 1.0999999999999999\nFinal state: tensor([2.1000])\n</pre> <p>Now you'll notice that the final time exceeds <code>t1</code>, why did this happen?</p> <p>If you recall the discussion on finite precision in Introduction to ODEs, what has happened is that the accumulation of time values is inexact. The second-to-last timestep is at a time that is near indistinguishable from the final timestep, but due to the imprecision, the value is less than <code>t1</code>:</p> In\u00a0[6]: Copied! <pre>print(f\"Second-to-last time: {intermediate_states[-2][1].item():.16f}\")\nprint(f\"Target final time: {t1.item():.16f}\")\n</pre> print(f\"Second-to-last time: {intermediate_states[-2][1].item():.16f}\") print(f\"Target final time: {t1.item():.16f}\") <pre>Second-to-last time: 0.9999999999999999\nTarget final time: 1.0000000000000000\n</pre> <p>Due to this small difference, we end up taking one step too many and exceeding the final time.</p> <p>To resolve this, we need to account for the difference. One solution is to check if we are within one timestep of the final time and then adjust the timestep for the final step. Alternatively, we could treat these time as equivalent by simply checking if they are close to each other within some tolerance. A final method would be to track the loss of precision using a complex summing routine like Kahan Summation.</p> <p>The first leads to variation of timesteps that would not necessarily solve the problem if the difference is too small for the finite precision arithmetic (we end up with timesteps so small that adding them to the current time leads to no change).</p> <p>The second can lead to issues if we need to continue integration and find that we are not exactly at the time that we intended to be. This will be exacerbated by lower precision floating point types such as <code>float32</code> and <code>float16</code>.</p> <p>The third is the most mathematically precise, but may not always solve the problem as adding too small a value can change nothing of the end result.</p> <p>A combination of the first and third will lead to the most precise results and thus that will be our eventual implementation. For now, we will use the first method and avoid complexity.</p> <p>Rewriting our loop we have:</p> In\u00a0[7]: Copied! <pre># We don't want to modify the pre-existing state variables so we clone them\ncurrent_time = t0.clone()\ncurrent_state = x0.clone()\n\n# We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times\nintermediate_states = [(current_state.clone(), current_time.clone())]\n\n# We integrate as long as the current_time+dt is less than t1\n# Essentially, we want to stop the integration if taking another step\n# would lead to exceeding the final time\nwhile torch.any((current_time + dt) &lt; t1):\n    current_state = current_state + simple_fn(current_state, current_time)*dt\n    current_time = current_time + dt\n    intermediate_states.append((current_state.clone(), current_time.clone()))\ncurrent_state = current_state + simple_fn(current_state, current_time)*(t1 - current_time)\ncurrent_time = current_time + (t1 - current_time)\nintermediate_states.append((current_state.clone(), current_time.clone()))\n    \nprint(f\"Final time: {current_time}\")\nprint(f\"Final state: {current_state}\")\n</pre> # We don't want to modify the pre-existing state variables so we clone them current_time = t0.clone() current_state = x0.clone()  # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times intermediate_states = [(current_state.clone(), current_time.clone())]  # We integrate as long as the current_time+dt is less than t1 # Essentially, we want to stop the integration if taking another step # would lead to exceeding the final time while torch.any((current_time + dt) &lt; t1):     current_state = current_state + simple_fn(current_state, current_time)*dt     current_time = current_time + dt     intermediate_states.append((current_state.clone(), current_time.clone())) current_state = current_state + simple_fn(current_state, current_time)*(t1 - current_time) current_time = current_time + (t1 - current_time) intermediate_states.append((current_state.clone(), current_time.clone()))      print(f\"Final time: {current_time}\") print(f\"Final state: {current_state}\") <pre>Final time: 1.0\nFinal state: tensor([2.0000])\n</pre> <p>Excellent, that seems to have resolved our issue in the expected fashion!</p> In\u00a0[8]: Copied! <pre>plot_times = [i[1].cpu().item() for i in intermediate_states]\nplot_states = [i[0].cpu().item() for i in intermediate_states]\n</pre> plot_times = [i[1].cpu().item() for i in intermediate_states] plot_states = [i[0].cpu().item() for i in intermediate_states] <p>Then compute the ideal solution states at each time for comparison:</p> In\u00a0[9]: Copied! <pre>solution_states = [simple_fn_solution(x0, i[1]).cpu().item() for i in intermediate_states]\n</pre> solution_states = [simple_fn_solution(x0, i[1]).cpu().item() for i in intermediate_states] In\u00a0[10]: Copied! <pre>fig = plt.figure(figsize=(12, 3), tight_layout=True)\nax = fig.add_subplot(111)\nax.plot(plot_times, solution_states, label='Ideal Solution')\nax.plot(plot_times, plot_states, label='Euler Method', linestyle='--')\nax.set_xlabel(r'Time $(\\mathrm{s})$')\nax.set_ylabel(r'$x(t)$')\nax.legend()\n</pre> fig = plt.figure(figsize=(12, 3), tight_layout=True) ax = fig.add_subplot(111) ax.plot(plot_times, solution_states, label='Ideal Solution') ax.plot(plot_times, plot_states, label='Euler Method', linestyle='--') ax.set_xlabel(r'Time $(\\mathrm{s})$') ax.set_ylabel(r'$x(t)$') ax.legend() Out[10]: <pre>&lt;matplotlib.legend.Legend at 0x20c712f5160&gt;</pre> <p>Amazing, the lines overlap exactly! Where did all the imprecision of the Euler method go?</p> <p>Well, the Euler method is exact for a constant derivative and therefore, regardless of step size, the solutions will overlap exactly (barring deviations due to floating-point imprecision). We can visualise this more clearly by looking at the error between the ideal and the euler solution:</p> In\u00a0[11]: Copied! <pre>fig = plt.figure(figsize=(12, 5), tight_layout=True)\nax = fig.add_subplot(211)\nax.plot(plot_times, solution_states, label='Ideal Solution')\nax.plot(plot_times, plot_states, label='Euler Method', linestyle='--')\nax.set_ylabel(r'$x(t)$')\nax.legend()\nax = fig.add_subplot(212, sharex=ax)\nax.plot(plot_times, [(i - j) for i,j in zip(solution_states, plot_states)], label='Residual')\nax.set_xlabel(r'Time $(\\mathrm{s})$')\nax.set_ylabel(r'$\\Delta x$')\nax.legend()\n</pre> fig = plt.figure(figsize=(12, 5), tight_layout=True) ax = fig.add_subplot(211) ax.plot(plot_times, solution_states, label='Ideal Solution') ax.plot(plot_times, plot_states, label='Euler Method', linestyle='--') ax.set_ylabel(r'$x(t)$') ax.legend() ax = fig.add_subplot(212, sharex=ax) ax.plot(plot_times, [(i - j) for i,j in zip(solution_states, plot_states)], label='Residual') ax.set_xlabel(r'Time $(\\mathrm{s})$') ax.set_ylabel(r'$\\Delta x$') ax.legend() Out[11]: <pre>&lt;matplotlib.legend.Legend at 0x20c72a7eb40&gt;</pre> In\u00a0[19]: Copied! <pre>def integrate_euler(fn, initial_state, initial_time, final_time, timestep):\n    \"\"\"\n    A general integration routine for solving an Initial Value Problem\n    using the Euler Method\n    :param fn: the function to be integrated\n    :param initial_state: the initial state to integrate from\n    :param initial_time: the initial time to integrate from\n    :param final_time: the final time to integrate to\n    :param timestep: the time increments to integrate with \n    :return: a tuple of ((the final state, the final time), the intermediates states [list]) \n    \"\"\"\n    # The names for the variables have been shortened for concision, and\n    # to avoid overlap with variables in the outer scope\n    # I have also left the annotations as they are invaluable for tracking the method. \n    c_time = initial_time.clone()\n    c_state = initial_state.clone()\n    \n    # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times\n    i_states = [(c_state.clone(), c_time.clone())]\n    \n    # We integrate as long as the current_time+dt is less than t1\n    # Essentially, we want to stop the integration if taking another step\n    # would lead to exceeding the final time\n    while torch.any((c_time + timestep) &lt; final_time):\n        c_state = c_state + fn(c_state, c_time)*timestep\n        c_time = c_time + timestep\n        i_states.append((c_state.clone(), c_time.clone()))\n    c_state = c_state + fn(c_state, c_time)*(final_time - c_time)\n    c_time = c_time + (final_time - c_time)\n    i_states.append((c_state.clone(), c_time.clone()))\n        \n    return (c_state, c_time), i_states\n</pre> def integrate_euler(fn, initial_state, initial_time, final_time, timestep):     \"\"\"     A general integration routine for solving an Initial Value Problem     using the Euler Method     :param fn: the function to be integrated     :param initial_state: the initial state to integrate from     :param initial_time: the initial time to integrate from     :param final_time: the final time to integrate to     :param timestep: the time increments to integrate with      :return: a tuple of ((the final state, the final time), the intermediates states [list])      \"\"\"     # The names for the variables have been shortened for concision, and     # to avoid overlap with variables in the outer scope     # I have also left the annotations as they are invaluable for tracking the method.      c_time = initial_time.clone()     c_state = initial_state.clone()          # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times     i_states = [(c_state.clone(), c_time.clone())]          # We integrate as long as the current_time+dt is less than t1     # Essentially, we want to stop the integration if taking another step     # would lead to exceeding the final time     while torch.any((c_time + timestep) &lt; final_time):         c_state = c_state + fn(c_state, c_time)*timestep         c_time = c_time + timestep         i_states.append((c_state.clone(), c_time.clone()))     c_state = c_state + fn(c_state, c_time)*(final_time - c_time)     c_time = c_time + (final_time - c_time)     i_states.append((c_state.clone(), c_time.clone()))              return (c_state, c_time), i_states <p>Let's wrap the plotting into a single routine as well:</p> In\u00a0[13]: Copied! <pre>def plot_trajectory(initial_state, integration_states, ideal_solution_fn, axes=None, method_label=\"Euler Method\"):\n    \"\"\"\n    Plots the trajectory resulting from a numerical integration and its ideal solution. \n    \n    Expects the trajectory of a 1-dimensional system.\n    \n    :param initial_state: The initial state of the system for the `ideal_solution_fn` \n    :param integration_states: The trajectory of the system\n    :param ideal_solution_fn: A function that takes the `initial_state` and the time and returns the ideal solution\n    :param axes: (optional) Axes to plot on\n    :param method_label: (optional) Label for the trajectory\n    :return: figure, axes resulting from plot\n    \"\"\"\n    plot_times = [i[1].cpu().item() for i in integration_states]\n    plot_states = [i[0].cpu().item() for i in integration_states]\n    solution_states = [ideal_solution_fn(initial_state, i[1]).cpu().item() for i in integration_states]\n    \n    if axes is None:\n        fig = plt.figure(figsize=(12, 5), tight_layout=True)\n        axes = []\n    else:\n        fig = axes[0].get_figure()\n    \n    if len(axes) &lt; 2:\n        ax = fig.add_subplot(211)\n        axes.append(ax)\n    else:\n        ax = axes[0]\n    ax.plot(plot_times, solution_states, label='Ideal Solution')\n    ax.plot(plot_times, plot_states, label=method_label, linestyle='--')\n    ax.set_ylabel(r'$x(t)$')\n    ax.legend()\n    if len(axes) &lt; 2:\n        ax = fig.add_subplot(212, sharex=ax)\n        axes.append(ax)\n    else:\n        ax = axes[1]\n    ax.plot(plot_times, [abs(i - j) for i,j in zip(solution_states, plot_states)], label=f'{method_label} Residual')\n    ax.set_xlabel(r'Time $(\\mathrm{s})$')\n    ax.set_ylabel(r'$\\Delta x$')\n    ax.legend()\n    return fig, axes\n</pre> def plot_trajectory(initial_state, integration_states, ideal_solution_fn, axes=None, method_label=\"Euler Method\"):     \"\"\"     Plots the trajectory resulting from a numerical integration and its ideal solution.           Expects the trajectory of a 1-dimensional system.          :param initial_state: The initial state of the system for the `ideal_solution_fn`      :param integration_states: The trajectory of the system     :param ideal_solution_fn: A function that takes the `initial_state` and the time and returns the ideal solution     :param axes: (optional) Axes to plot on     :param method_label: (optional) Label for the trajectory     :return: figure, axes resulting from plot     \"\"\"     plot_times = [i[1].cpu().item() for i in integration_states]     plot_states = [i[0].cpu().item() for i in integration_states]     solution_states = [ideal_solution_fn(initial_state, i[1]).cpu().item() for i in integration_states]          if axes is None:         fig = plt.figure(figsize=(12, 5), tight_layout=True)         axes = []     else:         fig = axes[0].get_figure()          if len(axes) &lt; 2:         ax = fig.add_subplot(211)         axes.append(ax)     else:         ax = axes[0]     ax.plot(plot_times, solution_states, label='Ideal Solution')     ax.plot(plot_times, plot_states, label=method_label, linestyle='--')     ax.set_ylabel(r'$x(t)$')     ax.legend()     if len(axes) &lt; 2:         ax = fig.add_subplot(212, sharex=ax)         axes.append(ax)     else:         ax = axes[1]     ax.plot(plot_times, [abs(i - j) for i,j in zip(solution_states, plot_states)], label=f'{method_label} Residual')     ax.set_xlabel(r'Time $(\\mathrm{s})$')     ax.set_ylabel(r'$\\Delta x$')     ax.legend()     return fig, axes <p>And let's test that the integration routine works as expected</p> In\u00a0[14]: Copied! <pre>(f_state, f_time), sub_states = integrate_euler(simple_fn, x0, t0, t1, dt)\n\nplot_trajectory(x0, sub_states, simple_fn_solution)\n</pre> (f_state, f_time), sub_states = integrate_euler(simple_fn, x0, t0, t1, dt)  plot_trajectory(x0, sub_states, simple_fn_solution) Out[14]: <pre>(&lt;Figure size 1200x500 with 2 Axes&gt;,\n [&lt;Axes: ylabel='$x(t)$'&gt;,\n  &lt;Axes: xlabel='Time $(\\\\mathrm{s})$', ylabel='$\\\\Delta x$'&gt;])</pre> In\u00a0[15]: Copied! <pre>def exponential_fn(x, t):\n    \"\"\"\n    The derivative of an exponential system\n    :param x: the current state of the system\n    :param t: the current time of the system\n    :return: the derivative of the exponential system at (x,t)\n    \"\"\"\n    return -x\n\ndef exponential_fn_solution(initial_state, t):\n    \"\"\"\n    The solution of an exponentially decaying system\n    :param initial_state: the initial state of the systme\n    :param t: the time at which the solution is desired\n    :return: the state at time t\n    \"\"\"\n    return initial_state*torch.exp(-t)\n</pre> def exponential_fn(x, t):     \"\"\"     The derivative of an exponential system     :param x: the current state of the system     :param t: the current time of the system     :return: the derivative of the exponential system at (x,t)     \"\"\"     return -x  def exponential_fn_solution(initial_state, t):     \"\"\"     The solution of an exponentially decaying system     :param initial_state: the initial state of the systme     :param t: the time at which the solution is desired     :return: the state at time t     \"\"\"     return initial_state*torch.exp(-t) In\u00a0[16]: Copied! <pre>(f_state, f_time), sub_states = integrate_euler(exponential_fn, x0, t0, t1, dt)\n\nplot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"Euler Method\")\n</pre> (f_state, f_time), sub_states = integrate_euler(exponential_fn, x0, t0, t1, dt)  plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"Euler Method\") Out[16]: <pre>(&lt;Figure size 1200x500 with 2 Axes&gt;,\n [&lt;Axes: ylabel='$x(t)$'&gt;,\n  &lt;Axes: xlabel='Time $(\\\\mathrm{s})$', ylabel='$\\\\Delta x$'&gt;])</pre> <p>As we can see, the euler method diverges from the true solution quite quickly. Let's look at how the error at the final state changes as we vary the step size:</p> In\u00a0[17]: Copied! <pre>(final_state_full_step, final_time_full_step), _ = integrate_euler(exponential_fn, x0, t0, t1, dt)\n(final_state_half_step, final_time_half_step), _ = integrate_euler(exponential_fn, x0, t0, t1, dt/2)\ntrue_final_state = exponential_fn_solution(x0, t1)\n\nerror_in_full_step = (final_state_full_step - true_final_state).abs()\nerror_in_half_step = (final_state_half_step - true_final_state).abs()\n\nprint(f\"Error in Full Step (Euler): {error_in_full_step.item()}\")\nprint(f\"Error in Half Step (Euler): {error_in_half_step.item()}\")\nprint(f\"Improvement in Error (Euler): {(error_in_full_step/error_in_half_step).item()}\")\n</pre> (final_state_full_step, final_time_full_step), _ = integrate_euler(exponential_fn, x0, t0, t1, dt) (final_state_half_step, final_time_half_step), _ = integrate_euler(exponential_fn, x0, t0, t1, dt/2) true_final_state = exponential_fn_solution(x0, t1)  error_in_full_step = (final_state_full_step - true_final_state).abs() error_in_half_step = (final_state_half_step - true_final_state).abs()  print(f\"Error in Full Step (Euler): {error_in_full_step.item()}\") print(f\"Error in Half Step (Euler): {error_in_half_step.item()}\") print(f\"Improvement in Error (Euler): {(error_in_full_step/error_in_half_step).item()}\") <pre>Error in Full Step (Euler): 0.019201001071442403\nError in Half Step (Euler): 0.00939351876290001\nImprovement in Error (Euler): 2.04406906039059\n</pre> <p>From this we see that the error improvement is linear in the timestep, halving the timestep roughly halves the error. If we'd like to achieve an error on the order of machine precision, in this case float64, we would need to reduce the step until the error is on the order of ${10}^{-16}$ which is the smallest delta representable by float64.</p> <p>Fortunately, PyTorch provides utilities for extracting this value, and assuming linear error reduction and our initial error, we can compute the step size we need:</p> In\u00a0[18]: Copied! <pre>machine_precision_dt = dt * (torch.finfo(torch.float64).eps/error_in_full_step.item())\nprint(f\"Timestep for machine precision (Euler): {machine_precision_dt}\")\n</pre> machine_precision_dt = dt * (torch.finfo(torch.float64).eps/error_in_full_step.item()) print(f\"Timestep for machine precision (Euler): {machine_precision_dt}\") <pre>Timestep for machine precision (Euler): 1.156422022470889e-15\n</pre> <p>A timestep of that size, implies  $(t_1-t_0)/dt=1/dt\\sim{10}^{15}$ steps, inordinately many steps to achieve high precision results. To that end, we need to introduce higher order integration schemes.</p> <p>Let's start with a simple fourth-order scheme: The Classical Fourth-Order Method</p> In\u00a0[20]: Copied! <pre>def integrate_runge_kutta_4(fn, initial_state, initial_time, final_time, timestep):\n    \"\"\"\n    A general integration routine for solving an Initial Value Problem\n    using the Classical Fourth-Order Method\n    :param fn: the function to be integrated\n    :param initial_state: the initial state to integrate from\n    :param initial_time: the initial time to integrate from\n    :param final_time: the final time to integrate to\n    :param timestep: the time increments to integrate with \n    :return: a tuple of ((the final state, the final time), the intermediates states [list]) \n    \"\"\"\n    # The names for the variables have been shortened for concision, and\n    # to avoid overlap with variables in the outer scope\n    # I have also left the annotations as they are invaluable for tracking the method. \n    c_time = initial_time.clone()\n    c_state = initial_state.clone()\n    \n    # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times\n    i_states = [(c_state.clone(), c_time.clone())]\n    \n    # We write out the butcher tableau for easier computation\n    butcher_tableau = torch.tensor([\n        # c0, a00, a01, a02, a03\n        [0.0, 0.0, 0.0, 0.0, 0.0],\n        # c1, a10, a11, a12, a13\n        [0.5, 0.5, 0.0, 0.0, 0.0],\n        # c2, a20, a21, a22, a23\n        [0.5, 0.0, 0.5, 0.0, 0.0],\n        # c3, a30, a31, a32, a33\n        [1.0, 0.0, 0.0, 1.0, 0.0],\n        #     b0,  b1,  b2,  b3\n        [0.0, 1/6, 2/6, 2/6, 1/6]\n    ], dtype=initial_state.dtype, device=initial_state.device)\n    # We see here that this method of writing out the tableau, while complete,\n    # is inefficient given that so many entries are zero.\n    # Ideally, we would have a more compressed format, but for our purposes,\n    # this works.\n    \n    # At each step we repeat the same procedure with differing states and times \n    # (and step sizes for the last step), thus it makes sense to \n    # wrap everything in a local function which can be called repeatedly.\n    def compute_step(state, time, step):\n        # We need to store the intermediate stages\n        k_stages = torch.stack([torch.zeros_like(time)]*(butcher_tableau.shape[1] - 1))\n        # we subtract one since the last row is the final state\n        for stage_index in range(butcher_tableau.shape[0] - 1):\n            c_coeff, *a_coeff = butcher_tableau[stage_index]\n            k_stages[stage_index] = fn(\n                state + step * sum(k*a for k,a in zip(k_stages, a_coeff)), \n                time + c_coeff * step\n            )\n        return state + step * sum(k*b for k,b in zip(k_stages, butcher_tableau[-1, 1:])), time + step\n    \n    # We integrate as long as the current_time+dt is less than t1\n    # Essentially, we want to stop the integration if taking another step\n    # would lead to exceeding the final time\n    while torch.any((c_time + timestep) &lt; final_time):\n        c_state, c_time = compute_step(c_state, c_time, timestep)\n        i_states.append((c_state.clone(), c_time.clone()))\n    c_state, c_time = compute_step(c_state, c_time, final_time - c_time)\n    i_states.append((c_state.clone(), c_time.clone()))\n    \n    return (c_state, c_time), i_states\n</pre> def integrate_runge_kutta_4(fn, initial_state, initial_time, final_time, timestep):     \"\"\"     A general integration routine for solving an Initial Value Problem     using the Classical Fourth-Order Method     :param fn: the function to be integrated     :param initial_state: the initial state to integrate from     :param initial_time: the initial time to integrate from     :param final_time: the final time to integrate to     :param timestep: the time increments to integrate with      :return: a tuple of ((the final state, the final time), the intermediates states [list])      \"\"\"     # The names for the variables have been shortened for concision, and     # to avoid overlap with variables in the outer scope     # I have also left the annotations as they are invaluable for tracking the method.      c_time = initial_time.clone()     c_state = initial_state.clone()          # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times     i_states = [(c_state.clone(), c_time.clone())]          # We write out the butcher tableau for easier computation     butcher_tableau = torch.tensor([         # c0, a00, a01, a02, a03         [0.0, 0.0, 0.0, 0.0, 0.0],         # c1, a10, a11, a12, a13         [0.5, 0.5, 0.0, 0.0, 0.0],         # c2, a20, a21, a22, a23         [0.5, 0.0, 0.5, 0.0, 0.0],         # c3, a30, a31, a32, a33         [1.0, 0.0, 0.0, 1.0, 0.0],         #     b0,  b1,  b2,  b3         [0.0, 1/6, 2/6, 2/6, 1/6]     ], dtype=initial_state.dtype, device=initial_state.device)     # We see here that this method of writing out the tableau, while complete,     # is inefficient given that so many entries are zero.     # Ideally, we would have a more compressed format, but for our purposes,     # this works.          # At each step we repeat the same procedure with differing states and times      # (and step sizes for the last step), thus it makes sense to      # wrap everything in a local function which can be called repeatedly.     def compute_step(state, time, step):         # We need to store the intermediate stages         k_stages = torch.stack([torch.zeros_like(time)]*(butcher_tableau.shape[1] - 1))         # we subtract one since the last row is the final state         for stage_index in range(butcher_tableau.shape[0] - 1):             c_coeff, *a_coeff = butcher_tableau[stage_index]             k_stages[stage_index] = fn(                 state + step * sum(k*a for k,a in zip(k_stages, a_coeff)),                  time + c_coeff * step             )         return state + step * sum(k*b for k,b in zip(k_stages, butcher_tableau[-1, 1:])), time + step          # We integrate as long as the current_time+dt is less than t1     # Essentially, we want to stop the integration if taking another step     # would lead to exceeding the final time     while torch.any((c_time + timestep) &lt; final_time):         c_state, c_time = compute_step(c_state, c_time, timestep)         i_states.append((c_state.clone(), c_time.clone()))     c_state, c_time = compute_step(c_state, c_time, final_time - c_time)     i_states.append((c_state.clone(), c_time.clone()))          return (c_state, c_time), i_states In\u00a0[21]: Copied! <pre>(final_state_euler, final_time_euler), _ = integrate_euler(exponential_fn, x0, t0, t1, dt)\n(final_state_rk4, final_time_rk4), _ = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, dt)\ntrue_final_state = exponential_fn_solution(x0, t1)\n\nerror_in_euler = (final_state_euler - true_final_state).abs()\nerror_in_rk4 = (final_state_rk4 - true_final_state).abs()\n\nprint(f\"Error in Euler: {error_in_euler.item()}\")\nprint(f\"Error in RK4: {error_in_rk4.item()}\")\nprint(f\"Improvement in Error: {(error_in_euler/error_in_rk4).item()}\")\n</pre> (final_state_euler, final_time_euler), _ = integrate_euler(exponential_fn, x0, t0, t1, dt) (final_state_rk4, final_time_rk4), _ = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, dt) true_final_state = exponential_fn_solution(x0, t1)  error_in_euler = (final_state_euler - true_final_state).abs() error_in_rk4 = (final_state_rk4 - true_final_state).abs()  print(f\"Error in Euler: {error_in_euler.item()}\") print(f\"Error in RK4: {error_in_rk4.item()}\") print(f\"Improvement in Error: {(error_in_euler/error_in_rk4).item()}\") <pre>Error in Euler: 0.019201001071442403\nError in RK4: 3.332410560275001e-07\nImprovement in Error: 57618.953979841775\n</pre> <p>We see that the 4th order method shows significantly less error than the Euler method for the same timestep. Furthermore, we can see how the error scales as a function of the timestep:</p> In\u00a0[22]: Copied! <pre>(final_state_full_step, final_time_full_step), _ = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, dt)\n(final_state_half_step, final_time_half_step), _ = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, dt/2)\ntrue_final_state = exponential_fn_solution(x0, t1)\n\nexponential_fn_solution(x0, t1)\n\nerror_in_full_step = (final_state_full_step - true_final_state).abs()\nerror_in_half_step = (final_state_half_step - true_final_state).abs()\n\nprint(f\"Error in Full Step (RK4): {error_in_full_step.item()}\")\nprint(f\"Error in Half Step (RK4): {error_in_half_step.item()}\")\nprint(f\"Improvement in Error (RK4): {(error_in_full_step/error_in_half_step).item()}\")\n</pre> (final_state_full_step, final_time_full_step), _ = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, dt) (final_state_half_step, final_time_half_step), _ = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, dt/2) true_final_state = exponential_fn_solution(x0, t1)  exponential_fn_solution(x0, t1)  error_in_full_step = (final_state_full_step - true_final_state).abs() error_in_half_step = (final_state_half_step - true_final_state).abs()  print(f\"Error in Full Step (RK4): {error_in_full_step.item()}\") print(f\"Error in Half Step (RK4): {error_in_half_step.item()}\") print(f\"Improvement in Error (RK4): {(error_in_full_step/error_in_half_step).item()}\") <pre>Error in Full Step (RK4): 3.332410560275001e-07\nError in Half Step (RK4): 1.9976097442864216e-08\nImprovement in Error (RK4): 16.68198991222578\n</pre> <p>Incredible! Halving the timestep improved our results by a factor of 16, this is exactly as expected of a fourth order method where the error should improve proportional to the fourth power of the step size.</p> <p>Plotting the same curves as before, we can see how the error varies over the integration time:</p> In\u00a0[23]: Copied! <pre>(f_state, f_time), sub_states = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, dt)\n\nplot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"RK4 Method\")\n</pre> (f_state, f_time), sub_states = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, dt)  plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"RK4 Method\") Out[23]: <pre>(&lt;Figure size 1200x500 with 2 Axes&gt;,\n [&lt;Axes: ylabel='$x(t)$'&gt;,\n  &lt;Axes: xlabel='Time $(\\\\mathrm{s})$', ylabel='$\\\\Delta x$'&gt;])</pre> <p>Reducing the error down to machine precision now requires a more reasonable number of steps:</p> In\u00a0[24]: Copied! <pre>machine_precision_dt = dt * (torch.finfo(torch.float64).eps/error_in_full_step.item())**(1/4)\nprint(f\"Timestep for machine precision (RK4): {machine_precision_dt}\")\n</pre> machine_precision_dt = dt * (torch.finfo(torch.float64).eps/error_in_full_step.item())**(1/4) print(f\"Timestep for machine precision (RK4): {machine_precision_dt}\") <pre>Timestep for machine precision (RK4): 0.0005080663487927019\n</pre> In\u00a0[25]: Copied! <pre>(f_state, f_time), sub_states = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, machine_precision_dt)\n\nplot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"RK4 Method\")\n\nprint(f\"Error in Machine Precision Step (RK4): {(f_state - exponential_fn_solution(x0, t1)).abs().item()}\")\n</pre> (f_state, f_time), sub_states = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, machine_precision_dt)  plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"RK4 Method\")  print(f\"Error in Machine Precision Step (RK4): {(f_state - exponential_fn_solution(x0, t1)).abs().item()}\") <pre>Error in Machine Precision Step (RK4): 1.4210854715202004e-14\n</pre> <p>While the error achieved is on the order of machine precision, the error now shows a very different behaviour than before. Instead of decaying to a constant negative value, it appears to move randomly.</p> <p>More precisely, the error is now no longer a correlated function of the integral but rather what is referred to as a Random Walk where the changes in the error can be modelled as a stochastic process.</p> <p>This occurs due to the two competing sources of error in our computation: The imprecision of the integration scheme and the finite precision of the underlying arithmetic type. Having improved the former to its limits, the latter now becomes the dominant source of error.</p> <p>Theoretically, this error can be reduced by using a compensated summation scheme where we track the truncated bits in the arithmetic and reintroduce them at each step. In practice, this yields only a modest improvement in results.</p> <p>Kahan Summation is a compensated summation scheme that tracks truncated bits in the sum, it takes advantage of the order of operations to estimate these bits and then reintroduces them into the sum.</p> <p>The variant we'll implement is the Kahan-Babu\u0161ka-Neumaier algorithm, it takes into account the magnitude of the terms and accounts for which terms bits are truncated.</p> In\u00a0[28]: Copied! <pre>def compensated_sum(iterable_to_sum):\n    \"\"\"\n    Functional equivalent to the python function `sum` but\n    uses the Kahan-Babu\u0161ka-Neumaier scheme to track truncated bits.\n    \n    Usage is the same as `sum`, but only works with pytorch tensors.\n    \n    References:\n        [1] https://en.wikipedia.org/wiki/Kahan_summation_algorithm#Further_enhancements\n    \n    :param iterable_to_sum: any kind of iterable including generators that returns tensors\n    :return: the compensated sum\n    \"\"\"\n    partial_sum = None\n    truncated_bits = None\n    for v in iterable_to_sum:\n        if partial_sum is None:\n            partial_sum = torch.zeros_like(v)\n            truncated_bits = torch.zeros_like(v)\n        temporary_partial_sum = partial_sum + v\n        truncated_bits = truncated_bits + torch.where(\n            torch.abs(partial_sum) &gt;= torch.abs(v),       # When the magnitude of the partial sum is larger, truncation occurs for v and vice versa when v is larger\n            (partial_sum - temporary_partial_sum) + v,    # First the negation of the truncated value of v is computed from the partial sum and the temporary partial sum, and then adding it to v gives the truncated bits\n            (v - temporary_partial_sum) + partial_sum     # As before, but the role of v and partial_sum are swapped\n        )\n        partial_sum = temporary_partial_sum\n    return partial_sum + truncated_bits                   # Add truncated bits back to the sum\n</pre> def compensated_sum(iterable_to_sum):     \"\"\"     Functional equivalent to the python function `sum` but     uses the Kahan-Babu\u0161ka-Neumaier scheme to track truncated bits.          Usage is the same as `sum`, but only works with pytorch tensors.          References:         [1] https://en.wikipedia.org/wiki/Kahan_summation_algorithm#Further_enhancements          :param iterable_to_sum: any kind of iterable including generators that returns tensors     :return: the compensated sum     \"\"\"     partial_sum = None     truncated_bits = None     for v in iterable_to_sum:         if partial_sum is None:             partial_sum = torch.zeros_like(v)             truncated_bits = torch.zeros_like(v)         temporary_partial_sum = partial_sum + v         truncated_bits = truncated_bits + torch.where(             torch.abs(partial_sum) &gt;= torch.abs(v),       # When the magnitude of the partial sum is larger, truncation occurs for v and vice versa when v is larger             (partial_sum - temporary_partial_sum) + v,    # First the negation of the truncated value of v is computed from the partial sum and the temporary partial sum, and then adding it to v gives the truncated bits             (v - temporary_partial_sum) + partial_sum     # As before, but the role of v and partial_sum are swapped         )         partial_sum = temporary_partial_sum     return partial_sum + truncated_bits                   # Add truncated bits back to the sum In\u00a0[29]: Copied! <pre>def integrate_runge_kutta_4_compensated(fn, initial_state, initial_time, final_time, timestep):\n    \"\"\"\n    A general integration routine for solving an Initial Value Problem\n    using the Classical Fourth-Order Method.\n    \n    Instead of naively summing the changes, we use compensated summation.\n    \n    :param fn: the function to be integrated\n    :param initial_state: the initial state to integrate from\n    :param initial_time: the initial time to integrate from\n    :param final_time: the final time to integrate to\n    :param timestep: the time increments to integrate with \n    :return: a tuple of ((the final state, the final time), the intermediates states [list]) \n    \"\"\"\n    # The names for the variables have been shortened for concision, and\n    # to avoid overlap with variables in the outer scope\n    # I have also left the annotations as they are invaluable for tracking the method. \n    c_time = initial_time.clone()\n    c_state = initial_state.clone()\n    \n    # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times\n    i_states = [(c_state.clone(), c_time.clone())]\n    \n    # We write out the butcher tableau for easier computation\n    butcher_tableau = torch.tensor([\n        # c0, a00, a01, a02, a03\n        [0.0, 0.0, 0.0, 0.0, 0.0],\n        # c1, a10, a11, a12, a13\n        [0.5, 0.5, 0.0, 0.0, 0.0],\n        # c2, a20, a21, a22, a23\n        [0.5, 0.0, 0.5, 0.0, 0.0],\n        # c3, a30, a31, a32, a33\n        [1.0, 0.0, 0.0, 1.0, 0.0],\n        #     b0,  b1,  b2,  b3\n        [0.0, 1/6, 2/6, 2/6, 1/6]\n    ], dtype=initial_state.dtype, device=initial_state.device)\n    # We see here that this method of writing out the tableau, while complete,\n    # is inefficient given that so many entries are zero.\n    # Ideally, we would have a more compressed format, but for our purposes,\n    # this works.\n    \n    # At each step we repeat the same procedure with differing states and times \n    # (and step sizes for the last step), thus it makes sense to \n    # wrap everything in a local function which can be called repeatedly.\n    def compute_step(state, time, step):\n        # We need to store the intermediate stages\n        k_stages = torch.stack([torch.zeros_like(time)]*(butcher_tableau.shape[1] - 1))\n        # we subtract one since the last row is the final state\n        for stage_index in range(butcher_tableau.shape[0] - 1):\n            c_coeff, *a_coeff = butcher_tableau[stage_index]\n            k_stages[stage_index] = fn(\n                # We use `compensated_sum` instead of sum in order to avoid truncation at each stage calculation\n                state + step * compensated_sum(k*a for k,a in zip(k_stages, a_coeff)), \n                time + c_coeff * step\n            )\n        # We return the change in the state and the change in the time as we track partial sums\n        # Again, we use `compensated_sum` to avoid truncation in the final stage calculations\n        return step * compensated_sum(k*b for k,b in zip(k_stages, butcher_tableau[-1, 1:])), step\n    \n    # Initialise compensated sum terms\n    # We don't need the partial sum variable as `c_state` and `c_time` take on that role\n    truncated_bits_state = torch.zeros_like(c_state)\n    truncated_bits_time  = torch.zeros_like(c_time)\n    \n    # We integrate as long as the current_time+dt is less than t1\n    # Essentially, we want to stop the integration if taking another step\n    # would lead to exceeding the final time\n    while torch.any((c_time + timestep) &lt; final_time):\n        # To compute the change in state, we add the truncated bits back\n        delta_state, delta_time = compute_step(c_state + truncated_bits_state, c_time + truncated_bits_time, timestep)\n        \n        temporary = c_state + delta_state, c_time + delta_time\n        truncated_bits_state = truncated_bits_state + torch.where(\n            torch.abs(c_state) &gt;= torch.abs(delta_state),\n            (c_state - temporary[0]) + delta_state,\n            (delta_state - temporary[0]) + c_state\n        )\n        truncated_bits_time = truncated_bits_time + torch.where(\n            torch.abs(c_time) &gt;= torch.abs(delta_time),\n            (c_time - temporary[1]) + delta_time,\n            (delta_time - temporary[1]) + c_time\n        )\n        c_state, c_time = temporary\n        # We would like to store the intermediate states with the compensated partial sums\n        i_states.append((c_state + truncated_bits_state, c_time + truncated_bits_time))\n    delta_state, delta_time = compute_step(c_state + truncated_bits_state, c_time + truncated_bits_time, final_time - (c_time + truncated_bits_time))\n    temporary = c_state + delta_state, c_time + delta_time\n    truncated_bits_state = truncated_bits_state + torch.where(\n        torch.abs(c_state) &gt;= torch.abs(delta_state),\n        (c_state - temporary[0]) + delta_state,\n        (delta_state - temporary[0]) + c_state\n    )\n    truncated_bits_time = truncated_bits_time + torch.where(\n        torch.abs(c_time) &gt;= torch.abs(delta_time),\n        (c_time - temporary[1]) + delta_time,\n        (delta_time - temporary[1]) + c_time\n    )\n    c_state, c_time = temporary\n    c_state, c_time = c_state + truncated_bits_state, c_time + truncated_bits_time\n    i_states.append((c_state.clone(), c_time.clone()))\n    \n    return (c_state, c_time), i_states\n</pre> def integrate_runge_kutta_4_compensated(fn, initial_state, initial_time, final_time, timestep):     \"\"\"     A general integration routine for solving an Initial Value Problem     using the Classical Fourth-Order Method.          Instead of naively summing the changes, we use compensated summation.          :param fn: the function to be integrated     :param initial_state: the initial state to integrate from     :param initial_time: the initial time to integrate from     :param final_time: the final time to integrate to     :param timestep: the time increments to integrate with      :return: a tuple of ((the final state, the final time), the intermediates states [list])      \"\"\"     # The names for the variables have been shortened for concision, and     # to avoid overlap with variables in the outer scope     # I have also left the annotations as they are invaluable for tracking the method.      c_time = initial_time.clone()     c_state = initial_state.clone()          # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times     i_states = [(c_state.clone(), c_time.clone())]          # We write out the butcher tableau for easier computation     butcher_tableau = torch.tensor([         # c0, a00, a01, a02, a03         [0.0, 0.0, 0.0, 0.0, 0.0],         # c1, a10, a11, a12, a13         [0.5, 0.5, 0.0, 0.0, 0.0],         # c2, a20, a21, a22, a23         [0.5, 0.0, 0.5, 0.0, 0.0],         # c3, a30, a31, a32, a33         [1.0, 0.0, 0.0, 1.0, 0.0],         #     b0,  b1,  b2,  b3         [0.0, 1/6, 2/6, 2/6, 1/6]     ], dtype=initial_state.dtype, device=initial_state.device)     # We see here that this method of writing out the tableau, while complete,     # is inefficient given that so many entries are zero.     # Ideally, we would have a more compressed format, but for our purposes,     # this works.          # At each step we repeat the same procedure with differing states and times      # (and step sizes for the last step), thus it makes sense to      # wrap everything in a local function which can be called repeatedly.     def compute_step(state, time, step):         # We need to store the intermediate stages         k_stages = torch.stack([torch.zeros_like(time)]*(butcher_tableau.shape[1] - 1))         # we subtract one since the last row is the final state         for stage_index in range(butcher_tableau.shape[0] - 1):             c_coeff, *a_coeff = butcher_tableau[stage_index]             k_stages[stage_index] = fn(                 # We use `compensated_sum` instead of sum in order to avoid truncation at each stage calculation                 state + step * compensated_sum(k*a for k,a in zip(k_stages, a_coeff)),                  time + c_coeff * step             )         # We return the change in the state and the change in the time as we track partial sums         # Again, we use `compensated_sum` to avoid truncation in the final stage calculations         return step * compensated_sum(k*b for k,b in zip(k_stages, butcher_tableau[-1, 1:])), step          # Initialise compensated sum terms     # We don't need the partial sum variable as `c_state` and `c_time` take on that role     truncated_bits_state = torch.zeros_like(c_state)     truncated_bits_time  = torch.zeros_like(c_time)          # We integrate as long as the current_time+dt is less than t1     # Essentially, we want to stop the integration if taking another step     # would lead to exceeding the final time     while torch.any((c_time + timestep) &lt; final_time):         # To compute the change in state, we add the truncated bits back         delta_state, delta_time = compute_step(c_state + truncated_bits_state, c_time + truncated_bits_time, timestep)                  temporary = c_state + delta_state, c_time + delta_time         truncated_bits_state = truncated_bits_state + torch.where(             torch.abs(c_state) &gt;= torch.abs(delta_state),             (c_state - temporary[0]) + delta_state,             (delta_state - temporary[0]) + c_state         )         truncated_bits_time = truncated_bits_time + torch.where(             torch.abs(c_time) &gt;= torch.abs(delta_time),             (c_time - temporary[1]) + delta_time,             (delta_time - temporary[1]) + c_time         )         c_state, c_time = temporary         # We would like to store the intermediate states with the compensated partial sums         i_states.append((c_state + truncated_bits_state, c_time + truncated_bits_time))     delta_state, delta_time = compute_step(c_state + truncated_bits_state, c_time + truncated_bits_time, final_time - (c_time + truncated_bits_time))     temporary = c_state + delta_state, c_time + delta_time     truncated_bits_state = truncated_bits_state + torch.where(         torch.abs(c_state) &gt;= torch.abs(delta_state),         (c_state - temporary[0]) + delta_state,         (delta_state - temporary[0]) + c_state     )     truncated_bits_time = truncated_bits_time + torch.where(         torch.abs(c_time) &gt;= torch.abs(delta_time),         (c_time - temporary[1]) + delta_time,         (delta_time - temporary[1]) + c_time     )     c_state, c_time = temporary     c_state, c_time = c_state + truncated_bits_state, c_time + truncated_bits_time     i_states.append((c_state.clone(), c_time.clone()))          return (c_state, c_time), i_states In\u00a0[30]: Copied! <pre>(f_state, f_time), sub_states = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, machine_precision_dt)\n(f_state_compensated, f_time_compensated), sub_states_compensated = integrate_runge_kutta_4_compensated(exponential_fn, x0, t0, t1, machine_precision_dt)\n\nfig, axes = plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"Uncompensated RK4\")\nfig, axes = plot_trajectory(x0, sub_states_compensated, exponential_fn_solution, axes=axes, method_label=\"Compensated RK4\")\naxes[1].set_yscale('log')\n\nprint(f\"Error in Machine Precision Step (RK4): {(f_state - exponential_fn_solution(x0, t1)).abs().item()}\")\nprint(f\"Error in Machine Precision Step (RK4): {(f_state_compensated - exponential_fn_solution(x0, t1)).abs().item()}\")\n</pre> (f_state, f_time), sub_states = integrate_runge_kutta_4(exponential_fn, x0, t0, t1, machine_precision_dt) (f_state_compensated, f_time_compensated), sub_states_compensated = integrate_runge_kutta_4_compensated(exponential_fn, x0, t0, t1, machine_precision_dt)  fig, axes = plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"Uncompensated RK4\") fig, axes = plot_trajectory(x0, sub_states_compensated, exponential_fn_solution, axes=axes, method_label=\"Compensated RK4\") axes[1].set_yscale('log')  print(f\"Error in Machine Precision Step (RK4): {(f_state - exponential_fn_solution(x0, t1)).abs().item()}\") print(f\"Error in Machine Precision Step (RK4): {(f_state_compensated - exponential_fn_solution(x0, t1)).abs().item()}\") <pre>Error in Machine Precision Step (RK4): 1.4210854715202004e-14\nError in Machine Precision Step (RK4): 2.220446049250313e-16\n</pre> <p>We have gained 2 orders of magnitude improvement, but this is only relevant once our step size is small enough that truncation error is the dominant; with neural networks we're rarely interested in machine precision accuracy and 64-bit floating point numbers are generally slower to compute with (and store).</p> <p>Thus, for neural networks we try to minimise the arithmetic precision to improve the compute speed and reduce the memory bandwidth required. While 32-bit floats are the most common, mixed-precision is being used to focus on keeping precision for only the operations where it matters most while reducing the precision down to 16-bit floats or even 8-bit floats.</p> <p>Let's look at the benefit of compensated summation on using 32-bit floats:</p> In\u00a0[31]: Copied! <pre>machine_precision_dt32 = dt * (torch.finfo(torch.float32).eps/error_in_full_step.item())**(1/4)\nprint(f\"Timestep for machine precision (RK4 - float32): {machine_precision_dt32}\")\n\n(f_state, f_time), sub_states = integrate_runge_kutta_4(exponential_fn, x0.to(torch.float32), t0.to(torch.float32), t1.to(torch.float32), machine_precision_dt32)\n(f_state_compensated, f_time_compensated), sub_states_compensated = integrate_runge_kutta_4_compensated(exponential_fn, x0.to(torch.float32), t0.to(torch.float32), t1.to(torch.float32), machine_precision_dt32)\n\nfig, axes = plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"Uncompensated RK4\")\nfig, axes = plot_trajectory(x0, sub_states_compensated, exponential_fn_solution, axes=axes, method_label=\"Compensated RK4\")\naxes[1].set_yscale('log')\n\nprint(f\"Error in Machine Precision Step (RK4): {(f_state - exponential_fn_solution(x0, t1)).abs().item()}\")\nprint(f\"Error in Machine Precision Step (RK4): {(f_state_compensated - exponential_fn_solution(x0, t1)).abs().item()}\")\n</pre> machine_precision_dt32 = dt * (torch.finfo(torch.float32).eps/error_in_full_step.item())**(1/4) print(f\"Timestep for machine precision (RK4 - float32): {machine_precision_dt32}\")  (f_state, f_time), sub_states = integrate_runge_kutta_4(exponential_fn, x0.to(torch.float32), t0.to(torch.float32), t1.to(torch.float32), machine_precision_dt32) (f_state_compensated, f_time_compensated), sub_states_compensated = integrate_runge_kutta_4_compensated(exponential_fn, x0.to(torch.float32), t0.to(torch.float32), t1.to(torch.float32), machine_precision_dt32)  fig, axes = plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"Uncompensated RK4\") fig, axes = plot_trajectory(x0, sub_states_compensated, exponential_fn_solution, axes=axes, method_label=\"Compensated RK4\") axes[1].set_yscale('log')  print(f\"Error in Machine Precision Step (RK4): {(f_state - exponential_fn_solution(x0, t1)).abs().item()}\") print(f\"Error in Machine Precision Step (RK4): {(f_state_compensated - exponential_fn_solution(x0, t1)).abs().item()}\") <pre>Timestep for machine precision (RK4 - float32): 0.07733710296035012\nError in Machine Precision Step (RK4): 1.283590447265226e-07\nError in Machine Precision Step (RK4): 9.855672233882729e-08\n</pre> <p>As we can see, the gain is marginal, perhaps this is partially due to the less frequent step sizes, but also because the error in the arithmetic is entirely from the imprecision of the calculations themselves and not from truncation error.</p> In\u00a0[36]: Copied! <pre>class IntegrateRK4(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, fn, x0, t0, t1, dt):\n        \"\"\"\n        A general integration routine for solving an Initial Value Problem\n        using the Classical Fourth-Order Method.\n        \n        Instead of naively summing the changes, we use compensated summation.\n        \n        :param fn: the function to be integrated\n        :param initial_state: the initial state to integrate from\n        :param initial_time: the initial time to integrate from\n        :param final_time: the final time to integrate to\n        :param timestep: the time increments to integrate with \n        :return: a tuple of ((the final state, the final time), the intermediates states [list]) \n        \"\"\"\n        # The names for the variables have been shortened for concision, and\n        # to avoid overlap with variables in the outer scope\n        # I have also left the annotations as they are invaluable for tracking the method. \n        c_time = t0.clone()\n        c_state = x0.clone()\n        \n        # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times\n        i_states = [(c_state.clone(), c_time.clone())]\n        \n        # We write out the butcher tableau for easier computation\n        butcher_tableau = torch.tensor([\n            # c0, a00, a01, a02, a03\n            [0.0, 0.0, 0.0, 0.0, 0.0],\n            # c1, a10, a11, a12, a13\n            [0.5, 0.5, 0.0, 0.0, 0.0],\n            # c2, a20, a21, a22, a23\n            [0.5, 0.0, 0.5, 0.0, 0.0],\n            # c3, a30, a31, a32, a33\n            [1.0, 0.0, 0.0, 1.0, 0.0],\n            #     b0,  b1,  b2,  b3\n            [0.0, 1/6, 2/6, 2/6, 1/6]\n        ], dtype=x0.dtype, device=x0.device)\n        # We see here that this method of writing out the tableau, while complete,\n        # is inefficient given that so many entries are zero.\n        # Ideally, we would have a more compressed format, but for our purposes,\n        # this works.\n        \n        # At each step we repeat the same procedure with differing states and times \n        # (and step sizes for the last step), thus it makes sense to \n        # wrap everything in a local function which can be called repeatedly.\n        def compute_step(state, time, step):\n            # We need to store the intermediate stages\n            k_stages = torch.stack([torch.zeros_like(time)]*(butcher_tableau.shape[1] - 1))\n            # we subtract one since the last row is the final state\n            for stage_index in range(butcher_tableau.shape[0] - 1):\n                c_coeff, *a_coeff = butcher_tableau[stage_index]\n                k_stages[stage_index] = fn(\n                    # We use `compensated_sum` instead of sum in order to avoid truncation at each stage calculation\n                    state + step * compensated_sum(k*a for k,a in zip(k_stages, a_coeff)), \n                    time + c_coeff * step\n                )\n            # We return the change in the state and the change in the time as we track partial sums\n            # Again, we use `compensated_sum` to avoid truncation in the final stage calculations\n            return step * compensated_sum(k*b for k,b in zip(k_stages, butcher_tableau[-1, 1:])), step\n        \n        # Initialise compensated sum terms\n        # We don't need the partial sum variable as `c_state` and `c_time` take on that role\n        truncated_bits_state = torch.zeros_like(c_state)\n        truncated_bits_time  = torch.zeros_like(c_time)\n        \n        # We integrate as long as the current_time+dt is less than t1\n        # Essentially, we want to stop the integration if taking another step\n        # would lead to exceeding the final time\n        while torch.any((c_time + dt) &lt; t1):\n            # To compute the change in state, we add the truncated bits back\n            delta_state, delta_time = compute_step(c_state + truncated_bits_state, c_time + truncated_bits_time, dt)\n            \n            temporary = c_state + delta_state, c_time + delta_time\n            truncated_bits_state = truncated_bits_state + torch.where(\n                torch.abs(c_state) &gt;= torch.abs(delta_state),\n                (c_state - temporary[0]) + delta_state,\n                (delta_state - temporary[0]) + c_state\n            )\n            truncated_bits_time = truncated_bits_time + torch.where(\n                torch.abs(c_time) &gt;= torch.abs(delta_time),\n                (c_time - temporary[1]) + delta_time,\n                (delta_time - temporary[1]) + c_time\n            )\n            c_state, c_time = temporary\n            # We would like to store the intermediate states with the compensated partial sums\n            i_states.append((c_state + truncated_bits_state, c_time + truncated_bits_time))\n        delta_state, delta_time = compute_step(c_state + truncated_bits_state, c_time + truncated_bits_time, t1 - (c_time + truncated_bits_time))\n        temporary = c_state + delta_state, c_time + delta_time\n        truncated_bits_state = truncated_bits_state + torch.where(\n            torch.abs(c_state) &gt;= torch.abs(delta_state),\n            (c_state - temporary[0]) + delta_state,\n            (delta_state - temporary[0]) + c_state\n        )\n        truncated_bits_time = truncated_bits_time + torch.where(\n            torch.abs(c_time) &gt;= torch.abs(delta_time),\n            (c_time - temporary[1]) + delta_time,\n            (delta_time - temporary[1]) + c_time\n        )\n        c_state, c_time = temporary\n        c_state, c_time = c_state + truncated_bits_state, c_time + truncated_bits_time\n        i_states.append((c_state.clone(), c_time.clone()))\n        \n        ctx.save_for_backward((c_state, c_time, i_states))\n        ctx.integration_function = fn        \n        \n        return (c_state, c_time), i_states\n</pre> class IntegrateRK4(torch.autograd.Function):     @staticmethod     def forward(ctx, fn, x0, t0, t1, dt):         \"\"\"         A general integration routine for solving an Initial Value Problem         using the Classical Fourth-Order Method.                  Instead of naively summing the changes, we use compensated summation.                  :param fn: the function to be integrated         :param initial_state: the initial state to integrate from         :param initial_time: the initial time to integrate from         :param final_time: the final time to integrate to         :param timestep: the time increments to integrate with          :return: a tuple of ((the final state, the final time), the intermediates states [list])          \"\"\"         # The names for the variables have been shortened for concision, and         # to avoid overlap with variables in the outer scope         # I have also left the annotations as they are invaluable for tracking the method.          c_time = t0.clone()         c_state = x0.clone()                  # We'd like to track the whole trajectory, not just the final state, so we use a list to store the intermediate states and times         i_states = [(c_state.clone(), c_time.clone())]                  # We write out the butcher tableau for easier computation         butcher_tableau = torch.tensor([             # c0, a00, a01, a02, a03             [0.0, 0.0, 0.0, 0.0, 0.0],             # c1, a10, a11, a12, a13             [0.5, 0.5, 0.0, 0.0, 0.0],             # c2, a20, a21, a22, a23             [0.5, 0.0, 0.5, 0.0, 0.0],             # c3, a30, a31, a32, a33             [1.0, 0.0, 0.0, 1.0, 0.0],             #     b0,  b1,  b2,  b3             [0.0, 1/6, 2/6, 2/6, 1/6]         ], dtype=x0.dtype, device=x0.device)         # We see here that this method of writing out the tableau, while complete,         # is inefficient given that so many entries are zero.         # Ideally, we would have a more compressed format, but for our purposes,         # this works.                  # At each step we repeat the same procedure with differing states and times          # (and step sizes for the last step), thus it makes sense to          # wrap everything in a local function which can be called repeatedly.         def compute_step(state, time, step):             # We need to store the intermediate stages             k_stages = torch.stack([torch.zeros_like(time)]*(butcher_tableau.shape[1] - 1))             # we subtract one since the last row is the final state             for stage_index in range(butcher_tableau.shape[0] - 1):                 c_coeff, *a_coeff = butcher_tableau[stage_index]                 k_stages[stage_index] = fn(                     # We use `compensated_sum` instead of sum in order to avoid truncation at each stage calculation                     state + step * compensated_sum(k*a for k,a in zip(k_stages, a_coeff)),                      time + c_coeff * step                 )             # We return the change in the state and the change in the time as we track partial sums             # Again, we use `compensated_sum` to avoid truncation in the final stage calculations             return step * compensated_sum(k*b for k,b in zip(k_stages, butcher_tableau[-1, 1:])), step                  # Initialise compensated sum terms         # We don't need the partial sum variable as `c_state` and `c_time` take on that role         truncated_bits_state = torch.zeros_like(c_state)         truncated_bits_time  = torch.zeros_like(c_time)                  # We integrate as long as the current_time+dt is less than t1         # Essentially, we want to stop the integration if taking another step         # would lead to exceeding the final time         while torch.any((c_time + dt) &lt; t1):             # To compute the change in state, we add the truncated bits back             delta_state, delta_time = compute_step(c_state + truncated_bits_state, c_time + truncated_bits_time, dt)                          temporary = c_state + delta_state, c_time + delta_time             truncated_bits_state = truncated_bits_state + torch.where(                 torch.abs(c_state) &gt;= torch.abs(delta_state),                 (c_state - temporary[0]) + delta_state,                 (delta_state - temporary[0]) + c_state             )             truncated_bits_time = truncated_bits_time + torch.where(                 torch.abs(c_time) &gt;= torch.abs(delta_time),                 (c_time - temporary[1]) + delta_time,                 (delta_time - temporary[1]) + c_time             )             c_state, c_time = temporary             # We would like to store the intermediate states with the compensated partial sums             i_states.append((c_state + truncated_bits_state, c_time + truncated_bits_time))         delta_state, delta_time = compute_step(c_state + truncated_bits_state, c_time + truncated_bits_time, t1 - (c_time + truncated_bits_time))         temporary = c_state + delta_state, c_time + delta_time         truncated_bits_state = truncated_bits_state + torch.where(             torch.abs(c_state) &gt;= torch.abs(delta_state),             (c_state - temporary[0]) + delta_state,             (delta_state - temporary[0]) + c_state         )         truncated_bits_time = truncated_bits_time + torch.where(             torch.abs(c_time) &gt;= torch.abs(delta_time),             (c_time - temporary[1]) + delta_time,             (delta_time - temporary[1]) + c_time         )         c_state, c_time = temporary         c_state, c_time = c_state + truncated_bits_state, c_time + truncated_bits_time         i_states.append((c_state.clone(), c_time.clone()))                  ctx.save_for_backward((c_state, c_time, i_states))         ctx.integration_function = fn                          return (c_state, c_time), i_states In\u00a0[38]: Copied! <pre>(f_state, f_time), sub_states = IntegrateRK4.apply(exponential_fn, x0, t0, t1, machine_precision_dt)\n\nfig, axes = plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"Compensated RK4\")\n\nprint(f\"Error in Machine Precision Step (RK4): {(f_state - exponential_fn_solution(x0, t1)).abs().item()}\")\n</pre> (f_state, f_time), sub_states = IntegrateRK4.apply(exponential_fn, x0, t0, t1, machine_precision_dt)  fig, axes = plot_trajectory(x0, sub_states, exponential_fn_solution, method_label=\"Compensated RK4\")  print(f\"Error in Machine Precision Step (RK4): {(f_state - exponential_fn_solution(x0, t1)).abs().item()}\") <pre>Error in Machine Precision Step (RK4): 2.220446049250313e-16\n</pre> <p>In the next notebook, we will take this function and show how an arbitrary butcher tableau may be used for integration. We will also discuss the modifications needed to implement an adaptive integration scheme.</p>"},{"location":"01-simple-integration-routines/#simple-integration-routine","title":"Simple Integration Routine\u00b6","text":"<p>In this notebook, we will go through the process of implementing a function, <code>integrate(fn, x0, t0, t1, dt)</code>, that integrates a function, <code>fn(x, t)</code>, from <code>t0</code> to <code>t1</code> in increments of <code>dt</code> using <code>x0</code> as the initial state.</p> <p>We will implement the Euler method, demonstrate the Runge-Kutta 4 method and show how to implement this function in a way that is compatible with PyTorch's autograd framework in order to facilitate easy integration into a neural network pipeline</p>"},{"location":"01-simple-integration-routines/#define-fnxt","title":"Define <code>fn(x,t)</code>\u00b6","text":"<p>First, we'll start with a simple test function, $\\frac{dx}{dt}=1$ whose solution is known: $x(t)=x_0+t$</p>"},{"location":"01-simple-integration-routines/#define-integratefn-x0-t0-t1-dt","title":"Define <code>integrate(fn, x0, t0, t1, dt)</code>\u00b6","text":"<p>Next, we need to write our integration routine. In a notebook environment like this one, it is easier to first write the code and then convert it into a function, such that step-by-step debugging becomes a simpler task.</p> <p>Thus, we'll first define our integration variables: the initial state, the initial time, the final time and the timestep.</p>"},{"location":"01-simple-integration-routines/#plotting-results","title":"Plotting results\u00b6","text":"<p>First let's extract the times and the states from our list:</p>"},{"location":"01-simple-integration-routines/#wrapping-up-into-one-routine","title":"Wrapping up into one routine\u00b6","text":"<p>Let's put all the Euler method machinery into one function and look at how it behaves with a different system.</p>"},{"location":"01-simple-integration-routines/#a-more-complex-system","title":"A more complex system\u00b6","text":"<p>Let's now increase the complexity of our system by adding a dependency on <code>x</code>. A simple such system is $$ x(t)=x_0\\exp\\left[-t\\right] \\\\ \\frac{dx}{dt}(x,t)=-x $$</p>"},{"location":"01-simple-integration-routines/#the-classical-fourth-order-method","title":"The Classical Fourth-Order Method\u00b6","text":"<p>The Classical Fourth-Order Method has four stages, using the butcher tableau notation, we can write it as:</p> <p>$$ \\begin{array}{c|c} 0 \\\\ 0.5 &amp; 0.5 \\\\ 0.5 &amp; 0.0 &amp; 0.5\\\\ 1.0 &amp; 0.0 &amp; 0.0 &amp; 1.0 \\\\ \\hline &amp; 0.1\\bar{6} &amp; 0.3\\bar{3} &amp; 0.3\\bar{3} &amp; 0.1\\bar{6} \\\\ \\end{array} $$</p> <p>We take the same function from before, and modify it to account for the intermediate stages:</p>"},{"location":"01-simple-integration-routines/#wrapping-up","title":"Wrapping Up\u00b6","text":"<p>PyTorch uses the <code>torch.autograd.Function</code> class to define various functions and their gradients. This framework will also allow us to easily compose integrations as well as use the <code>torch.autograd.gradcheck</code> function to validate our implementation.</p> <p>A good primer on how to correctly extend <code>torch.autograd.Function</code> can be found in the official documentation here: https://pytorch.org/docs/stable/notes/extending.html#extending-autograd</p>"},{"location":"02-arbitrary-adaptive-tableaus/","title":"Arbitrary Butcher Tableaus and Adaptive Integration","text":"In\u00a0[1]: Copied! <pre>import typing\nimport warnings\n\nimport torch\nimport einops\nimport neuralode\n\nwarnings.simplefilter('once', RuntimeWarning)\n</pre> import typing import warnings  import torch import einops import neuralode  warnings.simplefilter('once', RuntimeWarning) In\u00a0[2]: Copied! <pre># For convenience, we define the default tensor device and dtype here\ntorch.set_default_device('cpu')\n# In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training\ntorch.set_default_dtype(torch.float64)\n</pre> # For convenience, we define the default tensor device and dtype here torch.set_default_device('cpu') # In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training torch.set_default_dtype(torch.float64) <p>We'll be starting with the same function from the previous notebook, but we have tidied up the code by using <code>neuralode.util.partial_compensated_sum</code> to track the truncated bits instead of duplicating the code through our integration function, and placed common elements in their own modularised functions.</p> <p>We will test that, with the modularised code, we can still reproduce the previous results on the same exponential system.</p> In\u00a0[3]: Copied! <pre>x0 = torch.tensor([1.0])\nt0 = torch.tensor(0.0)\nt1 = torch.tensor(1.0)\nN  = 10\ndt = (t1 - t0) / N\n</pre> x0 = torch.tensor([1.0]) t0 = torch.tensor(0.0) t1 = torch.tensor(1.0) N  = 10 dt = (t1 - t0) / N In\u00a0[4]: Copied! <pre>(f_state, f_time), sub_states = neuralode.integrators.IntegrateRK4.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt)\nreference_trajectory = [neuralode.dynamics.exponential_fn_solution(x0, t) for _,t in sub_states]\n\nfig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference(sub_states, reference_trajectory, method_label=\"Compensated RK4\")\n\nprint(f\"Error in RK4: {(f_state - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\")\n</pre> (f_state, f_time), sub_states = neuralode.integrators.IntegrateRK4.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt) reference_trajectory = [neuralode.dynamics.exponential_fn_solution(x0, t) for _,t in sub_states]  fig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference(sub_states, reference_trajectory, method_label=\"Compensated RK4\")  print(f\"Error in RK4: {(f_state - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\") <pre>Error in RK4: 3.3324105608301124e-07\n</pre> <p>Now that we have verified that the previous code is still correct and working with our code in the module, we can start parametrisation of the integrator as a function of the Butcher Tableau.</p> <p>The following code takes advantage of the dynamic type system in Python, more specifically, we can generate a class and define its methods dynamically as well as store the tableau itself as an attribute of the class. In this fashion, we can have a function that takes a tableau as a PyTorch tensor and returns a PyTorch function that can integrate a system.</p> <p>This will enable us to test out different integration algorithms without having to write a new class for each one. Eventually we will move this function into the module and add different integration algorithms at import time.</p> In\u00a0[5]: Copied! <pre>def get_integrator(integrator_tableau: torch.Tensor, integrator_name: str = None) -&gt; torch.autograd.Function:\n    __integrator_type = type(integrator_name, (torch.autograd.Function,), {\n        \"integrator_tableau\": integrator_tableau\n    })\n    def __internal_forward(ctx, fn, x0, t0, t1, dt):\n        \"\"\"\n        A general integration routine for solving an Initial Value Problem\n        using any arbitrary Butcher Tableau\n        \n        Instead of naively summing the changes, we use compensated summation.\n        \n        :param fn: the function to be integrated\n        :param initial_state: the initial state to integrate from\n        :param initial_time: the initial time to integrate from\n        :param final_time: the final time to integrate to\n        :param timestep: the time increments to integrate with \n        :return: a tuple of ((the final state, the final time), the intermediates states [list]) \n        \"\"\"\n        \n        butcher_tableau = __integrator_type.integrator_tableau.clone().to(x0.device, x0.dtype)\n        \n        # The names for the variables have been shortened for concision, and\n        # to avoid overlap with variables in the outer scope\n        # I have also left the annotations as they are invaluable for tracking the method.\n        c_time = t0.clone()\n        c_state = x0.clone()\n        \n        c_state, c_time, i_states = neuralode.integrators.routines.integrate_system(fn, c_state, c_time, t1, dt, butcher_tableau)\n        \n        ctx.save_for_backward((c_state, c_time, i_states))\n        ctx.integration_function = fn\n        \n        return (c_state, c_time), i_states        \n    __integrator_type.forward = staticmethod(__internal_forward)\n    return __integrator_type\n</pre> def get_integrator(integrator_tableau: torch.Tensor, integrator_name: str = None) -&gt; torch.autograd.Function:     __integrator_type = type(integrator_name, (torch.autograd.Function,), {         \"integrator_tableau\": integrator_tableau     })     def __internal_forward(ctx, fn, x0, t0, t1, dt):         \"\"\"         A general integration routine for solving an Initial Value Problem         using any arbitrary Butcher Tableau                  Instead of naively summing the changes, we use compensated summation.                  :param fn: the function to be integrated         :param initial_state: the initial state to integrate from         :param initial_time: the initial time to integrate from         :param final_time: the final time to integrate to         :param timestep: the time increments to integrate with          :return: a tuple of ((the final state, the final time), the intermediates states [list])          \"\"\"                  butcher_tableau = __integrator_type.integrator_tableau.clone().to(x0.device, x0.dtype)                  # The names for the variables have been shortened for concision, and         # to avoid overlap with variables in the outer scope         # I have also left the annotations as they are invaluable for tracking the method.         c_time = t0.clone()         c_state = x0.clone()                  c_state, c_time, i_states = neuralode.integrators.routines.integrate_system(fn, c_state, c_time, t1, dt, butcher_tableau)                  ctx.save_for_backward((c_state, c_time, i_states))         ctx.integration_function = fn                  return (c_state, c_time), i_states             __integrator_type.forward = staticmethod(__internal_forward)     return __integrator_type In\u00a0[6]: Copied! <pre>explicit_rk4_integrator = get_integrator(torch.tensor([\n            # c0, a00, a01, a02, a03\n            [0.0, 0.0, 0.0, 0.0, 0.0],\n            # c1, a10, a11, a12, a13\n            [0.5, 0.5, 0.0, 0.0, 0.0],\n            # c2, a20, a21, a22, a23\n            [0.5, 0.0, 0.5, 0.0, 0.0],\n            # c3, a30, a31, a32, a33\n            [1.0, 0.0, 0.0, 1.0, 0.0],\n            #     b0,  b1,  b2,  b3\n            [0.0, 1/6, 2/6, 2/6, 1/6]\n        ], dtype=torch.float64), integrator_name = \"ExplicitRungeKutta4\")\nexplicit_midpoint_integrator = get_integrator(torch.tensor([\n            # c0, a00, a01\n            [0.0, 0.0, 0.0],\n            # c1, a10, a11\n            [0.5, 0.5, 0.0],\n            #     b0,  b1\n            [0.0, 0.0, 1.0]\n        ], dtype=torch.float64), integrator_name = \"ExplicitMidpoint\")\n</pre> explicit_rk4_integrator = get_integrator(torch.tensor([             # c0, a00, a01, a02, a03             [0.0, 0.0, 0.0, 0.0, 0.0],             # c1, a10, a11, a12, a13             [0.5, 0.5, 0.0, 0.0, 0.0],             # c2, a20, a21, a22, a23             [0.5, 0.0, 0.5, 0.0, 0.0],             # c3, a30, a31, a32, a33             [1.0, 0.0, 0.0, 1.0, 0.0],             #     b0,  b1,  b2,  b3             [0.0, 1/6, 2/6, 2/6, 1/6]         ], dtype=torch.float64), integrator_name = \"ExplicitRungeKutta4\") explicit_midpoint_integrator = get_integrator(torch.tensor([             # c0, a00, a01             [0.0, 0.0, 0.0],             # c1, a10, a11             [0.5, 0.5, 0.0],             #     b0,  b1             [0.0, 0.0, 1.0]         ], dtype=torch.float64), integrator_name = \"ExplicitMidpoint\") In\u00a0[7]: Copied! <pre>dt = (t1 - t0)/2000\n\n(f_state, f_time), sub_states = explicit_midpoint_integrator.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt)\nreference_trajectory = [neuralode.dynamics.exponential_fn_solution(x0, t) for _,t in sub_states]\n\nprint(f\"Error in {explicit_midpoint_integrator}: {(f_state - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\")\n\nfig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference(sub_states, reference_trajectory, method_label=\"Midpoint Method\")\n\n(f_state, f_time), sub_states = explicit_rk4_integrator.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt)\nfig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference(sub_states, reference_trajectory, axes=axes, method_label=\"Runge-Kutta 4\")\n\nprint(f\"Error in {explicit_rk4_integrator}: {(f_state - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\")\n</pre> dt = (t1 - t0)/2000  (f_state, f_time), sub_states = explicit_midpoint_integrator.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt) reference_trajectory = [neuralode.dynamics.exponential_fn_solution(x0, t) for _,t in sub_states]  print(f\"Error in {explicit_midpoint_integrator}: {(f_state - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\")  fig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference(sub_states, reference_trajectory, method_label=\"Midpoint Method\")  (f_state, f_time), sub_states = explicit_rk4_integrator.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt) fig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference(sub_states, reference_trajectory, axes=axes, method_label=\"Runge-Kutta 4\")  print(f\"Error in {explicit_rk4_integrator}: {(f_state - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\") <pre>Error in &lt;class '__main__.ExplicitMidpoint'&gt;: 1.5334058689475683e-08\nError in &lt;class '__main__.ExplicitRungeKutta4'&gt;: 1.1102230246251565e-16\n</pre> <p>We can see that this simplifies testing of different integration methods, and we could even use it to optimise the tableau itself since PyTorch allows for differentiating tensor variables (but I digress). For now, we will focus on how to implement an adaptive integration scheme. To identify whether a tableau if adaptive, we can use the first column of the rows with the $b_i$ coefficients and put a <code>torch.inf</code> value in their place. As this value is ignored during integration, it will be a way of signalling that the rows with <code>torch.inf</code> are used as the $b_i$ coefficients.</p> <p>Let's implement this!</p> In\u00a0[8]: Copied! <pre>def compute_step_adaptive(fn, state, time, step, tableau):\n    # We need to store the intermediate stages\n    k_stages = torch.stack([torch.zeros_like(state)]*(tableau.shape[0]-2))\n    # we subtract one since the last row is the final state\n    for stage_index in range(k_stages.shape[0]):\n        c_coeff, *a_coeff = tableau[stage_index]\n        k_stages[stage_index] = fn(\n            # We use `compensated_sum` instead of `sum` to avoid truncation at each stage calculation\n            state + step * neuralode.util.compensated_sum(k*a for k,a in zip(k_stages, a_coeff)), \n            time + c_coeff * step\n        )\n    lower_order_estimate = step * neuralode.util.compensated_sum(k*b for k,b in zip(k_stages, tableau[-1, 1:]))\n    higher_order_estimate = step * neuralode.util.compensated_sum(k*b for k,b in zip(k_stages, tableau[-2, 1:]))\n    # From a numerical perspective, this implementation is not necessarily ideal as\n    # we can lose precision when subtracting the two solutions. A more numerically accurate\n    # implementation would have one row `b_i` coefficients and another row the coefficients\n    # for computing the error directly\n    return lower_order_estimate, higher_order_estimate, step\n\ndef integrate_system_adaptive(fn: typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n                              c_state: torch.Tensor,\n                              c_time: torch.Tensor,\n                              final_time: torch.Tensor,\n                              dt: torch.Tensor,\n                              atol: torch.Tensor,\n                              rtol: torch.Tensor,\n                              is_adaptive: bool,\n                              use_local_extrapolation: bool,\n                              integrator_order: int,\n                              tableau: torch.Tensor):\n    \n    i_states = [(c_state.clone(), c_time.clone())]\n    error_in_state = [torch.zeros(tuple(), device=x0.device, dtype=x0.dtype)]\n    \n    c_state, truncated_bits_state = neuralode.util.partial_compensated_sum(c_state)\n    c_time,  truncated_bits_time  = neuralode.util.partial_compensated_sum(c_time)\n    \n    if is_adaptive:\n        compute_step = compute_step_adaptive\n    else:\n        compute_step = neuralode.integrators.helpers.compute_step\n    \n    while torch.any((c_time + dt) &lt; final_time):\n        delta_state_lower, delta_state_upper, delta_time = compute_step(fn, c_state + truncated_bits_state, c_time + truncated_bits_time, dt, tableau)\n        # If local extrapolation is enabled, we take the higher order estimate, otherwise the lower order one\n        delta_state = delta_state_upper if use_local_extrapolation else delta_state_lower\n        \n        # We use `torch.linalg.norm` to compute the magnitude of the error\n        # we can adjust this by passing in the `ord` keyword to choose a different\n        # vector norm, but the 2-norm suffices for our purposes\n        # We also detach just in case since this variable should not be differentiated\n        current_error = torch.linalg.norm(delta_state_upper - delta_state_lower).detach()\n        if is_adaptive:\n            # To save on computation, we only compute the max error tolerated and the step \n            # correction when the method is adaptive\n            max_error = (atol + torch.linalg.norm(rtol * c_state)).detach()\n            step_correction = 0.8*torch.where(current_error != 0.0, max_error/current_error, 1.0)**(1/integrator_order)\n            # Based on the error, we correct the step size\n            dt = step_correction*dt\n            if current_error &gt;= max_error:\n                # If the error exceeds our error threshold, we don't commit the step and redo it\n                continue\n            else:\n                error_in_state.append(current_error)\n        c_state, truncated_bits_state = neuralode.util.partial_compensated_sum(delta_state, (c_state, truncated_bits_state))\n        c_time,  truncated_bits_time  = neuralode.util.partial_compensated_sum(delta_time, (c_time, truncated_bits_time))\n        \n        i_states.append((c_state + truncated_bits_state, c_time + truncated_bits_time))\n        \n    delta_state_lower, delta_state_upper, delta_time = compute_step(fn, c_state + truncated_bits_state, c_time + truncated_bits_time, (final_time - c_time) - truncated_bits_time, tableau)\n    delta_state = delta_state_upper if use_local_extrapolation else delta_state_lower\n    c_state, truncated_bits_state = neuralode.util.partial_compensated_sum(delta_state, (c_state, truncated_bits_state))\n    c_time,  truncated_bits_time  = neuralode.util.partial_compensated_sum(delta_time, (c_time, truncated_bits_time))\n            \n    error_in_state.append(torch.linalg.norm(delta_state_upper - delta_state_lower))\n    i_states.append((c_state + truncated_bits_state, c_time + truncated_bits_time))\n    \n    return c_state, c_time, i_states, error_in_state\n</pre> def compute_step_adaptive(fn, state, time, step, tableau):     # We need to store the intermediate stages     k_stages = torch.stack([torch.zeros_like(state)]*(tableau.shape[0]-2))     # we subtract one since the last row is the final state     for stage_index in range(k_stages.shape[0]):         c_coeff, *a_coeff = tableau[stage_index]         k_stages[stage_index] = fn(             # We use `compensated_sum` instead of `sum` to avoid truncation at each stage calculation             state + step * neuralode.util.compensated_sum(k*a for k,a in zip(k_stages, a_coeff)),              time + c_coeff * step         )     lower_order_estimate = step * neuralode.util.compensated_sum(k*b for k,b in zip(k_stages, tableau[-1, 1:]))     higher_order_estimate = step * neuralode.util.compensated_sum(k*b for k,b in zip(k_stages, tableau[-2, 1:]))     # From a numerical perspective, this implementation is not necessarily ideal as     # we can lose precision when subtracting the two solutions. A more numerically accurate     # implementation would have one row `b_i` coefficients and another row the coefficients     # for computing the error directly     return lower_order_estimate, higher_order_estimate, step  def integrate_system_adaptive(fn: typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor],                               c_state: torch.Tensor,                               c_time: torch.Tensor,                               final_time: torch.Tensor,                               dt: torch.Tensor,                               atol: torch.Tensor,                               rtol: torch.Tensor,                               is_adaptive: bool,                               use_local_extrapolation: bool,                               integrator_order: int,                               tableau: torch.Tensor):          i_states = [(c_state.clone(), c_time.clone())]     error_in_state = [torch.zeros(tuple(), device=x0.device, dtype=x0.dtype)]          c_state, truncated_bits_state = neuralode.util.partial_compensated_sum(c_state)     c_time,  truncated_bits_time  = neuralode.util.partial_compensated_sum(c_time)          if is_adaptive:         compute_step = compute_step_adaptive     else:         compute_step = neuralode.integrators.helpers.compute_step          while torch.any((c_time + dt) &lt; final_time):         delta_state_lower, delta_state_upper, delta_time = compute_step(fn, c_state + truncated_bits_state, c_time + truncated_bits_time, dt, tableau)         # If local extrapolation is enabled, we take the higher order estimate, otherwise the lower order one         delta_state = delta_state_upper if use_local_extrapolation else delta_state_lower                  # We use `torch.linalg.norm` to compute the magnitude of the error         # we can adjust this by passing in the `ord` keyword to choose a different         # vector norm, but the 2-norm suffices for our purposes         # We also detach just in case since this variable should not be differentiated         current_error = torch.linalg.norm(delta_state_upper - delta_state_lower).detach()         if is_adaptive:             # To save on computation, we only compute the max error tolerated and the step              # correction when the method is adaptive             max_error = (atol + torch.linalg.norm(rtol * c_state)).detach()             step_correction = 0.8*torch.where(current_error != 0.0, max_error/current_error, 1.0)**(1/integrator_order)             # Based on the error, we correct the step size             dt = step_correction*dt             if current_error &gt;= max_error:                 # If the error exceeds our error threshold, we don't commit the step and redo it                 continue             else:                 error_in_state.append(current_error)         c_state, truncated_bits_state = neuralode.util.partial_compensated_sum(delta_state, (c_state, truncated_bits_state))         c_time,  truncated_bits_time  = neuralode.util.partial_compensated_sum(delta_time, (c_time, truncated_bits_time))                  i_states.append((c_state + truncated_bits_state, c_time + truncated_bits_time))              delta_state_lower, delta_state_upper, delta_time = compute_step(fn, c_state + truncated_bits_state, c_time + truncated_bits_time, (final_time - c_time) - truncated_bits_time, tableau)     delta_state = delta_state_upper if use_local_extrapolation else delta_state_lower     c_state, truncated_bits_state = neuralode.util.partial_compensated_sum(delta_state, (c_state, truncated_bits_state))     c_time,  truncated_bits_time  = neuralode.util.partial_compensated_sum(delta_time, (c_time, truncated_bits_time))                  error_in_state.append(torch.linalg.norm(delta_state_upper - delta_state_lower))     i_states.append((c_state + truncated_bits_state, c_time + truncated_bits_time))          return c_state, c_time, i_states, error_in_state In\u00a0[9]: Copied! <pre>def get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True, integrator_name: str = None) -&gt; torch.autograd.Function:\n    # We look at the first column of the last two rows, and if both are `inf`, we know the method is adaptive\n    is_adaptive = torch.isinf(integrator_tableau[-1,0]) and torch.isinf(integrator_tableau[-2,0])\n    # The number of stages is the number of rows minus the last row\n    # (or last two rows if the method is adaptive)\n    number_of_stages = integrator_tableau.shape[0] - 1\n    if is_adaptive:\n        number_of_stages -= 1\n    # The `type` function in this form works to dynamically create a class\n    # the first parameter is the class name, the second are parent classes,\n    # and the last are the class attributes. We store the integrator attributes \n    # here, and reference them in the integration code.\n    # In this way, we can query these parameters at a future point.\n    __integrator_type = type(integrator_name, (torch.autograd.Function,), {\n        \"integrator_tableau\": integrator_tableau,\n        \"integrator_order\": integrator_order,\n        \"is_adaptive\": is_adaptive,\n        \"number_of_stages\": number_of_stages\n    })\n    def __internal_forward(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor], \n                           x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor,\n                           atol: torch.Tensor, rtol: torch.Tensor, *additional_dynamic_args):\n        \"\"\"\n        A general integration routine for solving an Initial Value Problem\n        using any arbitrary Butcher Tableau\n        \n        Instead of naively summing the changes, we use compensated summation.\n        \n        :param fn: the function to be integrated\n        :param initial_state: the initial state to integrate from\n        :param initial_time: the initial time to integrate from\n        :param final_time: the final time to integrate to\n        :param timestep: the time increments to integrate with\n        :param atol: The absolute tolerance for the error in an adaptive integration\n        :param rtol: The relative tolerance for the error in an adaptive integration\n        :param additional_dynamic_args: additional arguments to pass to the function\n        :return: a tuple of ((the final state, the final time), the intermediate states [list[torch.Tensor]], the error values [list[torch.Tensor]]) \n        \"\"\"        \n        if __integrator_type.is_adaptive:\n            # We need to check that both `atol` and `rtol` are valid values and are compatible with the state\n            atol = neuralode.integrators.helpers.ensure_tolerance(atol, x0, \"Absolute tolerance\", \"atol\")\n            rtol = neuralode.integrators.helpers.ensure_tolerance(rtol, x0, \"Relative tolerance\", \"rtol\")\n        \n        dt = neuralode.integrators.helpers.ensure_timestep(dt, t0, t1)\n        \n        butcher_tableau = __integrator_type.integrator_tableau.clone().to(x0.device, x0.dtype)\n        \n        def forward_fn(state, time):\n            return fn(state, time, *additional_dynamic_args)\n        \n        c_state = x0.clone()\n        c_time = t0.clone()\n        \n        c_state, c_time, i_states, error_in_state = integrate_system_adaptive(forward_fn, c_state, c_time, t1, dt, atol, rtol, \n                                                                              __integrator_type.is_adaptive, use_local_extrapolation,\n                                                                              __integrator_type.integrator_order, butcher_tableau)\n        \n        # We save parameters for the backward pass, but these won't be used \n        # until we implement the adjoint method for backpropagation\n        ctx.save_for_backward(c_state, c_time, *additional_dynamic_args, *[i[0] for i in i_states], *[i[1] for i in i_states])\n        ctx.integration_function = fn\n        \n        return (c_state, c_time), i_states, error_in_state\n    \n    if not __integrator_type.is_adaptive:\n        # If the method isn't adaptive, neither atol nor rtol are required, but because of\n        # how `torch.autograd.Function` works, we cannot have keyword arguments\n        # For that reason, we use an alternative implementation to fill those values with a stub\n        def __internal_forward_nonadaptive(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor], \n                                           x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor, *additional_dynamic_args):\n            return __internal_forward(ctx, fn, x0, t0, t1, dt, torch.inf, torch.inf, *additional_dynamic_args)\n        __integrator_type.forward = staticmethod(__internal_forward_nonadaptive)\n    else:\n        __integrator_type.forward = staticmethod(__internal_forward)\n    return __integrator_type\n</pre> def get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True, integrator_name: str = None) -&gt; torch.autograd.Function:     # We look at the first column of the last two rows, and if both are `inf`, we know the method is adaptive     is_adaptive = torch.isinf(integrator_tableau[-1,0]) and torch.isinf(integrator_tableau[-2,0])     # The number of stages is the number of rows minus the last row     # (or last two rows if the method is adaptive)     number_of_stages = integrator_tableau.shape[0] - 1     if is_adaptive:         number_of_stages -= 1     # The `type` function in this form works to dynamically create a class     # the first parameter is the class name, the second are parent classes,     # and the last are the class attributes. We store the integrator attributes      # here, and reference them in the integration code.     # In this way, we can query these parameters at a future point.     __integrator_type = type(integrator_name, (torch.autograd.Function,), {         \"integrator_tableau\": integrator_tableau,         \"integrator_order\": integrator_order,         \"is_adaptive\": is_adaptive,         \"number_of_stages\": number_of_stages     })     def __internal_forward(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor],                             x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor,                            atol: torch.Tensor, rtol: torch.Tensor, *additional_dynamic_args):         \"\"\"         A general integration routine for solving an Initial Value Problem         using any arbitrary Butcher Tableau                  Instead of naively summing the changes, we use compensated summation.                  :param fn: the function to be integrated         :param initial_state: the initial state to integrate from         :param initial_time: the initial time to integrate from         :param final_time: the final time to integrate to         :param timestep: the time increments to integrate with         :param atol: The absolute tolerance for the error in an adaptive integration         :param rtol: The relative tolerance for the error in an adaptive integration         :param additional_dynamic_args: additional arguments to pass to the function         :return: a tuple of ((the final state, the final time), the intermediate states [list[torch.Tensor]], the error values [list[torch.Tensor]])          \"\"\"                 if __integrator_type.is_adaptive:             # We need to check that both `atol` and `rtol` are valid values and are compatible with the state             atol = neuralode.integrators.helpers.ensure_tolerance(atol, x0, \"Absolute tolerance\", \"atol\")             rtol = neuralode.integrators.helpers.ensure_tolerance(rtol, x0, \"Relative tolerance\", \"rtol\")                  dt = neuralode.integrators.helpers.ensure_timestep(dt, t0, t1)                  butcher_tableau = __integrator_type.integrator_tableau.clone().to(x0.device, x0.dtype)                  def forward_fn(state, time):             return fn(state, time, *additional_dynamic_args)                  c_state = x0.clone()         c_time = t0.clone()                  c_state, c_time, i_states, error_in_state = integrate_system_adaptive(forward_fn, c_state, c_time, t1, dt, atol, rtol,                                                                                __integrator_type.is_adaptive, use_local_extrapolation,                                                                               __integrator_type.integrator_order, butcher_tableau)                  # We save parameters for the backward pass, but these won't be used          # until we implement the adjoint method for backpropagation         ctx.save_for_backward(c_state, c_time, *additional_dynamic_args, *[i[0] for i in i_states], *[i[1] for i in i_states])         ctx.integration_function = fn                  return (c_state, c_time), i_states, error_in_state          if not __integrator_type.is_adaptive:         # If the method isn't adaptive, neither atol nor rtol are required, but because of         # how `torch.autograd.Function` works, we cannot have keyword arguments         # For that reason, we use an alternative implementation to fill those values with a stub         def __internal_forward_nonadaptive(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor],                                             x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor, *additional_dynamic_args):             return __internal_forward(ctx, fn, x0, t0, t1, dt, torch.inf, torch.inf, *additional_dynamic_args)         __integrator_type.forward = staticmethod(__internal_forward_nonadaptive)     else:         __integrator_type.forward = staticmethod(__internal_forward)     return __integrator_type In\u00a0[10]: Copied! <pre>adaptive_rk45_integrator = get_integrator(torch.tensor([\n    [0.0,       0.0,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [1/5,       1/5,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [3/10,      3/40,        9/40,       0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [4/5,       44/45,      -56/15,      32/9,        0.0,      0.0,          0.0,      0.0 ],\n    [8/9,       19372/6561, -25360/2187, 64448/6561, -212/729,  0.0,          0.0,      0.0 ],\n    [1.0,       9017/3168,  -355/33,     46732/5247,  49/176,  -5103/18656,   0.0,      0.0 ],\n    [1.0,       35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],\n    [torch.inf, 35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],\n    [torch.inf, 5179/57600,  0.0,        7571/16695,  393/640, -92097/339200, 187/2100, 1/40]\n], dtype=torch.float64), integrator_order = 5, use_local_extrapolation = False, integrator_name = \"AdaptiveRK45Integrator\")\n</pre> adaptive_rk45_integrator = get_integrator(torch.tensor([     [0.0,       0.0,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],     [1/5,       1/5,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],     [3/10,      3/40,        9/40,       0.0,         0.0,      0.0,          0.0,      0.0 ],     [4/5,       44/45,      -56/15,      32/9,        0.0,      0.0,          0.0,      0.0 ],     [8/9,       19372/6561, -25360/2187, 64448/6561, -212/729,  0.0,          0.0,      0.0 ],     [1.0,       9017/3168,  -355/33,     46732/5247,  49/176,  -5103/18656,   0.0,      0.0 ],     [1.0,       35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],     [torch.inf, 35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],     [torch.inf, 5179/57600,  0.0,        7571/16695,  393/640, -92097/339200, 187/2100, 1/40] ], dtype=torch.float64), integrator_order = 5, use_local_extrapolation = False, integrator_name = \"AdaptiveRK45Integrator\") <p>Having demonstrated an adaptive integrator with an arbitrary butcher tableau, we are now ready to try higher dimensional systems and implement backpropagation through our algorithm.</p> <p>First, let's write the dynamics of a system where a parameter can control the different behaviours. A harmonic oscillator is the ideal candidate as its solution can be determined exactly (if effort is expended) and has physically intuitive parameters. From physics, we know that the dynamical equation for a harmonic oscillator is:</p> <p>$$ \\vec{F}=-k\\vec{x}-c\\vec{x}^{(1)} $$</p> <p>And using the good old $\\vec{F}=m\\vec{a}$ and $\\vec{a}=\\vec{x}^{(2)}$ we can rewrite our equations as:</p> <p>$$ \\vec{x}^{(2)}=-\\frac{k}{m}\\vec{x}-\\frac{c}{m}\\vec{x}^{(1)} $$</p> <p>This equation is written with arbitrary spatial dimensions in mind, and is a second order equation. For our purposes, we can treat this as having a single spatial dimension and convert it into a one-dimensional system using the methods we discussed in the introduction.</p> <p>Let's introduce the variable $v=x^{(1)}$ which will allow us to write:</p> <p>$$ \\begin{bmatrix}     x^{(1)} \\\\     v^{(1)} \\end{bmatrix} = \\begin{bmatrix} v \\\\ -\\frac{k}{m}x-\\frac{c}{m}v \\end{bmatrix} $$</p> <p>As physicists like to introduce quantities that are more intuitive, we'll rewrite the equation as:</p> <p>$$ \\begin{bmatrix}     x^{(1)} \\\\     v^{(1)} \\end{bmatrix} = \\begin{bmatrix} v \\\\ -\\omega^2x-2\\zeta\\omega v \\end{bmatrix} $$</p> <p>Using the definition of $\\omega = \\sqrt{\\frac{k}{m}}$, corresponding to the undamped angular frequency, and $\\zeta=\\frac{c}{2\\sqrt{mk}$, corresponding to the damping ratio.</p> <p>This is a linear equation in which we can write it as the original vector multiplied by some matrix as follows:</p> <p>$$ \\begin{bmatrix}     x^{(1)} \\\\     v^{(1)} \\end{bmatrix} =  \\mathbf{A} \\begin{bmatrix}     x \\\\     v \\end{bmatrix} $$</p> <p>where</p> <p>$$ \\mathbf{A} = \\begin{bmatrix}     0 &amp; 1 \\\\     -\\omega^2 &amp; -2\\zeta\\omega \\end{bmatrix} $$</p> <p>With this, we now have a system where some parameters control the dynamics, and we can ask questions like \"Given some samples of this oscillator, can we estimate these parameters from numerical integration?\"</p> In\u00a0[11]: Copied! <pre>def sha_dynamics(x, t, frequency, damping):\n    # The dynamics above can easily be represented as a matrix multiplication\n    # First the matrix with the corresponding terms\n    A = torch.stack([\n        # no x term for the derivative of x as it is equal to v\n        torch.stack([torch.zeros_like(frequency), torch.ones_like(frequency)], dim=-1),\n        # first we have the omega^2 term, then the 2*zeta*omega term\n        torch.stack([-frequency**2, -2*frequency*damping], dim=-1),\n    ], dim=-2)\n    # We implement the matrix multiplication using einops\n    # This is not necessarily the most efficient, but it allows\n    # us to track the exact operation without worrying about the shapes\n    # of our tensors too much\n    # You can read '...,ij,...j-&gt;...i' as:\n    #   - The first argument is a tensor with arbitrary dimensions, but\n    #   the last two of which are of interest, labelled as 'i' and 'j'\n    #   - The second argument is a tensor with arbitrary dimensions, but\n    #   the last of which is commensurate with the number of rows of the input matrix\n    #   - Take the sum of A[...,i,j]*x[...,j] over all 'j' and the output will be indexed \n    #   by 'i' in the last dimension  \n    return einops.einsum(A, x, '... row col,... col-&gt;... row')\n</pre> def sha_dynamics(x, t, frequency, damping):     # The dynamics above can easily be represented as a matrix multiplication     # First the matrix with the corresponding terms     A = torch.stack([         # no x term for the derivative of x as it is equal to v         torch.stack([torch.zeros_like(frequency), torch.ones_like(frequency)], dim=-1),         # first we have the omega^2 term, then the 2*zeta*omega term         torch.stack([-frequency**2, -2*frequency*damping], dim=-1),     ], dim=-2)     # We implement the matrix multiplication using einops     # This is not necessarily the most efficient, but it allows     # us to track the exact operation without worrying about the shapes     # of our tensors too much     # You can read '...,ij,...j-&gt;...i' as:     #   - The first argument is a tensor with arbitrary dimensions, but     #   the last two of which are of interest, labelled as 'i' and 'j'     #   - The second argument is a tensor with arbitrary dimensions, but     #   the last of which is commensurate with the number of rows of the input matrix     #   - Take the sum of A[...,i,j]*x[...,j] over all 'j' and the output will be indexed      #   by 'i' in the last dimension       return einops.einsum(A, x, '... row col,... col-&gt;... row') <p>Before we had to define only one initial state for the system, but since we now have two dimensions, there are two initial conditions to specify. These can physically be interpreted as the initial position of a spring and the initial velocity of a spring.</p> In\u00a0[12]: Copied! <pre>initial_position = torch.tensor(1.0)\ninitial_velocity = torch.tensor(0.0)\n\nfrequency = torch.ones_like(initial_position)\ndamping = torch.ones_like(initial_position)*0.25\ninitial_x = torch.stack([\n    initial_position,\n    initial_velocity,\n], dim=-1)\n\ninitial_time = torch.tensor(0.0)\nfinal_time   = torch.tensor(25.0)\n\ninitial_timestep = (final_time - initial_time) / 10\n\natol, rtol = torch.tensor(5e-8), torch.tensor(5e-8)\n</pre> initial_position = torch.tensor(1.0) initial_velocity = torch.tensor(0.0)  frequency = torch.ones_like(initial_position) damping = torch.ones_like(initial_position)*0.25 initial_x = torch.stack([     initial_position,     initial_velocity, ], dim=-1)  initial_time = torch.tensor(0.0) final_time   = torch.tensor(25.0)  initial_timestep = (final_time - initial_time) / 10  atol, rtol = torch.tensor(5e-8), torch.tensor(5e-8) In\u00a0[13]: Copied! <pre>_, sha_states, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, frequency, damping)\n</pre> _, sha_states, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, frequency, damping) In\u00a0[14]: Copied! <pre>fig, axes = neuralode.plot.trajectory.plot_trajectory(sha_states, method_label=\"RK4(5) - Simple Harmonic Oscillator\")\n</pre> fig, axes = neuralode.plot.trajectory.plot_trajectory(sha_states, method_label=\"RK4(5) - Simple Harmonic Oscillator\") <p>Excellent! We can see that we've implemented the oscillator correctly, and it is oscillating and decaying as expected. Now, let's try something interesting with this, let's assume we don't know the frequency and the damping factor, but we do know the initial state, and we would like to estimate the frequency/damping based on the previously computed trajectory.</p> <p>This requires splitting the problem into several parts:</p> <ul> <li>First, we must integrate our system with the guesses to each point in time that we've solved the system above.</li> <li>Second, we must compute an error at each point in time</li> <li>Third, we must compute the gradients of the error with respect to our parameters</li> <li>Fourth, we must update our parameters based on the gradients</li> </ul> <p>We'll write the code for the first part, essentially we'll create a few parameters that we'll be optimising</p> In\u00a0[15]: Copied! <pre># You'll note that we set requires_grad=True to let PyTorch know that we want to track the operations\n# involving these variables and compute their gradients\noptimised_frequency = torch.tensor(0.1, requires_grad=True)\noptimised_damping = torch.tensor(1.0, requires_grad=True)\n</pre> # You'll note that we set requires_grad=True to let PyTorch know that we want to track the operations # involving these variables and compute their gradients optimised_frequency = torch.tensor(0.1, requires_grad=True) optimised_damping = torch.tensor(1.0, requires_grad=True) <p>Second, we'll need to write some code that integrates our values piecewise to each point in time we're interested in:</p> In\u00a0[16]: Copied! <pre>times_to_integrate = [time for _, time in sha_states][1:]\n# We skip the first time as we know the initial state\ncurrent_state = initial_x.clone()\ncurrent_time  = initial_time.clone()\n\n## DETAIL THE ERROR FUNCTION AND HOW THAT'S USED\n# Let's look at the first sample_time and see what output we get\nfor sample_index, sample_time in enumerate(times_to_integrate[:1]):\n    (current_state, current_time), _, _ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, dt, atol, rtol, optimised_frequency, optimised_damping)\n    error = torch.linalg.norm(sha_states[sample_index][0] - current_state)\n    print(f\"The current state: {current_state}\")\n    print(f\"Error at time {current_time} is {error}\")\n</pre> times_to_integrate = [time for _, time in sha_states][1:] # We skip the first time as we know the initial state current_state = initial_x.clone() current_time  = initial_time.clone()  ## DETAIL THE ERROR FUNCTION AND HOW THAT'S USED # Let's look at the first sample_time and see what output we get for sample_index, sample_time in enumerate(times_to_integrate[:1]):     (current_state, current_time), _, _ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, dt, atol, rtol, optimised_frequency, optimised_damping)     error = torch.linalg.norm(sha_states[sample_index][0] - current_state)     print(f\"The current state: {current_state}\")     print(f\"Error at time {current_time} is {error}\") <pre>The current state: tensor([ 1.0000e+00, -4.9998e-06])\nError at time 0.0005 is 4.999750162497297e-06\n</pre> <p>Now let's look at how pytorch represents operations whose gradients is <code>taped</code>:</p> In\u00a0[17]: Copied! <pre>optimised_frequency*2\n</pre> optimised_frequency*2 Out[17]: <pre>tensor(0.2000, grad_fn=&lt;MulBackward0&gt;)</pre> <p>It seems that the output from our integrator is missing the appropriate <code>grad_fn</code> which should point to our integrator. Looking into the documentation and reading this issue we see that <code>torch.autograd.Function</code> requires outputs to be flat tensors and not nested inside python structures like lists and tuples. This means we need to rework our code so that the output is compatible with PyTorch's gradient tracking.</p> In\u00a0[18]: Copied! <pre>def get_forward_method(integrator_type, use_local_extrapolation):\n    def __internal_forward(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor], \n                           x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor,\n                           atol: torch.Tensor, rtol: torch.Tensor, *additional_dynamic_args):\n        \"\"\"\n        A general integration routine for solving an Initial Value Problem\n        using any arbitrary Butcher Tableau\n        \n        Instead of naively summing the changes, we use compensated summation.\n        \n        :param fn: the function to be integrated\n        :param initial_state: the initial state to integrate from\n        :param initial_time: the initial time to integrate from\n        :param final_time: the final time to integrate to\n        :param timestep: the time increments to integrate with\n        :param atol: The absolute tolerance for the error in an adaptive integration\n        :param rtol: The relative tolerance for the error in an adaptive integration\n        :param additional_dynamic_args: additional arguments to pass to the function\n        :return: a tuple of ((the final state, the final time), the intermediate states [list[torch.Tensor]], the error values [list[torch.Tensor]]) \n        \"\"\"      \n        if integrator_type.is_adaptive:\n            # We need to check that both `atol` and `rtol` are valid values and are compatible with the state\n            atol = neuralode.integrators.helpers.ensure_tolerance(atol, x0, \"Absolute tolerance\", \"atol\")\n            rtol = neuralode.integrators.helpers.ensure_tolerance(rtol, x0, \"Relative tolerance\", \"rtol\")\n        \n        dt = neuralode.integrators.helpers.ensure_timestep(dt, t0, t1)\n        \n        butcher_tableau = integrator_type.integrator_tableau.clone().to(x0.device, x0.dtype)\n        \n        def forward_fn(state, time):\n            return fn(state, time, *additional_dynamic_args)\n        \n        c_state = x0.clone()\n        c_time = t0.clone()\n        \n        c_state, c_time, i_states, error_in_state = integrate_system_adaptive(forward_fn, c_state, c_time, t1, dt, atol, rtol, \n                                                                              integrator_type.is_adaptive, use_local_extrapolation,\n                                                                              integrator_type.integrator_order, butcher_tableau)\n        \n        intermediate_states, intermediate_times = zip(*i_states)\n        \n        # As we said, these need to be converted to tensors for proper tracking\n        intermediate_states = torch.stack(intermediate_states, dim=0)\n        intermediate_times = torch.stack(intermediate_times, dim=0)\n        \n        # We should also put the errors we're returning into a tensor too\n        error_in_state = torch.stack(error_in_state, dim=0)\n        \n        # We save parameters for the backward pass, but these won't be used \n        # until we implement the adjoint method for backpropagation\n        if ctx is not None:\n            ctx.save_for_backward(x0, t0, t1, dt, atol, rtol, c_state, c_time, intermediate_states, intermediate_times, *additional_dynamic_args)\n            ctx.integration_function = fn\n        \n        # Now we're returning a flat structure where each element is a tensor, and so its gradients can be properly tracked\n        return c_state, c_time, intermediate_states, intermediate_times, error_in_state\n    return __internal_forward\n\n\ndef get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True, integrator_name: str = None) -&gt; torch.autograd.Function:\n    # We look at the first column of the last two rows, and if both are `inf`, we know the method is adaptive\n    is_adaptive = torch.isinf(integrator_tableau[-1,0]) and torch.isinf(integrator_tableau[-2,0])\n    # The number of stages is the number of rows minus the last row\n    # (or last two rows if the method is adaptive)\n    number_of_stages = integrator_tableau.shape[0] - 1\n    if is_adaptive:\n        number_of_stages -= 1\n    # The `type` function in this form works to dynamically create a class\n    # the first parameter is the class name, the second are parent classes,\n    # and the last are the class attributes. We store the integrator attributes \n    # here, and reference them in the integration code.\n    # In this way, we can query these parameters at a future point.\n    __integrator_type = type(integrator_name, (torch.autograd.Function,), {\n        \"integrator_tableau\": integrator_tableau,\n        \"integrator_order\": integrator_order,\n        \"is_adaptive\": is_adaptive,\n        \"number_of_stages\": number_of_stages\n    })\n    \n    __internal_forward = get_forward_method(__integrator_type, use_local_extrapolation)\n    \n    if not __integrator_type.is_adaptive:\n        # If the method isn't adaptive, neither atol nor rtol are required, but because of\n        # how `torch.autograd.Function` works, we cannot have keyword arguments\n        # For that reason, we use an alternative implementation to fill those values with a stub\n        def __internal_forward_nonadaptive(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor], \n                                           x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor, *additional_dynamic_args):\n            return __internal_forward(ctx, fn, x0, t0, t1, dt, torch.inf, torch.inf, *additional_dynamic_args)\n        __integrator_type.forward = staticmethod(__internal_forward_nonadaptive)\n    else:\n        __integrator_type.forward = staticmethod(__internal_forward)\n    return __integrator_type\n</pre> def get_forward_method(integrator_type, use_local_extrapolation):     def __internal_forward(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor],                             x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor,                            atol: torch.Tensor, rtol: torch.Tensor, *additional_dynamic_args):         \"\"\"         A general integration routine for solving an Initial Value Problem         using any arbitrary Butcher Tableau                  Instead of naively summing the changes, we use compensated summation.                  :param fn: the function to be integrated         :param initial_state: the initial state to integrate from         :param initial_time: the initial time to integrate from         :param final_time: the final time to integrate to         :param timestep: the time increments to integrate with         :param atol: The absolute tolerance for the error in an adaptive integration         :param rtol: The relative tolerance for the error in an adaptive integration         :param additional_dynamic_args: additional arguments to pass to the function         :return: a tuple of ((the final state, the final time), the intermediate states [list[torch.Tensor]], the error values [list[torch.Tensor]])          \"\"\"               if integrator_type.is_adaptive:             # We need to check that both `atol` and `rtol` are valid values and are compatible with the state             atol = neuralode.integrators.helpers.ensure_tolerance(atol, x0, \"Absolute tolerance\", \"atol\")             rtol = neuralode.integrators.helpers.ensure_tolerance(rtol, x0, \"Relative tolerance\", \"rtol\")                  dt = neuralode.integrators.helpers.ensure_timestep(dt, t0, t1)                  butcher_tableau = integrator_type.integrator_tableau.clone().to(x0.device, x0.dtype)                  def forward_fn(state, time):             return fn(state, time, *additional_dynamic_args)                  c_state = x0.clone()         c_time = t0.clone()                  c_state, c_time, i_states, error_in_state = integrate_system_adaptive(forward_fn, c_state, c_time, t1, dt, atol, rtol,                                                                                integrator_type.is_adaptive, use_local_extrapolation,                                                                               integrator_type.integrator_order, butcher_tableau)                  intermediate_states, intermediate_times = zip(*i_states)                  # As we said, these need to be converted to tensors for proper tracking         intermediate_states = torch.stack(intermediate_states, dim=0)         intermediate_times = torch.stack(intermediate_times, dim=0)                  # We should also put the errors we're returning into a tensor too         error_in_state = torch.stack(error_in_state, dim=0)                  # We save parameters for the backward pass, but these won't be used          # until we implement the adjoint method for backpropagation         if ctx is not None:             ctx.save_for_backward(x0, t0, t1, dt, atol, rtol, c_state, c_time, intermediate_states, intermediate_times, *additional_dynamic_args)             ctx.integration_function = fn                  # Now we're returning a flat structure where each element is a tensor, and so its gradients can be properly tracked         return c_state, c_time, intermediate_states, intermediate_times, error_in_state     return __internal_forward   def get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True, integrator_name: str = None) -&gt; torch.autograd.Function:     # We look at the first column of the last two rows, and if both are `inf`, we know the method is adaptive     is_adaptive = torch.isinf(integrator_tableau[-1,0]) and torch.isinf(integrator_tableau[-2,0])     # The number of stages is the number of rows minus the last row     # (or last two rows if the method is adaptive)     number_of_stages = integrator_tableau.shape[0] - 1     if is_adaptive:         number_of_stages -= 1     # The `type` function in this form works to dynamically create a class     # the first parameter is the class name, the second are parent classes,     # and the last are the class attributes. We store the integrator attributes      # here, and reference them in the integration code.     # In this way, we can query these parameters at a future point.     __integrator_type = type(integrator_name, (torch.autograd.Function,), {         \"integrator_tableau\": integrator_tableau,         \"integrator_order\": integrator_order,         \"is_adaptive\": is_adaptive,         \"number_of_stages\": number_of_stages     })          __internal_forward = get_forward_method(__integrator_type, use_local_extrapolation)          if not __integrator_type.is_adaptive:         # If the method isn't adaptive, neither atol nor rtol are required, but because of         # how `torch.autograd.Function` works, we cannot have keyword arguments         # For that reason, we use an alternative implementation to fill those values with a stub         def __internal_forward_nonadaptive(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor],                                             x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor, *additional_dynamic_args):             return __internal_forward(ctx, fn, x0, t0, t1, dt, torch.inf, torch.inf, *additional_dynamic_args)         __integrator_type.forward = staticmethod(__internal_forward_nonadaptive)     else:         __integrator_type.forward = staticmethod(__internal_forward)     return __integrator_type In\u00a0[19]: Copied! <pre>adaptive_rk45_integrator = get_integrator(adaptive_rk45_integrator.integrator_tableau, integrator_order = 5, use_local_extrapolation = False, integrator_name = \"AdaptiveRK45Integrator\")\n\ntimes_to_integrate = [time for _, time in sha_states][1:]\n# We skip the first time as we know the initial state\ncurrent_state = initial_x.clone()\ncurrent_time  = initial_time.clone()\nerror = 0.0\n\n# Let's look at the first sample_time and see what output we get\nfor sample_index, sample_time in enumerate(times_to_integrate[:1]):\n    current_state, current_time, *_ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, dt, atol, rtol, optimised_frequency, optimised_damping)\n    error = error + torch.linalg.norm(sha_states[sample_index][0] - current_state)/len(times_to_integrate)\n    print(f\"The current state: {current_state}\")\n    print(f\"Error at time {current_time} is {error}\")\n</pre> adaptive_rk45_integrator = get_integrator(adaptive_rk45_integrator.integrator_tableau, integrator_order = 5, use_local_extrapolation = False, integrator_name = \"AdaptiveRK45Integrator\")  times_to_integrate = [time for _, time in sha_states][1:] # We skip the first time as we know the initial state current_state = initial_x.clone() current_time  = initial_time.clone() error = 0.0  # Let's look at the first sample_time and see what output we get for sample_index, sample_time in enumerate(times_to_integrate[:1]):     current_state, current_time, *_ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, dt, atol, rtol, optimised_frequency, optimised_damping)     error = error + torch.linalg.norm(sha_states[sample_index][0] - current_state)/len(times_to_integrate)     print(f\"The current state: {current_state}\")     print(f\"Error at time {current_time} is {error}\") <pre>The current state: tensor([ 1.0000e+00, -4.9998e-06], grad_fn=&lt;AdaptiveRK45IntegratorBackward&gt;)\nError at time 0.0005 is 4.03205658265911e-08\n</pre> <p>Excellent! We now see that pytorch is correctly tracking our integration as part of the gradient tape. Let's try computing the gradients!</p> In\u00a0[20]: Copied! <pre>try:\n    error.backward()\nexcept NotImplementedError as e:\n    print(f\"Encountered exception: {e}\")\n</pre> try:     error.backward() except NotImplementedError as e:     print(f\"Encountered exception: {e}\") <pre>Encountered exception: You must implement either the backward or vjp method for your custom autograd.Function to use it with backward mode AD.\n</pre> <p>Ah yes, we haven't implemented a backward method yet so let's do that:</p> In\u00a0[21]: Copied! <pre>def get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True, integrator_name: str = None) -&gt; torch.autograd.Function:\n    # We look at the first column of the last two rows, and if both are `inf`, we know the method is adaptive\n    is_adaptive = torch.isinf(integrator_tableau[-1,0]) and torch.isinf(integrator_tableau[-2,0])\n    # The number of stages is the number of rows minus the last row\n    # (or last two rows if the method is adaptive)\n    number_of_stages = integrator_tableau.shape[0] - 1\n    if is_adaptive:\n        number_of_stages -= 1\n    # The `type` function in this form works to dynamically create a class\n    # the first parameter is the class name, the second are parent classes,\n    # and the last are the class attributes. We store the integrator attributes \n    # here, and reference them in the integration code.\n    # In this way, we can query these parameters at a future point.\n    __integrator_type = type(integrator_name, (torch.autograd.Function,), {\n        \"integrator_tableau\": integrator_tableau,\n        \"integrator_order\": integrator_order,\n        \"is_adaptive\": is_adaptive,\n        \"number_of_stages\": number_of_stages\n    })\n    \n    __internal_forward = get_forward_method(__integrator_type, use_local_extrapolation)\n    \n    def __internal_backward(ctx, d_c_state, d_c_time, d_intermediate_states, d_intermediate_times, d_error_in_state):\n        \"\"\"\n        This function computes the gradient of the input variables for `__internal_forward` by exploiting the fact\n        that PyTorch can track the whole graph of operations used to derive a specific result. Thus each time backward is called,\n        we compute the actual graph of operations and propagate derivatives through it. Unfortunately, this is an exceptionally\n        slow method of computation that also uses a lot of memory.\n        \n        This is implemented here as a demonstration of how we could compute gradients and how these are expected to be propagated back\n        to the autograd tape. \n        \n        :param ctx: \n        :param d_c_state: \n        :param d_c_time: \n        :param d_intermediate_states: \n        :param d_intermediate_times: \n        :param d_error_in_state: \n        :return: \n        \"\"\"\n        \n        # First we retrieve our integration function that we stored in `__internal_forward`\n        fn = ctx.integration_function\n        # Then we retrieve the input variables and clone them to avoid influencing them in the later operations\n        x0, t0, t1, dt, atol, rtol, _, _, _, _, *additional_dynamic_args = [i.clone().requires_grad_(True) for i in ctx.saved_tensors]\n        inputs = fn, x0, t0, t1, dt, atol, rtol, *additional_dynamic_args        \n        if any(ctx.needs_input_grad):\n            # We ensure that gradients are enabled so that autograd tracks the variable operations \n            with torch.enable_grad():\n                # And then we integrate our system with the tracking of operations.\n                # We pass in `None` for the `ctx` to avoid issues with __internal_forward attempting to call methods that we don't want to use\n                # In the adjoint method; this will not be an issue\n                c_state, c_time, intermediate_states, intermediate_times, error_in_state = __internal_forward(None, fn, x0, t0, t1, dt, atol, rtol, *additional_dynamic_args)\n            # We collate the outputs that we can compute gradients for\n            # with this method, we are restricted to the final state and time\n            outputs = c_state, c_time #, intermediate_states, intermediate_times, error_in_state\n            grad_outputs = d_c_state, d_c_time, d_intermediate_states, d_intermediate_times, d_error_in_state\n            \n            # We also only consider the input and output variables that actually have gradients enabled\n            inputs_with_grad = [i for idx, i in enumerate(inputs) if ctx.needs_input_grad[idx]]\n            outputs_with_grad = [idx for idx, i in enumerate(outputs) if i.grad_fn is not None]\n            \n            grad_of_inputs_with_grad = torch.autograd.grad([outputs[idx] for idx in outputs_with_grad], inputs_with_grad, grad_outputs=[grad_outputs[idx] for idx in outputs_with_grad], allow_unused=True, materialize_grads=True)\n        else:\n            grad_of_inputs_with_grad = None\n        # For each input we must return a gradient\n        # Interestingly, this also includes the function we passed in...\n        # We create a list of None values \n        # (this tells autograd that there is no gradient for those variables).\n        # And for each variable that does have a gradient, we fill the values in\n        # before returning the list\n        input_grads = [None for _ in range(len(inputs))]\n        if grad_of_inputs_with_grad:\n            for idx in range(len(inputs)):\n                if ctx.needs_input_grad[idx]:\n                    input_grads[idx], *grad_of_inputs_with_grad = grad_of_inputs_with_grad\n        return tuple(input_grads)\n            \n    \n    if not __integrator_type.is_adaptive:\n        # If the method isn't adaptive, neither atol nor rtol are required, but because of\n        # how `torch.autograd.Function` works, we cannot have keyword arguments\n        # For that reason, we use an alternative implementation to fill those values with a stub\n        def __internal_forward_nonadaptive(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor], \n                                           x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor, *additional_dynamic_args):\n            return __internal_forward(ctx, fn, x0, t0, t1, dt, torch.inf, torch.inf, *additional_dynamic_args)\n        __integrator_type.forward = staticmethod(__internal_forward_nonadaptive)\n    else:\n        __integrator_type.forward = staticmethod(__internal_forward)\n    __integrator_type.backward = staticmethod(__internal_backward)\n    return __integrator_type\n</pre> def get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True, integrator_name: str = None) -&gt; torch.autograd.Function:     # We look at the first column of the last two rows, and if both are `inf`, we know the method is adaptive     is_adaptive = torch.isinf(integrator_tableau[-1,0]) and torch.isinf(integrator_tableau[-2,0])     # The number of stages is the number of rows minus the last row     # (or last two rows if the method is adaptive)     number_of_stages = integrator_tableau.shape[0] - 1     if is_adaptive:         number_of_stages -= 1     # The `type` function in this form works to dynamically create a class     # the first parameter is the class name, the second are parent classes,     # and the last are the class attributes. We store the integrator attributes      # here, and reference them in the integration code.     # In this way, we can query these parameters at a future point.     __integrator_type = type(integrator_name, (torch.autograd.Function,), {         \"integrator_tableau\": integrator_tableau,         \"integrator_order\": integrator_order,         \"is_adaptive\": is_adaptive,         \"number_of_stages\": number_of_stages     })          __internal_forward = get_forward_method(__integrator_type, use_local_extrapolation)          def __internal_backward(ctx, d_c_state, d_c_time, d_intermediate_states, d_intermediate_times, d_error_in_state):         \"\"\"         This function computes the gradient of the input variables for `__internal_forward` by exploiting the fact         that PyTorch can track the whole graph of operations used to derive a specific result. Thus each time backward is called,         we compute the actual graph of operations and propagate derivatives through it. Unfortunately, this is an exceptionally         slow method of computation that also uses a lot of memory.                  This is implemented here as a demonstration of how we could compute gradients and how these are expected to be propagated back         to the autograd tape.                   :param ctx:          :param d_c_state:          :param d_c_time:          :param d_intermediate_states:          :param d_intermediate_times:          :param d_error_in_state:          :return:          \"\"\"                  # First we retrieve our integration function that we stored in `__internal_forward`         fn = ctx.integration_function         # Then we retrieve the input variables and clone them to avoid influencing them in the later operations         x0, t0, t1, dt, atol, rtol, _, _, _, _, *additional_dynamic_args = [i.clone().requires_grad_(True) for i in ctx.saved_tensors]         inputs = fn, x0, t0, t1, dt, atol, rtol, *additional_dynamic_args                 if any(ctx.needs_input_grad):             # We ensure that gradients are enabled so that autograd tracks the variable operations              with torch.enable_grad():                 # And then we integrate our system with the tracking of operations.                 # We pass in `None` for the `ctx` to avoid issues with __internal_forward attempting to call methods that we don't want to use                 # In the adjoint method; this will not be an issue                 c_state, c_time, intermediate_states, intermediate_times, error_in_state = __internal_forward(None, fn, x0, t0, t1, dt, atol, rtol, *additional_dynamic_args)             # We collate the outputs that we can compute gradients for             # with this method, we are restricted to the final state and time             outputs = c_state, c_time #, intermediate_states, intermediate_times, error_in_state             grad_outputs = d_c_state, d_c_time, d_intermediate_states, d_intermediate_times, d_error_in_state                          # We also only consider the input and output variables that actually have gradients enabled             inputs_with_grad = [i for idx, i in enumerate(inputs) if ctx.needs_input_grad[idx]]             outputs_with_grad = [idx for idx, i in enumerate(outputs) if i.grad_fn is not None]                          grad_of_inputs_with_grad = torch.autograd.grad([outputs[idx] for idx in outputs_with_grad], inputs_with_grad, grad_outputs=[grad_outputs[idx] for idx in outputs_with_grad], allow_unused=True, materialize_grads=True)         else:             grad_of_inputs_with_grad = None         # For each input we must return a gradient         # Interestingly, this also includes the function we passed in...         # We create a list of None values          # (this tells autograd that there is no gradient for those variables).         # And for each variable that does have a gradient, we fill the values in         # before returning the list         input_grads = [None for _ in range(len(inputs))]         if grad_of_inputs_with_grad:             for idx in range(len(inputs)):                 if ctx.needs_input_grad[idx]:                     input_grads[idx], *grad_of_inputs_with_grad = grad_of_inputs_with_grad         return tuple(input_grads)                       if not __integrator_type.is_adaptive:         # If the method isn't adaptive, neither atol nor rtol are required, but because of         # how `torch.autograd.Function` works, we cannot have keyword arguments         # For that reason, we use an alternative implementation to fill those values with a stub         def __internal_forward_nonadaptive(ctx, fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor],                                             x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor, *additional_dynamic_args):             return __internal_forward(ctx, fn, x0, t0, t1, dt, torch.inf, torch.inf, *additional_dynamic_args)         __integrator_type.forward = staticmethod(__internal_forward_nonadaptive)     else:         __integrator_type.forward = staticmethod(__internal_forward)     __integrator_type.backward = staticmethod(__internal_backward)     return __integrator_type In\u00a0[22]: Copied! <pre>adaptive_rk45_integrator = get_integrator(adaptive_rk45_integrator.integrator_tableau, integrator_order = 5, use_local_extrapolation = False, integrator_name = \"AdaptiveRK45Integrator\")\n\ntimes_to_integrate = [time for _, time in sha_states][1:]\n# We skip the first time as we know the initial state\ncurrent_state = initial_x.clone()\ncurrent_time  = initial_time.clone()\n\nerror = 0.0\n\n# Let's look at the first sample_time and see what output we get\nfor sample_index, sample_time in enumerate(times_to_integrate[:1]):\n    current_state, current_time, *_ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, dt, atol, rtol, optimised_frequency, optimised_damping)\n    error = error + torch.linalg.norm(sha_states[sample_index][0] - current_state)/len(times_to_integrate)\n    print(f\"The current state: {current_state}\")\n    print(f\"Error at time {current_time} is {error}\")\nerror.backward()\n</pre> adaptive_rk45_integrator = get_integrator(adaptive_rk45_integrator.integrator_tableau, integrator_order = 5, use_local_extrapolation = False, integrator_name = \"AdaptiveRK45Integrator\")  times_to_integrate = [time for _, time in sha_states][1:] # We skip the first time as we know the initial state current_state = initial_x.clone() current_time  = initial_time.clone()  error = 0.0  # Let's look at the first sample_time and see what output we get for sample_index, sample_time in enumerate(times_to_integrate[:1]):     current_state, current_time, *_ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, dt, atol, rtol, optimised_frequency, optimised_damping)     error = error + torch.linalg.norm(sha_states[sample_index][0] - current_state)/len(times_to_integrate)     print(f\"The current state: {current_state}\")     print(f\"Error at time {current_time} is {error}\") error.backward() <pre>The current state: tensor([ 1.0000e+00, -4.9998e-06], grad_fn=&lt;AdaptiveRK45IntegratorBackward&gt;)\nError at time 0.0005 is 4.03205658265911e-08\n</pre> In\u00a0[23]: Copied! <pre>print(f\"Current Frequency: {optimised_frequency}, Frequency grad: {optimised_frequency.grad}\")\nprint(f\"Current Damping: {optimised_damping}, Damping grad: {optimised_damping.grad}\")\n</pre> print(f\"Current Frequency: {optimised_frequency}, Frequency grad: {optimised_frequency.grad}\") print(f\"Current Damping: {optimised_damping}, Damping grad: {optimised_damping.grad}\") <pre>Current Frequency: 0.1, Frequency grad: 8.063911562493276e-07\nCurrent Damping: 1.0, Damping grad: -2.0159946488567326e-12\n</pre> <p>Great, we see that the gradient of both <code>optimised_frequency</code> and <code>optimised_damping</code> have been populated. Let's check that this gradient is being computed correctly using <code>torch.autograd.gradcheck</code> with random initial conditions, frequencies and damping coefficients. In this way, we can check that the gradient is correct across multiple conditions.</p> <p><code>gradcheck</code> works by using finite differences as the numerical value and comparing that to the autodiff derived value for the gradient (or, more generally, the Jacobian). This requires multiple numerical integrations and running at the highest precision possible (hence the use of <code>atol**2</code> and <code>rtol**2</code> which achieves machine precision) to ensure that numerical inaccuracy is not the cause of incorrect gradients. This is quite an expensive procedure due to the depth of the autodiff graph that is generated and thus will take some time to compute.</p> In\u00a0[24]: Copied! <pre>from torch.autograd import gradcheck\n\ndef test_func(init_state, freq, damp):\n    res = adaptive_rk45_integrator.apply(sha_dynamics, init_state, initial_time, initial_time+0.01, dt, torch.tensor(1e-14), torch.tensor(1e-14), freq, damp)\n    return res[0]\n\ntest_variables = [initial_x, frequency, damping]\n\ndef generate_test_vars():\n    test_x = (2*torch.rand_like(initial_x) - 1.0)\n    test_frequency = torch.rand_like(frequency)\n    test_damping = torch.rand_like(damping)\n    return [i.requires_grad_(True) for i in [test_x, test_frequency, test_damping]]\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", RuntimeWarning)\n    num_tests = 4\n    print(f\"[0/{num_tests}] - vars: {[i.detach().cpu().tolist() for i in test_variables]}, success: \", end='')\n    print(gradcheck(test_func, [i.detach().clone().requires_grad_(True) for i in test_variables]))\n    for iter_idx in range(num_tests):\n        variables = generate_test_vars()\n        print(f\"[{iter_idx+1}/{num_tests}] - vars: {[i.detach().cpu().tolist() for i in variables]}, success: \", end='')\n        print(gradcheck(test_func, variables))\n</pre> from torch.autograd import gradcheck  def test_func(init_state, freq, damp):     res = adaptive_rk45_integrator.apply(sha_dynamics, init_state, initial_time, initial_time+0.01, dt, torch.tensor(1e-14), torch.tensor(1e-14), freq, damp)     return res[0]  test_variables = [initial_x, frequency, damping]  def generate_test_vars():     test_x = (2*torch.rand_like(initial_x) - 1.0)     test_frequency = torch.rand_like(frequency)     test_damping = torch.rand_like(damping)     return [i.requires_grad_(True) for i in [test_x, test_frequency, test_damping]]  with warnings.catch_warnings():     warnings.simplefilter(\"ignore\", RuntimeWarning)     num_tests = 4     print(f\"[0/{num_tests}] - vars: {[i.detach().cpu().tolist() for i in test_variables]}, success: \", end='')     print(gradcheck(test_func, [i.detach().clone().requires_grad_(True) for i in test_variables]))     for iter_idx in range(num_tests):         variables = generate_test_vars()         print(f\"[{iter_idx+1}/{num_tests}] - vars: {[i.detach().cpu().tolist() for i in variables]}, success: \", end='')         print(gradcheck(test_func, variables)) <pre>[0/4] - vars: [[1.0, 0.0], 1.0, 0.25], success: True\n[1/4] - vars: [[0.5662623888314278, -0.13291998917513825], 0.5862130150542093, 0.35079282251958555], success: True\n[2/4] - vars: [[-0.03426942938557631, -0.6035063069149991], 0.0016512400245205505, 0.8237204099086072], success: True\n[3/4] - vars: [[0.3611170099769341, -0.5509630565383719], 0.08240171994937384, 0.9659915931678263], success: True\n[4/4] - vars: [[0.14063778041676822, -0.22986764674631344], 0.5371906530543378, 0.9915331859191349], success: True\n</pre> <p>Now that we've validated the correctness of the gradients, we can implement an optimisation loop to fit our parameters to a prior trajectory.</p> In\u00a0[25]: Copied! <pre>times_to_integrate = [time for _, time in sha_states][1:]\n# We skip the first time as we know the initial state\n\n# We reinitialise our variables\noptimised_frequency = torch.tensor(0.1, requires_grad=True)\noptimised_damping = torch.tensor(1.0, requires_grad=True)\n# As damping needs to be a strictly positive quantity, we log-encode it\nlog_encoded_damping = torch.log(optimised_damping.detach()).requires_grad_(True)\n\n# First, we'll create an `optimiser` following pytorch convention\noptimiser = torch.optim.Adam([optimised_frequency, log_encoded_damping], lr=1e-2, amsgrad=True)\n\n# Next, we'll define a closure function whose sole purpose is to\n# zero the gradients and compute the error. This is useful as it allows switching to other\n# optimisers such as LBFGS or anything that re-evaluates the error without\n# computing its gradient\ndef sha_closure():\n    current_state = initial_x.clone()\n    current_time  = initial_time.clone()\n    optimiser.zero_grad()\n    error = 0.0\n    \n    for sample_index, sample_time in enumerate(times_to_integrate):\n        # We don't require high precision here as this system, with the parameters we've set is fairly stable\n        # We only need that the gradients have the right direction and roughly the right magnitude to do gradient descent\n        new_state, new_time, *_ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, torch.minimum(dt, sample_time - current_time), atol, rtol, optimised_frequency, torch.exp(log_encoded_damping))\n        error = error + torch.linalg.norm(sha_states[sample_index][0] - new_state)/len(times_to_integrate)\n        current_state, current_time = new_state.detach(), new_time.detach()\n    if error.requires_grad:\n        error.backward()\n    return error\n\n# Now we need an optimisation `loop` where we will take steps to minimise the error\nnumber_of_gd_steps = 256\n\n# We also need to track the best solution thus far\nbest_error = torch.inf\nbest_frequency, best_damping = optimised_frequency.detach().clone(), optimised_damping.detach().clone()\nfor step in range(number_of_gd_steps):\n    step_error = optimiser.step(sha_closure)\n    if step_error &lt; best_error:\n        best_error = step_error.item()\n        best_frequency = optimised_frequency.detach().clone()\n        best_damping = torch.exp(log_encoded_damping.detach().clone())\n    print(f\"[{step+1}/{number_of_gd_steps}] Error: {step_error.item():.6f}, Current Frequency: {optimised_frequency.item():.6f}, Current Damping: {torch.exp(log_encoded_damping).item():.6f}\")\n    \nprint(f\"Best frequency: {best_frequency.item():.6f}, relative error: {torch.mean(torch.abs(1 - best_frequency / frequency)).item():.6%}\")\nprint(f\"Best damping:   {best_damping.item():.6f}, relative error: {torch.mean(torch.abs(1 - best_damping / damping)).item():.6%}\")\n</pre> times_to_integrate = [time for _, time in sha_states][1:] # We skip the first time as we know the initial state  # We reinitialise our variables optimised_frequency = torch.tensor(0.1, requires_grad=True) optimised_damping = torch.tensor(1.0, requires_grad=True) # As damping needs to be a strictly positive quantity, we log-encode it log_encoded_damping = torch.log(optimised_damping.detach()).requires_grad_(True)  # First, we'll create an `optimiser` following pytorch convention optimiser = torch.optim.Adam([optimised_frequency, log_encoded_damping], lr=1e-2, amsgrad=True)  # Next, we'll define a closure function whose sole purpose is to # zero the gradients and compute the error. This is useful as it allows switching to other # optimisers such as LBFGS or anything that re-evaluates the error without # computing its gradient def sha_closure():     current_state = initial_x.clone()     current_time  = initial_time.clone()     optimiser.zero_grad()     error = 0.0          for sample_index, sample_time in enumerate(times_to_integrate):         # We don't require high precision here as this system, with the parameters we've set is fairly stable         # We only need that the gradients have the right direction and roughly the right magnitude to do gradient descent         new_state, new_time, *_ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, torch.minimum(dt, sample_time - current_time), atol, rtol, optimised_frequency, torch.exp(log_encoded_damping))         error = error + torch.linalg.norm(sha_states[sample_index][0] - new_state)/len(times_to_integrate)         current_state, current_time = new_state.detach(), new_time.detach()     if error.requires_grad:         error.backward()     return error  # Now we need an optimisation `loop` where we will take steps to minimise the error number_of_gd_steps = 256  # We also need to track the best solution thus far best_error = torch.inf best_frequency, best_damping = optimised_frequency.detach().clone(), optimised_damping.detach().clone() for step in range(number_of_gd_steps):     step_error = optimiser.step(sha_closure)     if step_error &lt; best_error:         best_error = step_error.item()         best_frequency = optimised_frequency.detach().clone()         best_damping = torch.exp(log_encoded_damping.detach().clone())     print(f\"[{step+1}/{number_of_gd_steps}] Error: {step_error.item():.6f}, Current Frequency: {optimised_frequency.item():.6f}, Current Damping: {torch.exp(log_encoded_damping).item():.6f}\")      print(f\"Best frequency: {best_frequency.item():.6f}, relative error: {torch.mean(torch.abs(1 - best_frequency / frequency)).item():.6%}\") print(f\"Best damping:   {best_damping.item():.6f}, relative error: {torch.mean(torch.abs(1 - best_damping / damping)).item():.6%}\") <pre>[1/256] Error: 0.718138, Current Frequency: 0.110000, Current Damping: 0.990051\n[2/256] Error: 0.686355, Current Frequency: 0.120008, Current Damping: 0.980191\n[3/256] Error: 0.655750, Current Frequency: 0.130027, Current Damping: 0.970422\n[4/256] Error: 0.626408, Current Frequency: 0.140062, Current Damping: 0.960753\n[5/256] Error: 0.598379, Current Frequency: 0.150117, Current Damping: 0.951206\n[6/256] Error: 0.571688, Current Frequency: 0.160195, Current Damping: 0.941820\n[7/256] Error: 0.546343, Current Frequency: 0.170300, Current Damping: 0.932656\n[8/256] Error: 0.522344, Current Frequency: 0.180436, Current Damping: 0.923802\n[9/256] Error: 0.499689, Current Frequency: 0.190608, Current Damping: 0.915383\n[10/256] Error: 0.478377, Current Frequency: 0.200820, Current Damping: 0.907575\n[11/256] Error: 0.458415, Current Frequency: 0.211077, Current Damping: 0.900601\n[12/256] Error: 0.439823, Current Frequency: 0.221385, Current Damping: 0.894715\n[13/256] Error: 0.422625, Current Frequency: 0.231747, Current Damping: 0.890171\n[14/256] Error: 0.406842, Current Frequency: 0.242170, Current Damping: 0.887130\n[15/256] Error: 0.392461, Current Frequency: 0.252655, Current Damping: 0.885589\n[16/256] Error: 0.379451, Current Frequency: 0.263205, Current Damping: 0.885403\n[17/256] Error: 0.367767, Current Frequency: 0.273820, Current Damping: 0.886400\n[18/256] Error: 0.357104, Current Frequency: 0.284501, Current Damping: 0.888357\n[19/256] Error: 0.347307, Current Frequency: 0.295248, Current Damping: 0.891028\n[20/256] Error: 0.338277, Current Frequency: 0.306060, Current Damping: 0.894147\n[21/256] Error: 0.329941, Current Frequency: 0.316937, Current Damping: 0.897394\n[22/256] Error: 0.322278, Current Frequency: 0.327880, Current Damping: 0.900522\n[23/256] Error: 0.315177, Current Frequency: 0.338887, Current Damping: 0.903220\n[24/256] Error: 0.308461, Current Frequency: 0.349960, Current Damping: 0.905077\n[25/256] Error: 0.302019, Current Frequency: 0.361099, Current Damping: 0.905660\n[26/256] Error: 0.295773, Current Frequency: 0.372305, Current Damping: 0.904608\n[27/256] Error: 0.289680, Current Frequency: 0.383577, Current Damping: 0.901750\n[28/256] Error: 0.283764, Current Frequency: 0.394918, Current Damping: 0.897167\n[29/256] Error: 0.278155, Current Frequency: 0.406327, Current Damping: 0.891216\n[30/256] Error: 0.273123, Current Frequency: 0.417807, Current Damping: 0.884071\n[31/256] Error: 0.268553, Current Frequency: 0.429359, Current Damping: 0.875970\n[32/256] Error: 0.264319, Current Frequency: 0.440986, Current Damping: 0.867120\n[33/256] Error: 0.260362, Current Frequency: 0.452691, Current Damping: 0.857686\n[34/256] Error: 0.256635, Current Frequency: 0.464476, Current Damping: 0.847800\n[35/256] Error: 0.253097, Current Frequency: 0.476344, Current Damping: 0.837563\n[36/256] Error: 0.249711, Current Frequency: 0.488295, Current Damping: 0.827056\n[37/256] Error: 0.246444, Current Frequency: 0.500333, Current Damping: 0.816345\n[38/256] Error: 0.243264, Current Frequency: 0.512460, Current Damping: 0.805480\n[39/256] Error: 0.240144, Current Frequency: 0.524676, Current Damping: 0.794503\n[40/256] Error: 0.237061, Current Frequency: 0.536984, Current Damping: 0.783448\n[41/256] Error: 0.233992, Current Frequency: 0.549385, Current Damping: 0.772342\n[42/256] Error: 0.230918, Current Frequency: 0.561879, Current Damping: 0.761208\n[43/256] Error: 0.227826, Current Frequency: 0.574467, Current Damping: 0.750065\n[44/256] Error: 0.224699, Current Frequency: 0.587150, Current Damping: 0.738929\n[45/256] Error: 0.221528, Current Frequency: 0.599929, Current Damping: 0.727814\n[46/256] Error: 0.218301, Current Frequency: 0.612802, Current Damping: 0.716732\n[47/256] Error: 0.215009, Current Frequency: 0.625771, Current Damping: 0.705692\n[48/256] Error: 0.211644, Current Frequency: 0.638834, Current Damping: 0.694702\n[49/256] Error: 0.208198, Current Frequency: 0.651991, Current Damping: 0.683771\n[50/256] Error: 0.204664, Current Frequency: 0.665240, Current Damping: 0.672904\n[51/256] Error: 0.201035, Current Frequency: 0.678579, Current Damping: 0.662108\n[52/256] Error: 0.197305, Current Frequency: 0.692005, Current Damping: 0.651388\n[53/256] Error: 0.193470, Current Frequency: 0.705512, Current Damping: 0.640747\n[54/256] Error: 0.189525, Current Frequency: 0.719092, Current Damping: 0.630192\n[55/256] Error: 0.185471, Current Frequency: 0.732722, Current Damping: 0.619725\n[56/256] Error: 0.181314, Current Frequency: 0.746363, Current Damping: 0.609352\n[57/256] Error: 0.177069, Current Frequency: 0.759956, Current Damping: 0.599080\n[58/256] Error: 0.172748, Current Frequency: 0.773464, Current Damping: 0.588911\n[59/256] Error: 0.168354, Current Frequency: 0.786880, Current Damping: 0.578846\n[60/256] Error: 0.163886, Current Frequency: 0.800205, Current Damping: 0.568887\n[61/256] Error: 0.159341, Current Frequency: 0.813448, Current Damping: 0.559033\n[62/256] Error: 0.154722, Current Frequency: 0.826611, Current Damping: 0.549287\n[63/256] Error: 0.150035, Current Frequency: 0.839691, Current Damping: 0.539648\n[64/256] Error: 0.145289, Current Frequency: 0.852672, Current Damping: 0.530120\n[65/256] Error: 0.140505, Current Frequency: 0.865514, Current Damping: 0.520706\n[66/256] Error: 0.135711, Current Frequency: 0.878160, Current Damping: 0.511411\n[67/256] Error: 0.130938, Current Frequency: 0.890555, Current Damping: 0.502239\n[68/256] Error: 0.126211, Current Frequency: 0.902653, Current Damping: 0.493190\n[69/256] Error: 0.121555, Current Frequency: 0.914407, Current Damping: 0.484268\n[70/256] Error: 0.117001, Current Frequency: 0.925759, Current Damping: 0.475475\n[71/256] Error: 0.112585, Current Frequency: 0.936632, Current Damping: 0.466816\n[72/256] Error: 0.108341, Current Frequency: 0.946943, Current Damping: 0.458296\n[73/256] Error: 0.104298, Current Frequency: 0.956605, Current Damping: 0.449918\n[74/256] Error: 0.100481, Current Frequency: 0.965520, Current Damping: 0.441689\n[75/256] Error: 0.096906, Current Frequency: 0.973594, Current Damping: 0.433613\n[76/256] Error: 0.093580, Current Frequency: 0.980734, Current Damping: 0.425697\n[77/256] Error: 0.090497, Current Frequency: 0.986856, Current Damping: 0.417946\n[78/256] Error: 0.087637, Current Frequency: 0.991888, Current Damping: 0.410366\n[79/256] Error: 0.084970, Current Frequency: 0.995767, Current Damping: 0.402960\n[80/256] Error: 0.082456, Current Frequency: 0.998458, Current Damping: 0.395733\n[81/256] Error: 0.080051, Current Frequency: 0.999949, Current Damping: 0.388685\n[82/256] Error: 0.077711, Current Frequency: 1.000257, Current Damping: 0.381817\n[83/256] Error: 0.075395, Current Frequency: 0.999422, Current Damping: 0.375128\n[84/256] Error: 0.073072, Current Frequency: 0.997512, Current Damping: 0.368614\n[85/256] Error: 0.070725, Current Frequency: 0.994614, Current Damping: 0.362269\n[86/256] Error: 0.068353, Current Frequency: 0.990838, Current Damping: 0.356087\n[87/256] Error: 0.065968, Current Frequency: 0.986310, Current Damping: 0.350061\n[88/256] Error: 0.063604, Current Frequency: 0.981175, Current Damping: 0.344185\n[89/256] Error: 0.061307, Current Frequency: 0.975590, Current Damping: 0.338450\n[90/256] Error: 0.059134, Current Frequency: 0.969731, Current Damping: 0.332852\n[91/256] Error: 0.057144, Current Frequency: 0.963785, Current Damping: 0.327386\n[92/256] Error: 0.055386, Current Frequency: 0.957946, Current Damping: 0.322051\n[93/256] Error: 0.053888, Current Frequency: 0.952415, Current Damping: 0.316846\n[94/256] Error: 0.052642, Current Frequency: 0.947385, Current Damping: 0.311773\n[95/256] Error: 0.051598, Current Frequency: 0.943027, Current Damping: 0.306836\n[96/256] Error: 0.050671, Current Frequency: 0.939483, Current Damping: 0.302037\n[97/256] Error: 0.049750, Current Frequency: 0.936860, Current Damping: 0.297379\n[98/256] Error: 0.048718, Current Frequency: 0.935224, Current Damping: 0.292863\n[99/256] Error: 0.047465, Current Frequency: 0.934593, Current Damping: 0.288490\n[100/256] Error: 0.045908, Current Frequency: 0.934943, Current Damping: 0.284258\n[101/256] Error: 0.043998, Current Frequency: 0.936210, Current Damping: 0.280164\n[102/256] Error: 0.041722, Current Frequency: 0.938294, Current Damping: 0.276202\n[103/256] Error: 0.039108, Current Frequency: 0.941064, Current Damping: 0.272367\n[104/256] Error: 0.036221, Current Frequency: 0.944357, Current Damping: 0.268653\n[105/256] Error: 0.033159, Current Frequency: 0.947982, Current Damping: 0.265056\n[106/256] Error: 0.030053, Current Frequency: 0.951718, Current Damping: 0.261571\n[107/256] Error: 0.027058, Current Frequency: 0.955319, Current Damping: 0.258197\n[108/256] Error: 0.024338, Current Frequency: 0.958529, Current Damping: 0.254939\n[109/256] Error: 0.022050, Current Frequency: 0.961105, Current Damping: 0.251807\n[110/256] Error: 0.020332, Current Frequency: 0.962857, Current Damping: 0.248819\n[111/256] Error: 0.019311, Current Frequency: 0.963734, Current Damping: 0.246005\n[112/256] Error: 0.019093, Current Frequency: 0.963559, Current Damping: 0.243415\n[113/256] Error: 0.019669, Current Frequency: 0.962275, Current Damping: 0.241101\n[114/256] Error: 0.020845, Current Frequency: 0.960001, Current Damping: 0.239078\n[115/256] Error: 0.022372, Current Frequency: 0.957020, Current Damping: 0.237332\n[116/256] Error: 0.024107, Current Frequency: 0.953687, Current Damping: 0.235833\n[117/256] Error: 0.025951, Current Frequency: 0.950354, Current Damping: 0.234553\n[118/256] Error: 0.027791, Current Frequency: 0.947331, Current Damping: 0.233465\n[119/256] Error: 0.029500, Current Frequency: 0.944854, Current Damping: 0.232547\n[120/256] Error: 0.030960, Current Frequency: 0.943076, Current Damping: 0.231781\n[121/256] Error: 0.032079, Current Frequency: 0.942071, Current Damping: 0.231152\n[122/256] Error: 0.032806, Current Frequency: 0.941834, Current Damping: 0.230649\n[123/256] Error: 0.033135, Current Frequency: 0.942293, Current Damping: 0.230265\n[124/256] Error: 0.033102, Current Frequency: 0.943323, Current Damping: 0.229995\n[125/256] Error: 0.032775, Current Frequency: 0.944763, Current Damping: 0.229834\n[126/256] Error: 0.032240, Current Frequency: 0.946428, Current Damping: 0.229783\n[127/256] Error: 0.031589, Current Frequency: 0.948126, Current Damping: 0.229839\n[128/256] Error: 0.030901, Current Frequency: 0.949679, Current Damping: 0.230002\n[129/256] Error: 0.030243, Current Frequency: 0.950939, Current Damping: 0.230269\n[130/256] Error: 0.029655, Current Frequency: 0.951802, Current Damping: 0.230635\n[131/256] Error: 0.029162, Current Frequency: 0.952222, Current Damping: 0.231094\n[132/256] Error: 0.028773, Current Frequency: 0.952212, Current Damping: 0.231633\n[133/256] Error: 0.028490, Current Frequency: 0.951839, Current Damping: 0.232239\n[134/256] Error: 0.028309, Current Frequency: 0.951209, Current Damping: 0.232897\n[135/256] Error: 0.028214, Current Frequency: 0.950451, Current Damping: 0.233588\n[136/256] Error: 0.028179, Current Frequency: 0.949698, Current Damping: 0.234296\n[137/256] Error: 0.028165, Current Frequency: 0.949068, Current Damping: 0.235005\n[138/256] Error: 0.028130, Current Frequency: 0.948651, Current Damping: 0.235700\n[139/256] Error: 0.028034, Current Frequency: 0.948505, Current Damping: 0.236371\n[140/256] Error: 0.027848, Current Frequency: 0.948650, Current Damping: 0.237009\n[141/256] Error: 0.027559, Current Frequency: 0.949073, Current Damping: 0.237608\n[142/256] Error: 0.027169, Current Frequency: 0.949727, Current Damping: 0.238163\n[143/256] Error: 0.026697, Current Frequency: 0.950546, Current Damping: 0.238675\n[144/256] Error: 0.026175, Current Frequency: 0.951446, Current Damping: 0.239143\n[145/256] Error: 0.025637, Current Frequency: 0.952340, Current Damping: 0.239568\n[146/256] Error: 0.025122, Current Frequency: 0.953147, Current Damping: 0.239951\n[147/256] Error: 0.024662, Current Frequency: 0.953800, Current Damping: 0.240295\n[148/256] Error: 0.024285, Current Frequency: 0.954252, Current Damping: 0.240601\n[149/256] Error: 0.024007, Current Frequency: 0.954484, Current Damping: 0.240869\n[150/256] Error: 0.023833, Current Frequency: 0.954502, Current Damping: 0.241099\n[151/256] Error: 0.023763, Current Frequency: 0.954336, Current Damping: 0.241290\n[152/256] Error: 0.023782, Current Frequency: 0.954034, Current Damping: 0.241441\n[153/256] Error: 0.023872, Current Frequency: 0.953650, Current Damping: 0.241550\n[154/256] Error: 0.024010, Current Frequency: 0.953245, Current Damping: 0.241617\n[155/256] Error: 0.024169, Current Frequency: 0.952871, Current Damping: 0.241643\n[156/256] Error: 0.024328, Current Frequency: 0.952570, Current Damping: 0.241628\n[157/256] Error: 0.024464, Current Frequency: 0.952370, Current Damping: 0.241575\n[158/256] Error: 0.024566, Current Frequency: 0.952281, Current Damping: 0.241488\n[159/256] Error: 0.024626, Current Frequency: 0.952300, Current Damping: 0.241372\n[160/256] Error: 0.024646, Current Frequency: 0.952408, Current Damping: 0.241231\n[161/256] Error: 0.024633, Current Frequency: 0.952578, Current Damping: 0.241071\n[162/256] Error: 0.024598, Current Frequency: 0.952777, Current Damping: 0.240898\n[163/256] Error: 0.024556, Current Frequency: 0.952969, Current Damping: 0.240718\n[164/256] Error: 0.024521, Current Frequency: 0.953123, Current Damping: 0.240536\n[165/256] Error: 0.024504, Current Frequency: 0.953215, Current Damping: 0.240357\n[166/256] Error: 0.024515, Current Frequency: 0.953230, Current Damping: 0.240186\n[167/256] Error: 0.024558, Current Frequency: 0.953164, Current Damping: 0.240023\n[168/256] Error: 0.024634, Current Frequency: 0.953026, Current Damping: 0.239873\n[169/256] Error: 0.024737, Current Frequency: 0.952831, Current Damping: 0.239735\n[170/256] Error: 0.024861, Current Frequency: 0.952602, Current Damping: 0.239610\n[171/256] Error: 0.024997, Current Frequency: 0.952365, Current Damping: 0.239497\n[172/256] Error: 0.025133, Current Frequency: 0.952143, Current Damping: 0.239396\n[173/256] Error: 0.025258, Current Frequency: 0.951959, Current Damping: 0.239307\n[174/256] Error: 0.025365, Current Frequency: 0.951826, Current Damping: 0.239228\n[175/256] Error: 0.025446, Current Frequency: 0.951753, Current Damping: 0.239160\n[176/256] Error: 0.025499, Current Frequency: 0.951739, Current Damping: 0.239102\n[177/256] Error: 0.025523, Current Frequency: 0.951777, Current Damping: 0.239054\n[178/256] Error: 0.025522, Current Frequency: 0.951855, Current Damping: 0.239016\n[179/256] Error: 0.025500, Current Frequency: 0.951958, Current Damping: 0.238989\n[180/256] Error: 0.025464, Current Frequency: 0.952069, Current Damping: 0.238973\n[181/256] Error: 0.025422, Current Frequency: 0.952171, Current Damping: 0.238968\n[182/256] Error: 0.025380, Current Frequency: 0.952253, Current Damping: 0.238974\n[183/256] Error: 0.025344, Current Frequency: 0.952305, Current Damping: 0.238989\n[184/256] Error: 0.025317, Current Frequency: 0.952326, Current Damping: 0.239014\n[185/256] Error: 0.025300, Current Frequency: 0.952316, Current Damping: 0.239047\n[186/256] Error: 0.025294, Current Frequency: 0.952282, Current Damping: 0.239086\n[187/256] Error: 0.025296, Current Frequency: 0.952232, Current Damping: 0.239130\n[188/256] Error: 0.025303, Current Frequency: 0.952177, Current Damping: 0.239178\n[189/256] Error: 0.025312, Current Frequency: 0.952127, Current Damping: 0.239226\n[190/256] Error: 0.025318, Current Frequency: 0.952091, Current Damping: 0.239275\n[191/256] Error: 0.025318, Current Frequency: 0.952075, Current Damping: 0.239323\n[192/256] Error: 0.025310, Current Frequency: 0.952081, Current Damping: 0.239368\n[193/256] Error: 0.025293, Current Frequency: 0.952110, Current Damping: 0.239411\n[194/256] Error: 0.025268, Current Frequency: 0.952157, Current Damping: 0.239451\n[195/256] Error: 0.025236, Current Frequency: 0.952217, Current Damping: 0.239487\n[196/256] Error: 0.025199, Current Frequency: 0.952282, Current Damping: 0.239519\n[197/256] Error: 0.025161, Current Frequency: 0.952346, Current Damping: 0.239549\n[198/256] Error: 0.025125, Current Frequency: 0.952401, Current Damping: 0.239575\n[199/256] Error: 0.025093, Current Frequency: 0.952442, Current Damping: 0.239598\n[200/256] Error: 0.025069, Current Frequency: 0.952467, Current Damping: 0.239618\n[201/256] Error: 0.025052, Current Frequency: 0.952475, Current Damping: 0.239636\n[202/256] Error: 0.025043, Current Frequency: 0.952468, Current Damping: 0.239650\n[203/256] Error: 0.025042, Current Frequency: 0.952448, Current Damping: 0.239662\n[204/256] Error: 0.025047, Current Frequency: 0.952420, Current Damping: 0.239670\n[205/256] Error: 0.025057, Current Frequency: 0.952389, Current Damping: 0.239675\n[206/256] Error: 0.025068, Current Frequency: 0.952361, Current Damping: 0.239677\n[207/256] Error: 0.025080, Current Frequency: 0.952338, Current Damping: 0.239676\n[208/256] Error: 0.025090, Current Frequency: 0.952323, Current Damping: 0.239672\n[209/256] Error: 0.025098, Current Frequency: 0.952318, Current Damping: 0.239665\n[210/256] Error: 0.025102, Current Frequency: 0.952321, Current Damping: 0.239656\n[211/256] Error: 0.025103, Current Frequency: 0.952331, Current Damping: 0.239645\n[212/256] Error: 0.025102, Current Frequency: 0.952345, Current Damping: 0.239632\n[213/256] Error: 0.025100, Current Frequency: 0.952360, Current Damping: 0.239619\n[214/256] Error: 0.025097, Current Frequency: 0.952374, Current Damping: 0.239605\n[215/256] Error: 0.025096, Current Frequency: 0.952383, Current Damping: 0.239592\n[216/256] Error: 0.025096, Current Frequency: 0.952387, Current Damping: 0.239578\n[217/256] Error: 0.025098, Current Frequency: 0.952383, Current Damping: 0.239566\n[218/256] Error: 0.025104, Current Frequency: 0.952374, Current Damping: 0.239554\n[219/256] Error: 0.025111, Current Frequency: 0.952359, Current Damping: 0.239543\n[220/256] Error: 0.025121, Current Frequency: 0.952341, Current Damping: 0.239533\n[221/256] Error: 0.025132, Current Frequency: 0.952323, Current Damping: 0.239524\n[222/256] Error: 0.025142, Current Frequency: 0.952305, Current Damping: 0.239516\n[223/256] Error: 0.025152, Current Frequency: 0.952291, Current Damping: 0.239509\n[224/256] Error: 0.025160, Current Frequency: 0.952281, Current Damping: 0.239503\n[225/256] Error: 0.025166, Current Frequency: 0.952277, Current Damping: 0.239498\n[226/256] Error: 0.025170, Current Frequency: 0.952277, Current Damping: 0.239493\n[227/256] Error: 0.025171, Current Frequency: 0.952281, Current Damping: 0.239490\n[228/256] Error: 0.025171, Current Frequency: 0.952288, Current Damping: 0.239487\n[229/256] Error: 0.025169, Current Frequency: 0.952296, Current Damping: 0.239485\n[230/256] Error: 0.025165, Current Frequency: 0.952305, Current Damping: 0.239484\n[231/256] Error: 0.025162, Current Frequency: 0.952312, Current Damping: 0.239484\n[232/256] Error: 0.025159, Current Frequency: 0.952318, Current Damping: 0.239485\n[233/256] Error: 0.025156, Current Frequency: 0.952320, Current Damping: 0.239487\n[234/256] Error: 0.025155, Current Frequency: 0.952320, Current Damping: 0.239489\n[235/256] Error: 0.025154, Current Frequency: 0.952318, Current Damping: 0.239493\n[236/256] Error: 0.025154, Current Frequency: 0.952314, Current Damping: 0.239496\n[237/256] Error: 0.025155, Current Frequency: 0.952309, Current Damping: 0.239500\n[238/256] Error: 0.025155, Current Frequency: 0.952305, Current Damping: 0.239504\n[239/256] Error: 0.025156, Current Frequency: 0.952302, Current Damping: 0.239508\n[240/256] Error: 0.025156, Current Frequency: 0.952301, Current Damping: 0.239512\n[241/256] Error: 0.025155, Current Frequency: 0.952302, Current Damping: 0.239516\n[242/256] Error: 0.025154, Current Frequency: 0.952304, Current Damping: 0.239519\n[243/256] Error: 0.025152, Current Frequency: 0.952308, Current Damping: 0.239522\n[244/256] Error: 0.025149, Current Frequency: 0.952314, Current Damping: 0.239525\n[245/256] Error: 0.025146, Current Frequency: 0.952319, Current Damping: 0.239528\n[246/256] Error: 0.025143, Current Frequency: 0.952324, Current Damping: 0.239530\n[247/256] Error: 0.025140, Current Frequency: 0.952328, Current Damping: 0.239532\n[248/256] Error: 0.025138, Current Frequency: 0.952331, Current Damping: 0.239534\n[249/256] Error: 0.025136, Current Frequency: 0.952332, Current Damping: 0.239535\n[250/256] Error: 0.025135, Current Frequency: 0.952332, Current Damping: 0.239537\n[251/256] Error: 0.025134, Current Frequency: 0.952331, Current Damping: 0.239538\n[252/256] Error: 0.025135, Current Frequency: 0.952329, Current Damping: 0.239538\n[253/256] Error: 0.025135, Current Frequency: 0.952326, Current Damping: 0.239539\n[254/256] Error: 0.025136, Current Frequency: 0.952324, Current Damping: 0.239539\n[255/256] Error: 0.025137, Current Frequency: 0.952321, Current Damping: 0.239539\n[256/256] Error: 0.025138, Current Frequency: 0.952320, Current Damping: 0.239538\nBest frequency: 0.963559, relative error: 3.644131%\nBest damping:   0.243415, relative error: 2.633959%\n</pre> In\u00a0[26]: Copied! <pre>_, _, sha_states_ref, sha_times_ref, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, frequency, damping)\nfig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\")\nfig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")\n\n_, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, best_frequency, best_damping)\n_ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\")\n_ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\")\n</pre> _, _, sha_states_ref, sha_times_ref, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, frequency, damping) fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\") fig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")  _, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, best_frequency, best_damping) _ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\") _ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\") <p>While we were able to infer the parameters closely through gradient descent, they are still imperfect, and you will notice that this implementation is quite slow. We could speed up some of the optimisation by using only subsets of the samples aka mini-batching at each gradient descent step.</p> <p>First, we'll collate the reference states into one tensor and the reference times into another. At each iteration, we'll sample some subset and integrate the system to the appropriate times taking care to sort the integration times to avoid integrating backwards (although we could).</p> <p>Second, we'll write a closure function that takes in the batch at each step and runs our previous closure routine.</p> In\u00a0[27]: Copied! <pre>time_dataset  = torch.stack([time for _, time in sha_states][1:])\nstate_dataset = torch.stack([state for state, _ in sha_states][1:])\n# We skip the first time as we know the initial state\n\n# We reinitialise our variables\noptimised_frequency = torch.tensor(0.1, requires_grad=True)\noptimised_damping = torch.tensor(1.0, requires_grad=True)\n# As damping needs to be a strictly positive quantity, we log-encode it\nlog_encoded_damping = torch.log(optimised_damping.detach()).requires_grad_(True)\n\n# First, we'll create an `optimiser` following pytorch convention\noptimiser = torch.optim.Adam([optimised_frequency, log_encoded_damping], lr=1e-2, amsgrad=True)\n\n# Next, we'll define a closure function whose sole purpose is to\n# zero the gradients and compute the error. This is useful as it allows switching to other\n# optimisers such as LBFGS or anything that re-evaluates the error without\n# computing its gradient\ndef sha_closure(minibatch):\n    current_state = initial_x.clone()\n    current_time  = initial_time.clone()\n    optimiser.zero_grad()\n    error = 0.0\n    \n    times = minibatch['times']\n    states = minibatch['states']\n    \n    # We need to sort both times and states simultaneously, so we'll use `argsort`\n    sorted_time_indices = torch.argsort(times)\n    times, states = times[sorted_time_indices], states[sorted_time_indices]\n    \n    for sample_state, sample_time in zip(states, times):\n        new_state, new_time, *_ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, torch.minimum(dt, sample_time - current_time), atol, rtol, optimised_frequency, torch.exp(log_encoded_damping))\n        error = error + torch.linalg.norm(sample_state - new_state)/times.shape[0]\n        current_state, current_time = new_state, new_time\n    if error.requires_grad:\n        error.backward()\n    return error\n\n# We need to set the size of our mini-batches\nbatch_size = 16\n\n# Now we need an optimisation `loop` where we will take steps to minimise the error\n# We set the number of steps proportionally smaller to account for the fact that at each iteration\n# we take `time_dataset.shape[0]//batch_size` steps instead of just 1\nnumber_of_gd_steps = 256*batch_size//time_dataset.shape[0]\n\n# We also need to track the best solution thus far\nbest_error = torch.inf\nbest_frequency, best_damping = optimised_frequency.detach().clone(), optimised_damping.detach().clone()\nfor step in range(number_of_gd_steps):\n    epoch_error = 0.0\n    shuffled_indices = torch.randperm(time_dataset.shape[0])\n    for batch_idx in range(0, time_dataset.shape[0], batch_size):\n        batch_dict = {\n            'times': time_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n            'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n        }\n    \n        step_error = optimiser.step(lambda: sha_closure(batch_dict))\n        epoch_error = epoch_error + step_error.item()*batch_dict['times'].shape[0]\n        print(f\"[{step+1}/{number_of_gd_steps}]/[{batch_idx}/{time_dataset.shape[0]}] Batch Error: {step_error:.6f}, Current Frequency: {optimised_frequency.item():.4f}, Current Damping: {torch.exp(log_encoded_damping).item():.4f}\", end='\\r')\n    epoch_error = epoch_error/time_dataset.shape[0]\n    if epoch_error &lt; best_error:\n        best_error = epoch_error\n        best_frequency = optimised_frequency.detach().clone()\n        best_damping = torch.exp(log_encoded_damping.detach().clone())\n    print(\" \"*128, end=\"\\r\")\n    print(f\"[{step+1}/{number_of_gd_steps}] Epoch Error: {epoch_error:.6f}, Current Frequency: {optimised_frequency.item():.6f}, Current Damping: {torch.exp(log_encoded_damping).item():.6f}\")\n\nprint(f\"Best frequency: {best_frequency.item():.6f}, relative error: {torch.mean(torch.abs(1 - best_frequency / frequency)).item():.6%}\")\nprint(f\"Best damping:   {best_damping.item():.6f}, relative error: {torch.mean(torch.abs(1 - best_damping / damping)).item():.6%}\")\n</pre> time_dataset  = torch.stack([time for _, time in sha_states][1:]) state_dataset = torch.stack([state for state, _ in sha_states][1:]) # We skip the first time as we know the initial state  # We reinitialise our variables optimised_frequency = torch.tensor(0.1, requires_grad=True) optimised_damping = torch.tensor(1.0, requires_grad=True) # As damping needs to be a strictly positive quantity, we log-encode it log_encoded_damping = torch.log(optimised_damping.detach()).requires_grad_(True)  # First, we'll create an `optimiser` following pytorch convention optimiser = torch.optim.Adam([optimised_frequency, log_encoded_damping], lr=1e-2, amsgrad=True)  # Next, we'll define a closure function whose sole purpose is to # zero the gradients and compute the error. This is useful as it allows switching to other # optimisers such as LBFGS or anything that re-evaluates the error without # computing its gradient def sha_closure(minibatch):     current_state = initial_x.clone()     current_time  = initial_time.clone()     optimiser.zero_grad()     error = 0.0          times = minibatch['times']     states = minibatch['states']          # We need to sort both times and states simultaneously, so we'll use `argsort`     sorted_time_indices = torch.argsort(times)     times, states = times[sorted_time_indices], states[sorted_time_indices]          for sample_state, sample_time in zip(states, times):         new_state, new_time, *_ = adaptive_rk45_integrator.apply(sha_dynamics, current_state, current_time, sample_time, torch.minimum(dt, sample_time - current_time), atol, rtol, optimised_frequency, torch.exp(log_encoded_damping))         error = error + torch.linalg.norm(sample_state - new_state)/times.shape[0]         current_state, current_time = new_state, new_time     if error.requires_grad:         error.backward()     return error  # We need to set the size of our mini-batches batch_size = 16  # Now we need an optimisation `loop` where we will take steps to minimise the error # We set the number of steps proportionally smaller to account for the fact that at each iteration # we take `time_dataset.shape[0]//batch_size` steps instead of just 1 number_of_gd_steps = 256*batch_size//time_dataset.shape[0]  # We also need to track the best solution thus far best_error = torch.inf best_frequency, best_damping = optimised_frequency.detach().clone(), optimised_damping.detach().clone() for step in range(number_of_gd_steps):     epoch_error = 0.0     shuffled_indices = torch.randperm(time_dataset.shape[0])     for batch_idx in range(0, time_dataset.shape[0], batch_size):         batch_dict = {             'times': time_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],             'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],         }              step_error = optimiser.step(lambda: sha_closure(batch_dict))         epoch_error = epoch_error + step_error.item()*batch_dict['times'].shape[0]         print(f\"[{step+1}/{number_of_gd_steps}]/[{batch_idx}/{time_dataset.shape[0]}] Batch Error: {step_error:.6f}, Current Frequency: {optimised_frequency.item():.4f}, Current Damping: {torch.exp(log_encoded_damping).item():.4f}\", end='\\r')     epoch_error = epoch_error/time_dataset.shape[0]     if epoch_error &lt; best_error:         best_error = epoch_error         best_frequency = optimised_frequency.detach().clone()         best_damping = torch.exp(log_encoded_damping.detach().clone())     print(\" \"*128, end=\"\\r\")     print(f\"[{step+1}/{number_of_gd_steps}] Epoch Error: {epoch_error:.6f}, Current Frequency: {optimised_frequency.item():.6f}, Current Damping: {torch.exp(log_encoded_damping).item():.6f}\")  print(f\"Best frequency: {best_frequency.item():.6f}, relative error: {torch.mean(torch.abs(1 - best_frequency / frequency)).item():.6%}\") print(f\"Best damping:   {best_damping.item():.6f}, relative error: {torch.mean(torch.abs(1 - best_damping / damping)).item():.6%}\") <pre>[1/33] Epoch Error: 0.628324, Current Frequency: 0.178396, Current Damping: 0.923665                                            \n[2/33] Epoch Error: 0.452757, Current Frequency: 0.250116, Current Damping: 0.852145                                            \n[3/33] Epoch Error: 0.352843, Current Frequency: 0.309024, Current Damping: 0.791153                                            \n[4/33] Epoch Error: 0.302845, Current Frequency: 0.355167, Current Damping: 0.748466                                            \n[5/33] Epoch Error: 0.281069, Current Frequency: 0.391215, Current Damping: 0.717315                                            \n[6/33] Epoch Error: 0.269684, Current Frequency: 0.417420, Current Damping: 0.700159                                            \n[7/33] Epoch Error: 0.264258, Current Frequency: 0.439627, Current Damping: 0.689579                                            \n[8/33] Epoch Error: 0.259327, Current Frequency: 0.458514, Current Damping: 0.684602                                            \n[9/33] Epoch Error: 0.254482, Current Frequency: 0.474936, Current Damping: 0.685596                                            \n[10/33] Epoch Error: 0.250911, Current Frequency: 0.492765, Current Damping: 0.683432                                           \n[11/33] Epoch Error: 0.246691, Current Frequency: 0.510047, Current Damping: 0.685052                                           \n[12/33] Epoch Error: 0.242817, Current Frequency: 0.528726, Current Damping: 0.684228                                           \n[13/33] Epoch Error: 0.238562, Current Frequency: 0.547995, Current Damping: 0.680715                                           \n[14/33] Epoch Error: 0.233992, Current Frequency: 0.567164, Current Damping: 0.677625                                           \n[15/33] Epoch Error: 0.229790, Current Frequency: 0.588169, Current Damping: 0.670971                                           \n[16/33] Epoch Error: 0.224707, Current Frequency: 0.609309, Current Damping: 0.664911                                           \n[17/33] Epoch Error: 0.219676, Current Frequency: 0.631531, Current Damping: 0.656331                                           \n[18/33] Epoch Error: 0.214181, Current Frequency: 0.654534, Current Damping: 0.644180                                           \n[19/33] Epoch Error: 0.208597, Current Frequency: 0.679353, Current Damping: 0.631005                                           \n[20/33] Epoch Error: 0.201547, Current Frequency: 0.703593, Current Damping: 0.618783                                           \n[21/33] Epoch Error: 0.195240, Current Frequency: 0.730725, Current Damping: 0.600403                                           \n[22/33] Epoch Error: 0.186872, Current Frequency: 0.758010, Current Damping: 0.582101                                           \n[23/33] Epoch Error: 0.177497, Current Frequency: 0.785443, Current Damping: 0.562757                                           \n[24/33] Epoch Error: 0.167940, Current Frequency: 0.815522, Current Damping: 0.536682                                           \n[25/33] Epoch Error: 0.155794, Current Frequency: 0.847253, Current Damping: 0.508472                                           \n[26/33] Epoch Error: 0.142342, Current Frequency: 0.881644, Current Damping: 0.473993                                           \n[27/33] Epoch Error: 0.124190, Current Frequency: 0.916691, Current Damping: 0.439423                                           \n[28/33] Epoch Error: 0.104539, Current Frequency: 0.953759, Current Damping: 0.399611                                           \n[29/33] Epoch Error: 0.080617, Current Frequency: 0.991097, Current Damping: 0.358279                                           \n[30/33] Epoch Error: 0.054462, Current Frequency: 1.022754, Current Damping: 0.317208                                           \n[31/33] Epoch Error: 0.034670, Current Frequency: 1.032316, Current Damping: 0.280054                                           \n[32/33] Epoch Error: 0.018756, Current Frequency: 1.004898, Current Damping: 0.255116                                           \n[33/33] Epoch Error: 0.006446, Current Frequency: 0.994217, Current Damping: 0.239427                                           \nBest frequency: 0.994217, relative error: 0.578333%\nBest damping:   0.239427, relative error: 4.229135%\n</pre> In\u00a0[28]: Copied! <pre>_, _, sha_states_ref, sha_times_ref, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, frequency, damping)\nfig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\")\nfig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")\n\n_, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, best_frequency, best_damping)\n_ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\")\n_ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\")\n</pre> _, _, sha_states_ref, sha_times_ref, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, frequency, damping) fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\") fig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")  _, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(sha_dynamics, initial_x, initial_time, final_time, dt, atol, rtol, best_frequency, best_damping) _ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\") _ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\") <p>Excellent, this takes the same number of optimisation steps, but we achieve better results. While at each step, the estimate is noisy as we don't account for all the points, we do gain in terms of convergence speed. Furthermore, we achieve a better error because we're less likely to be trapped in a local minimum.</p> <p>Let's now suppose that we don't know what the dynamics are at all, can we learn the appropriate system? Probably, but right now our method of computing the gradients is exceptionally inefficient, and is the reason why we cannot run our integration at higher precision. To resolve this, we will need to employ the adjoint method.</p> In\u00a0[29]: Copied! <pre>adaptive_rk45_integrator_without_local_extrapolation = get_integrator(torch.tensor([\n    [0.0,       0.0,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [1/5,       1/5,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [3/10,      3/40,        9/40,       0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [4/5,       44/45,      -56/15,      32/9,        0.0,      0.0,          0.0,      0.0 ],\n    [8/9,       19372/6561, -25360/2187, 64448/6561, -212/729,  0.0,          0.0,      0.0 ],\n    [1.0,       9017/3168,  -355/33,     46732/5247,  49/176,  -5103/18656,   0.0,      0.0 ],\n    [1.0,       35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],\n    [torch.inf, 35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],\n    [torch.inf, 5179/57600,  0.0,        7571/16695,  393/640, -92097/339200, 187/2100, 1/40]\n], dtype=torch.float64), integrator_order = 5, use_local_extrapolation = False, integrator_name = \"AdaptiveRK45IntegratorNoLocalExtrapolation\")\nadaptive_rk45_integrator_with_local_extrapolation = get_integrator(adaptive_rk45_integrator_without_local_extrapolation.integrator_tableau, integrator_order = 5, use_local_extrapolation = True, integrator_name = \"AdaptiveRK45IntegratorLocalExtrapolation\")\n</pre> adaptive_rk45_integrator_without_local_extrapolation = get_integrator(torch.tensor([     [0.0,       0.0,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],     [1/5,       1/5,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],     [3/10,      3/40,        9/40,       0.0,         0.0,      0.0,          0.0,      0.0 ],     [4/5,       44/45,      -56/15,      32/9,        0.0,      0.0,          0.0,      0.0 ],     [8/9,       19372/6561, -25360/2187, 64448/6561, -212/729,  0.0,          0.0,      0.0 ],     [1.0,       9017/3168,  -355/33,     46732/5247,  49/176,  -5103/18656,   0.0,      0.0 ],     [1.0,       35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],     [torch.inf, 35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],     [torch.inf, 5179/57600,  0.0,        7571/16695,  393/640, -92097/339200, 187/2100, 1/40] ], dtype=torch.float64), integrator_order = 5, use_local_extrapolation = False, integrator_name = \"AdaptiveRK45IntegratorNoLocalExtrapolation\") adaptive_rk45_integrator_with_local_extrapolation = get_integrator(adaptive_rk45_integrator_without_local_extrapolation.integrator_tableau, integrator_order = 5, use_local_extrapolation = True, integrator_name = \"AdaptiveRK45IntegratorLocalExtrapolation\") In\u00a0[30]: Copied! <pre>dt = (t1 - t0)/1e6\n\n*_, sub_states_upper, sub_times_upper, state_errors_upper = adaptive_rk45_integrator_with_local_extrapolation.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt, torch.tensor(5e-16), torch.tensor(5e-16))\n*_, sub_states_lower, sub_times_lower, state_errors_lower = adaptive_rk45_integrator_without_local_extrapolation.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt, torch.tensor(5e-16), torch.tensor(5e-16))\n\nprint(f\"Error in {adaptive_rk45_integrator_with_local_extrapolation}:    {(sub_states_upper[-1] - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\")\nprint(f\"Error in {adaptive_rk45_integrator_without_local_extrapolation}: {(sub_states_lower[-1] - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\")\n\nreference_trajectory = [neuralode.dynamics.exponential_fn_solution(x0, t) for t in sub_times_upper]\nfig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference([(i[0], j) for i, j in zip(sub_states_upper, sub_times_upper)], reference_trajectory, method_label=\"RK4(5) Method with Local Extrapolation\")\n# axes[1].plot([t.item() for _, t in sub_states_upper], [e.abs().item() for e in state_errors_upper], marker='x', label=\"Estimated Error\")\n# axes[1].legend()\n\nreference_trajectory = [neuralode.dynamics.exponential_fn_solution(x0, t) for t in sub_times_lower]\nfig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference([(i[0], j) for i, j in zip(sub_states_lower, sub_times_lower)], reference_trajectory, method_label=\"RK4(5) Method without Local Extrapolation\", axes=axes)\n# axes[1].plot([t.item() for _, t in sub_states_lower], [e.abs().item() for e in state_errors_lower], marker='x', label=\"Estimated Error\")\n# axes[1].legend()\n</pre> dt = (t1 - t0)/1e6  *_, sub_states_upper, sub_times_upper, state_errors_upper = adaptive_rk45_integrator_with_local_extrapolation.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt, torch.tensor(5e-16), torch.tensor(5e-16)) *_, sub_states_lower, sub_times_lower, state_errors_lower = adaptive_rk45_integrator_without_local_extrapolation.apply(neuralode.dynamics.exponential_fn, x0, t0, t1, dt, torch.tensor(5e-16), torch.tensor(5e-16))  print(f\"Error in {adaptive_rk45_integrator_with_local_extrapolation}:    {(sub_states_upper[-1] - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\") print(f\"Error in {adaptive_rk45_integrator_without_local_extrapolation}: {(sub_states_lower[-1] - neuralode.dynamics.exponential_fn_solution(x0, t1)).abs().item()}\")  reference_trajectory = [neuralode.dynamics.exponential_fn_solution(x0, t) for t in sub_times_upper] fig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference([(i[0], j) for i, j in zip(sub_states_upper, sub_times_upper)], reference_trajectory, method_label=\"RK4(5) Method with Local Extrapolation\") # axes[1].plot([t.item() for _, t in sub_states_upper], [e.abs().item() for e in state_errors_upper], marker='x', label=\"Estimated Error\") # axes[1].legend()  reference_trajectory = [neuralode.dynamics.exponential_fn_solution(x0, t) for t in sub_times_lower] fig, axes = neuralode.plot.trajectory.plot_trajectory_with_reference([(i[0], j) for i, j in zip(sub_states_lower, sub_times_lower)], reference_trajectory, method_label=\"RK4(5) Method without Local Extrapolation\", axes=axes) # axes[1].plot([t.item() for _, t in sub_states_lower], [e.abs().item() for e in state_errors_lower], marker='x', label=\"Estimated Error\") # axes[1].legend() <pre>C:\\Users\\ekin4\\AppData\\Local\\Temp\\ipykernel_5888\\3984266084.py:21: RuntimeWarning: Atol is smaller than the square root of the epsilon for torch.float64, this may increase truncation error\n  warnings.warn(f\"{tol_name.title()} is smaller than the square root of the epsilon for {x0.dtype}, this may increase truncation error\", RuntimeWarning)\nC:\\Users\\ekin4\\AppData\\Local\\Temp\\ipykernel_5888\\3984266084.py:21: RuntimeWarning: Rtol is smaller than the square root of the epsilon for torch.float64, this may increase truncation error\n  warnings.warn(f\"{tol_name.title()} is smaller than the square root of the epsilon for {x0.dtype}, this may increase truncation error\", RuntimeWarning)\n</pre> <pre>Error in &lt;class '__main__.AdaptiveRK45IntegratorLocalExtrapolation'&gt;:    5.551115123125783e-17\nError in &lt;class '__main__.AdaptiveRK45IntegratorNoLocalExtrapolation'&gt;: 4.6129766673175254e-14\n</pre> <p>As you can see, the use of local extrapolation leads to higher precision results over the whole trajectory for the same error tolerances. This usually implies that a) we can take fewer integration steps to achieve the same results and b) we can do so by relaxing the tolerances.</p>"},{"location":"02-arbitrary-adaptive-tableaus/#arbitrary-butcher-tableaus-and-adaptive-integration","title":"Arbitrary Butcher Tableaus and Adaptive Integration\u00b6","text":"<p>In this notebook, we will go through the process of expanding our code in order to allow for different Butcher tableaus and how to implement a step-size adaptation algorithm.</p> <p>We have moved the <code>compensated_sum</code> function into our module under <code>neuralode.util.compensated_sum</code> and added a function <code>neuralode.util.partial_compensated_sum</code> that can be called with the partial sums and truncated bits for iteratively updated the tracked values. This simplifies our integration code by removing the duplication.</p> <p>The plotting code has been moved into <code>neuralode.plot.trajectory.plot_trajectory</code> and <code>neuralode.plot.trajectory.plot_trajectory_with_reference</code>. The <code>neuralode.plot.trajectory.plot_trajectory</code> plots a trajectory without a reference solution which we will encounter with systems that don't have a closed form solution. We will be using the concept of a reference where a higher precision solver can be used to compute the trajectory.</p>"},{"location":"02-arbitrary-adaptive-tableaus/#appendix-local-extrapolation","title":"Appendix - Local Extrapolation\u00b6","text":"<p>If you'll notice, in the above adaptive integration schemes, we have two estimates of the trajectory, a higher and a lower order one. While the error analysis of stepsize adaptation applies to the trajectory of the lower order estimate, in practice it can make sense to use the higher order estimate as it provides improved convergence for many systems. This is referred to as local extrapolation.</p> <p>Below, we demonstrate how local extrapolation effects the error terms and their growth over a trajectory:</p>"},{"location":"03-the-adjoint-method/","title":"The Adjoint Method","text":"In\u00a0[1]: Copied! <pre>import typing\nimport warnings\n\nimport math\nimport torch\nimport einops\nimport neuralode\nimport random\nimport numpy as np\n\nwarnings.simplefilter('ignore', RuntimeWarning)\n</pre> import typing import warnings  import math import torch import einops import neuralode import random import numpy as np  warnings.simplefilter('ignore', RuntimeWarning) In\u00a0[2]: Copied! <pre># For convenience, we define the default tensor device and dtype here\ntorch.set_default_device('cpu')\n# In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training\ntorch.set_default_dtype(torch.float64)\n# Set the random seeds so that each run of this notebook is reproducible\ntorch.manual_seed(1)\nrandom.seed(1)\nnp.random.seed(1)\n</pre> # For convenience, we define the default tensor device and dtype here torch.set_default_device('cpu') # In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training torch.set_default_dtype(torch.float64) # Set the random seeds so that each run of this notebook is reproducible torch.manual_seed(1) random.seed(1) np.random.seed(1) <p>We'll be starting with the same function from the previous notebook, but we have tidied up the code by using <code>neuralode.util.partial_compensated_sum</code> to track the truncated bits instead of duplicating the code through our integration function.</p> <p>We have moved many of the functions into a submodule. Most importantly, the class creation and finalisation have been moved into <code>neuralode.integrators.classes</code>, and the integrators are now subclasses of <code>neuralode.integrators.classes.Integrator</code> which enables us to do type checking and signature checking both statically and at runtime if needed. Duplicated functionality in checking the tolerances has been moved into <code>neuralode.integrators.helpers.ensure_tolerance</code> function, and consistency checking of the timestep has been put into <code>neuralode.integrators.helpers.ensure_timestep</code>.</p> <p>Also, the entire forward integration loop has now been moved into <code>neuralode.integrators.routines.integrate_system</code> which allows us to separate the concerns of <code>torch.autograd.Function</code> and the actual integration (e.g. ctx does not need to be in the same scope). This requires passing around several packaged arguments, but gains us conciseness in the code here, allowing us to focus on the adjoint method implementation.</p> In\u00a0[3]: Copied! <pre>def get_backward_method_adjoint(integrator_type: typing.Type[neuralode.integrators.classes.Integrator]):\n    def __internal_backward(ctx: torch.autograd.function.FunctionCtx,\n                            d_c_state: torch.Tensor,\n                            d_c_time: torch.Tensor,\n                            d_intermediate_states: torch.Tensor,\n                            d_intermediate_times: torch.Tensor,\n                            d_error_in_state: torch.Tensor) -&gt; tuple[torch.Tensor | None]:\n        \"\"\"\n        This function computes the gradient of the input variables for `__internal_forward` by implementing\n        the adjoint method. This involves computing the adjoint state and adjoint state equation and systematically\n        integrating backwards, from t1 to t0, accumulating the gradient to obtain the gradient wrt. the input variables.\n\n        :param ctx: Function context for storing variables and function from the forward pass\n        :param d_c_state: incoming gradient of c_state\n        :param d_c_time: incoming gradient of c_time\n        :param d_intermediate_states: incoming gradient of intermediate_states\n        :param d_intermediate_times: incoming gradient of intermediate_times\n        :param d_error_in_state: incoming gradient of error_in_state. This output is non-differentiable.\n        :return: The gradients wrt. all the inputs.\n        \"\"\"\n\n        # First, we retrieve our integration function that we stored in `__internal_forward`\n        forward_fn: neuralode.integrators.signatures.integration_fn_signature = ctx.forward_fn\n\n        integrator_kwargs = ctx.integrator_kwargs\n\n        # Then we retrieve the input variables\n        (\n            x0,\n            t0,\n            t1,\n            dt,\n            c_state,\n            c_time,\n            intermediate_times,\n            *additional_dynamic_args,\n        ) = ctx.saved_tensors\n\n        inputs = forward_fn, x0, t0, t1, dt, integrator_kwargs, *additional_dynamic_args\n        input_grads: list[torch.Tensor | None] = [None for _ in range(len(inputs))]\n\n        if any(ctx.needs_input_grad):\n            # Construct the adjoint equation\n            adjoint_fn = neuralode.integrators.helpers.construct_adjoint_fn(forward_fn, c_state.shape)\n\n            # We ensure that gradients are enabled so that autograd tracks the variable operations\n            # For pointwise functionals, the initial adjoint state is simply the incoming gradients\n            parameter_shapes = [i.shape for i in additional_dynamic_args]\n            packed_reverse_state = torch.cat(\n                [\n                    c_state.ravel(),\n                    (d_c_state + d_intermediate_states[-1]).ravel(),\n                ]\n            )\n            if len(additional_dynamic_args) &gt; 0:\n                packed_reverse_state = torch.cat(\n                    [\n                        packed_reverse_state,\n                        torch.zeros(\n                            sum(map(math.prod, parameter_shapes)),\n                            device=c_state.device,\n                            dtype=c_state.dtype,\n                        ),\n                    ]\n                )\n\n            current_adj_time = t1\n            current_adj_state = packed_reverse_state\n\n            if torch.any(d_intermediate_states != 0.0):\n                adj_indices = torch.arange(\n                    c_state.numel(), 2 * c_state.numel(), device=c_state.device\n                )\n                # We only need to account for the incoming gradients if any are non-zero\n                for next_adj_time, d_inter_state in zip(\n                    intermediate_times[1:-1].flip(dims=[0]),\n                    d_intermediate_states[1:-1].flip(dims=[0]),\n                ):\n                    # The incoming gradients of the intermediate states are the gradients of the state defined at\n                    # various points in time. For each of these incoming gradients, we need to integrate up to that\n                    # temporal boundary and add them to adjoint state\n                    if torch.all(d_inter_state == 0.0):\n                        # No need to integrate up to the boundary if the incoming gradients are zero\n                        continue\n                    current_adj_state, current_adj_time, _, _, _ = (\n                        integrator_type.apply(\n                            adjoint_fn,\n                            current_adj_state,\n                            current_adj_time,\n                            next_adj_time,\n                            -dt,\n                            integrator_kwargs,\n                            *additional_dynamic_args,\n                        )\n                    )\n                    current_adj_state = torch.scatter(\n                        current_adj_state, 0, adj_indices, d_inter_state.ravel()\n                    )\n\n            final_adj_state, final_adj_time, _, _, _ = integrator_type.apply(\n                adjoint_fn,\n                current_adj_state,\n                current_adj_time,\n                t0,\n                -dt,\n                integrator_kwargs,\n                *additional_dynamic_args,\n            )\n\n            # This should be equivalent to the initial state we passed in, but it will\n            # be appropriately attached to the autograd graph for higher order derivatives\n            if torch.is_grad_enabled() and any(\n                i.requires_grad for i in [d_c_state, d_c_time, d_intermediate_states]\n            ):\n                adj_initial_state = final_adj_state[: c_state.numel()].reshape(\n                    c_state.shape\n                )\n            else:\n                adj_initial_state = x0.clone()\n            adj_variables = final_adj_state[c_state.numel() : 2 * c_state.numel()]\n            adj_parameter_gradients = final_adj_state[2 * c_state.numel() :]\n\n            # The gradients of the incoming state are equal to the gradients from the first element of the\n            # intermediate state plus the lagrange variables\n            initial_state_grads_from_adj = adj_variables.reshape(c_state.shape)\n            initial_state_grads_from_intermediate = d_intermediate_states[0]\n\n            input_grads[1] = (\n                initial_state_grads_from_adj + initial_state_grads_from_intermediate\n            )\n\n            # The gradient of the initial time is equal to the gradient from the first element of the intermediate times\n            # minus the product of the lagrange variables and the derivative of the system at the initial time\n            derivative_at_t0 = forward_fn(\n                adj_initial_state, final_adj_time, *additional_dynamic_args\n            )\n            initial_time_grads_from_ode = torch.sum(\n                adj_variables * derivative_at_t0.ravel()\n            )\n            initial_time_grads_from_intermediate = d_intermediate_times[0].ravel()\n\n            input_grads[2] = (\n                initial_time_grads_from_intermediate - initial_time_grads_from_ode\n            )\n            # The gradient of the final time is equal to the gradient from the gradient in the final state\n            # plus the product of the lagrange variables and the derivative of the system at the final time\n            derivative_at_t1 = forward_fn(c_state, c_time, *additional_dynamic_args)\n            final_time_grads_from_ode = torch.sum(\n                (d_c_state + d_intermediate_states[-1]) * derivative_at_t1\n            )\n            final_time_grads_from_intermediate = d_c_time + d_intermediate_times[-1]\n\n            input_grads[3] = (\n                final_time_grads_from_intermediate + final_time_grads_from_ode\n            )\n\n            parameter_gradients = []\n\n            for p_shape, num_elem in zip(\n                parameter_shapes, map(math.prod, parameter_shapes)\n            ):\n                parameter_gradients.append(\n                    adj_parameter_gradients[:num_elem].reshape(p_shape)\n                )\n                adj_parameter_gradients = adj_parameter_gradients[num_elem:]\n\n            input_grads[6:] = parameter_gradients\n            inputs_grad_not_finite = list(\n                map(\n                    lambda x: False if x is None else (~x.isfinite()).any(), input_grads\n                )\n            )\n            if any(inputs_grad_not_finite):\n                inp_nonfinite_indices = [\n                    inp_idx\n                    for inp_idx, inp_grad_is_not_finite in enumerate(\n                        inputs_grad_not_finite\n                    )\n                    if inp_grad_is_not_finite\n                ]\n                raise ValueError(\n                    f\"Encountered non-finite grads for inputs: {inp_nonfinite_indices}\"\n                )\n        return tuple(input_grads)\n    return __internal_backward\n</pre> def get_backward_method_adjoint(integrator_type: typing.Type[neuralode.integrators.classes.Integrator]):     def __internal_backward(ctx: torch.autograd.function.FunctionCtx,                             d_c_state: torch.Tensor,                             d_c_time: torch.Tensor,                             d_intermediate_states: torch.Tensor,                             d_intermediate_times: torch.Tensor,                             d_error_in_state: torch.Tensor) -&gt; tuple[torch.Tensor | None]:         \"\"\"         This function computes the gradient of the input variables for `__internal_forward` by implementing         the adjoint method. This involves computing the adjoint state and adjoint state equation and systematically         integrating backwards, from t1 to t0, accumulating the gradient to obtain the gradient wrt. the input variables.          :param ctx: Function context for storing variables and function from the forward pass         :param d_c_state: incoming gradient of c_state         :param d_c_time: incoming gradient of c_time         :param d_intermediate_states: incoming gradient of intermediate_states         :param d_intermediate_times: incoming gradient of intermediate_times         :param d_error_in_state: incoming gradient of error_in_state. This output is non-differentiable.         :return: The gradients wrt. all the inputs.         \"\"\"          # First, we retrieve our integration function that we stored in `__internal_forward`         forward_fn: neuralode.integrators.signatures.integration_fn_signature = ctx.forward_fn          integrator_kwargs = ctx.integrator_kwargs          # Then we retrieve the input variables         (             x0,             t0,             t1,             dt,             c_state,             c_time,             intermediate_times,             *additional_dynamic_args,         ) = ctx.saved_tensors          inputs = forward_fn, x0, t0, t1, dt, integrator_kwargs, *additional_dynamic_args         input_grads: list[torch.Tensor | None] = [None for _ in range(len(inputs))]          if any(ctx.needs_input_grad):             # Construct the adjoint equation             adjoint_fn = neuralode.integrators.helpers.construct_adjoint_fn(forward_fn, c_state.shape)              # We ensure that gradients are enabled so that autograd tracks the variable operations             # For pointwise functionals, the initial adjoint state is simply the incoming gradients             parameter_shapes = [i.shape for i in additional_dynamic_args]             packed_reverse_state = torch.cat(                 [                     c_state.ravel(),                     (d_c_state + d_intermediate_states[-1]).ravel(),                 ]             )             if len(additional_dynamic_args) &gt; 0:                 packed_reverse_state = torch.cat(                     [                         packed_reverse_state,                         torch.zeros(                             sum(map(math.prod, parameter_shapes)),                             device=c_state.device,                             dtype=c_state.dtype,                         ),                     ]                 )              current_adj_time = t1             current_adj_state = packed_reverse_state              if torch.any(d_intermediate_states != 0.0):                 adj_indices = torch.arange(                     c_state.numel(), 2 * c_state.numel(), device=c_state.device                 )                 # We only need to account for the incoming gradients if any are non-zero                 for next_adj_time, d_inter_state in zip(                     intermediate_times[1:-1].flip(dims=[0]),                     d_intermediate_states[1:-1].flip(dims=[0]),                 ):                     # The incoming gradients of the intermediate states are the gradients of the state defined at                     # various points in time. For each of these incoming gradients, we need to integrate up to that                     # temporal boundary and add them to adjoint state                     if torch.all(d_inter_state == 0.0):                         # No need to integrate up to the boundary if the incoming gradients are zero                         continue                     current_adj_state, current_adj_time, _, _, _ = (                         integrator_type.apply(                             adjoint_fn,                             current_adj_state,                             current_adj_time,                             next_adj_time,                             -dt,                             integrator_kwargs,                             *additional_dynamic_args,                         )                     )                     current_adj_state = torch.scatter(                         current_adj_state, 0, adj_indices, d_inter_state.ravel()                     )              final_adj_state, final_adj_time, _, _, _ = integrator_type.apply(                 adjoint_fn,                 current_adj_state,                 current_adj_time,                 t0,                 -dt,                 integrator_kwargs,                 *additional_dynamic_args,             )              # This should be equivalent to the initial state we passed in, but it will             # be appropriately attached to the autograd graph for higher order derivatives             if torch.is_grad_enabled() and any(                 i.requires_grad for i in [d_c_state, d_c_time, d_intermediate_states]             ):                 adj_initial_state = final_adj_state[: c_state.numel()].reshape(                     c_state.shape                 )             else:                 adj_initial_state = x0.clone()             adj_variables = final_adj_state[c_state.numel() : 2 * c_state.numel()]             adj_parameter_gradients = final_adj_state[2 * c_state.numel() :]              # The gradients of the incoming state are equal to the gradients from the first element of the             # intermediate state plus the lagrange variables             initial_state_grads_from_adj = adj_variables.reshape(c_state.shape)             initial_state_grads_from_intermediate = d_intermediate_states[0]              input_grads[1] = (                 initial_state_grads_from_adj + initial_state_grads_from_intermediate             )              # The gradient of the initial time is equal to the gradient from the first element of the intermediate times             # minus the product of the lagrange variables and the derivative of the system at the initial time             derivative_at_t0 = forward_fn(                 adj_initial_state, final_adj_time, *additional_dynamic_args             )             initial_time_grads_from_ode = torch.sum(                 adj_variables * derivative_at_t0.ravel()             )             initial_time_grads_from_intermediate = d_intermediate_times[0].ravel()              input_grads[2] = (                 initial_time_grads_from_intermediate - initial_time_grads_from_ode             )             # The gradient of the final time is equal to the gradient from the gradient in the final state             # plus the product of the lagrange variables and the derivative of the system at the final time             derivative_at_t1 = forward_fn(c_state, c_time, *additional_dynamic_args)             final_time_grads_from_ode = torch.sum(                 (d_c_state + d_intermediate_states[-1]) * derivative_at_t1             )             final_time_grads_from_intermediate = d_c_time + d_intermediate_times[-1]              input_grads[3] = (                 final_time_grads_from_intermediate + final_time_grads_from_ode             )              parameter_gradients = []              for p_shape, num_elem in zip(                 parameter_shapes, map(math.prod, parameter_shapes)             ):                 parameter_gradients.append(                     adj_parameter_gradients[:num_elem].reshape(p_shape)                 )                 adj_parameter_gradients = adj_parameter_gradients[num_elem:]              input_grads[6:] = parameter_gradients             inputs_grad_not_finite = list(                 map(                     lambda x: False if x is None else (~x.isfinite()).any(), input_grads                 )             )             if any(inputs_grad_not_finite):                 inp_nonfinite_indices = [                     inp_idx                     for inp_idx, inp_grad_is_not_finite in enumerate(                         inputs_grad_not_finite                     )                     if inp_grad_is_not_finite                 ]                 raise ValueError(                     f\"Encountered non-finite grads for inputs: {inp_nonfinite_indices}\"                 )         return tuple(input_grads)     return __internal_backward In\u00a0[4]: Copied! <pre>def get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True,\n                   integrator_name: str = None) -&gt; typing.Type[torch.autograd.Function]:\n    __integrator_type = neuralode.integrators.classes.create_integrator_class(integrator_tableau, integrator_order,\n                                                        use_local_extrapolation, integrator_name)\n    \n    # Forward integration method\n    __internal_forward = neuralode.integrators.integrators.get_forward_method(__integrator_type, use_local_extrapolation)\n    # Backward integration method\n    __internal_backward = get_backward_method_adjoint(__integrator_type)\n    # Enables batching along arbitrary dimensions using `torch.vmap`\n    __internal_vmap = neuralode.integrators.integrators.get_vmap_method(__integrator_type)\n    \n    neuralode.integrators.classes.finalise_integrator_class(__integrator_type, __internal_forward,\n                                      __internal_backward, __internal_vmap)\n    \n    return __integrator_type\n</pre> def get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True,                    integrator_name: str = None) -&gt; typing.Type[torch.autograd.Function]:     __integrator_type = neuralode.integrators.classes.create_integrator_class(integrator_tableau, integrator_order,                                                         use_local_extrapolation, integrator_name)          # Forward integration method     __internal_forward = neuralode.integrators.integrators.get_forward_method(__integrator_type, use_local_extrapolation)     # Backward integration method     __internal_backward = get_backward_method_adjoint(__integrator_type)     # Enables batching along arbitrary dimensions using `torch.vmap`     __internal_vmap = neuralode.integrators.integrators.get_vmap_method(__integrator_type)          neuralode.integrators.classes.finalise_integrator_class(__integrator_type, __internal_forward,                                       __internal_backward, __internal_vmap)          return __integrator_type In\u00a0[5]: Copied! <pre>initial_position = torch.tensor(1.0)\ninitial_velocity = torch.tensor(0.0)\n\nfrequency = (torch.ones_like(initial_position)).requires_grad_(True)\ndamping = (torch.ones_like(initial_position)*0.25).requires_grad_(True)\ninitial_state = torch.stack([\n    initial_position,\n    initial_velocity,\n], dim=-1).requires_grad_(True)\n\ninitial_time = torch.tensor(0.0).requires_grad_(True)\nfinal_time   = torch.tensor(10.0).requires_grad_(True)\n\ninitial_timestep = (final_time - initial_time) / 100\n\nadaptive_rk45_integrator = get_integrator(torch.tensor([\n    [0.0,       0.0,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [1/5,       1/5,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [3/10,      3/40,        9/40,       0.0,         0.0,      0.0,          0.0,      0.0 ],\n    [4/5,       44/45,      -56/15,      32/9,        0.0,      0.0,          0.0,      0.0 ],\n    [8/9,       19372/6561, -25360/2187, 64448/6561, -212/729,  0.0,          0.0,      0.0 ],\n    [1.0,       9017/3168,  -355/33,     46732/5247,  49/176,  -5103/18656,   0.0,      0.0 ],\n    [1.0,       35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],\n    [torch.inf, 35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],\n    [torch.inf, 5179/57600,  0.0,        7571/16695,  393/640, -92097/339200, 187/2100, 1/40]\n], dtype=torch.float64), integrator_order = 5, integrator_name = \"AdaptiveRK45Integrator\")\n\n\natol = torch.tensor(0.0)\nrtol = torch.tensor(5e-8)\nintegrator_kwargs = {'atol': atol, 'rtol': rtol}\n</pre> initial_position = torch.tensor(1.0) initial_velocity = torch.tensor(0.0)  frequency = (torch.ones_like(initial_position)).requires_grad_(True) damping = (torch.ones_like(initial_position)*0.25).requires_grad_(True) initial_state = torch.stack([     initial_position,     initial_velocity, ], dim=-1).requires_grad_(True)  initial_time = torch.tensor(0.0).requires_grad_(True) final_time   = torch.tensor(10.0).requires_grad_(True)  initial_timestep = (final_time - initial_time) / 100  adaptive_rk45_integrator = get_integrator(torch.tensor([     [0.0,       0.0,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],     [1/5,       1/5,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],     [3/10,      3/40,        9/40,       0.0,         0.0,      0.0,          0.0,      0.0 ],     [4/5,       44/45,      -56/15,      32/9,        0.0,      0.0,          0.0,      0.0 ],     [8/9,       19372/6561, -25360/2187, 64448/6561, -212/729,  0.0,          0.0,      0.0 ],     [1.0,       9017/3168,  -355/33,     46732/5247,  49/176,  -5103/18656,   0.0,      0.0 ],     [1.0,       35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],     [torch.inf, 35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],     [torch.inf, 5179/57600,  0.0,        7571/16695,  393/640, -92097/339200, 187/2100, 1/40] ], dtype=torch.float64), integrator_order = 5, integrator_name = \"AdaptiveRK45Integrator\")   atol = torch.tensor(0.0) rtol = torch.tensor(5e-8) integrator_kwargs = {'atol': atol, 'rtol': rtol} In\u00a0[6]: Copied! <pre>final_state, _, sha_states, sha_times, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, integrator_kwargs, frequency, damping)\n</pre> final_state, _, sha_states, sha_times, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, integrator_kwargs, frequency, damping) In\u00a0[7]: Copied! <pre>fig, axes = neuralode.plot.trajectory.plot_trajectory([(i, j) for i, j in zip(sha_states, sha_times)], method_label=\"RK4(5) - Simple Harmonic Oscillator\")\n</pre> fig, axes = neuralode.plot.trajectory.plot_trajectory([(i, j) for i, j in zip(sha_states, sha_times)], method_label=\"RK4(5) - Simple Harmonic Oscillator\") <p>The Jacobian is the matrix of gradients of each output with respect to each input. For given vector function $\\vec{f}(\\vec{x})$, the Jacobian is:</p> <p>$$ J =  \\begin{bmatrix} \\frac{\\partial{f_1}}{\\partial{x_1}} &amp; \\cdots &amp; \\frac{\\partial{f_1}}{\\partial{x_n}} \\\\ &amp; \\vdots &amp; \\\\ \\frac{\\partial{f_m}}{\\partial{x_1}} &amp; \\cdots &amp; \\frac{\\partial{f_m}}{\\partial{x_n}} \\\\ \\end{bmatrix} $$</p> <p>where $\\frac{\\partial{f_m}}{\\partial{x_n}}$ is the (partial) derivative of the $m^{th}$ component of $\\vec{f}$ with respect to the $n^{th}$ component of $\\vec{x}$. What this matrix tells us how $\\vec{f}$ when $\\vec{x}$ changes.</p> <p>If we think of our integration as a function $\\vec{f}$, the Jacobian indicates how the result of our integration changes with respect to the inputs. We can compute it by creating a function that simply returns our trajectory and then using the <code>pytorch.autograd</code> functional API as follows.</p> In\u00a0[8]: Copied! <pre>def compute_trajectory_return_final_state(x, t0, t1, f, d):\n    return adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, x, t0, t1, initial_timestep, {'atol': atol, 'rtol': rtol*1e-3}, f, d)[0][0]\n\ndef compute_trajectory_return_intermediate_state(x, t0, t1, f, d):\n    return adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, x, t0, t1, initial_timestep, {'atol': atol, 'rtol': rtol*1e-3}, f, d)[2][3][0]\n\ntest_variables = [initial_state.clone(), initial_time.clone(), final_time.clone(), frequency.clone(), damping.clone()]\ntest_variables = [i.detach().clone().requires_grad_(True) for i in test_variables]\n\njac_wrt_initial_state, jac_wrt_initial_time, jac_wrt_final_time, jac_wrt_freq, jac_wrt_damp = torch.autograd.functional.jacobian(compute_trajectory_return_final_state, tuple(test_variables))\nprint(f\"The Jacobian of x(t1) wrt. x(t0) is: {jac_wrt_initial_state.cpu().numpy()}\")\nprint(f\"The Jacobian of x(t1) wrt. t0 is: {jac_wrt_initial_time.cpu().numpy()}\")\nprint(f\"The Jacobian of x(t1) wrt. t1 is: {jac_wrt_final_time.cpu().numpy()}\")\nprint(f\"The Jacobian of x(t1) wrt. frequency is: {jac_wrt_freq.cpu().numpy()}\")\nprint(f\"The Jacobian of x(t1) wrt. damping is: {jac_wrt_damp.cpu().numpy()}\")\n\njac_wrt_initial_state, jac_wrt_initial_time, jac_wrt_final_time, jac_wrt_freq, jac_wrt_damp = torch.autograd.functional.jacobian(compute_trajectory_return_intermediate_state, tuple(test_variables))\nprint()\nprint(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. x(t0) is: {jac_wrt_initial_state.cpu().numpy()}\")\nprint(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. t0 is: {jac_wrt_initial_time.cpu().numpy()}\")\nprint(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. t1 is: {jac_wrt_final_time.cpu().numpy()}\")\nprint(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. frequency is: {jac_wrt_freq.cpu().numpy()}\")\nprint(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. damping is: {jac_wrt_damp.cpu().numpy()}\")\n</pre> def compute_trajectory_return_final_state(x, t0, t1, f, d):     return adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, x, t0, t1, initial_timestep, {'atol': atol, 'rtol': rtol*1e-3}, f, d)[0][0]  def compute_trajectory_return_intermediate_state(x, t0, t1, f, d):     return adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, x, t0, t1, initial_timestep, {'atol': atol, 'rtol': rtol*1e-3}, f, d)[2][3][0]  test_variables = [initial_state.clone(), initial_time.clone(), final_time.clone(), frequency.clone(), damping.clone()] test_variables = [i.detach().clone().requires_grad_(True) for i in test_variables]  jac_wrt_initial_state, jac_wrt_initial_time, jac_wrt_final_time, jac_wrt_freq, jac_wrt_damp = torch.autograd.functional.jacobian(compute_trajectory_return_final_state, tuple(test_variables)) print(f\"The Jacobian of x(t1) wrt. x(t0) is: {jac_wrt_initial_state.cpu().numpy()}\") print(f\"The Jacobian of x(t1) wrt. t0 is: {jac_wrt_initial_time.cpu().numpy()}\") print(f\"The Jacobian of x(t1) wrt. t1 is: {jac_wrt_final_time.cpu().numpy()}\") print(f\"The Jacobian of x(t1) wrt. frequency is: {jac_wrt_freq.cpu().numpy()}\") print(f\"The Jacobian of x(t1) wrt. damping is: {jac_wrt_damp.cpu().numpy()}\")  jac_wrt_initial_state, jac_wrt_initial_time, jac_wrt_final_time, jac_wrt_freq, jac_wrt_damp = torch.autograd.functional.jacobian(compute_trajectory_return_intermediate_state, tuple(test_variables)) print() print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. x(t0) is: {jac_wrt_initial_state.cpu().numpy()}\") print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. t0 is: {jac_wrt_initial_time.cpu().numpy()}\") print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. t1 is: {jac_wrt_final_time.cpu().numpy()}\") print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. frequency is: {jac_wrt_freq.cpu().numpy()}\") print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. damping is: {jac_wrt_damp.cpu().numpy()}\") <pre>The Jacobian of x(t1) wrt. x(t0) is: [-0.08477596 -0.02160443]\nThe Jacobian of x(t1) wrt. t0 is: -0.021604426103042366\nThe Jacobian of x(t1) wrt. t1 is: 0.021604426126233627\nThe Jacobian of x(t1) wrt. frequency is: 0.2160442607119664\nThe Jacobian of x(t1) wrt. damping is: 0.823620406656798\n\nThe Jacobian of x(2.9702e-01) wrt. x(t0) is: [0.9986817  0.05089237]\nThe Jacobian of x(2.9702e-01) wrt. t0 is: 0.05089236517502573\nThe Jacobian of x(2.9702e-01) wrt. t1 is: 0.0\nThe Jacobian of x(2.9702e-01) wrt. frequency is: -0.0026247345057088646\nThe Jacobian of x(2.9702e-01) wrt. damping is: 4.513039652480764e-05\n</pre> <p>We can further use the <code>torch.autograd.gradcheck</code> function to test that these Jacobians are correct. The basic overview of <code>gradcheck</code> is that it computes the gradient using finite differences^[1] and then compares them to our implementation of the gradient. Within some numerical tolerance, these should be identical and this function can test this. We can also test that the gradient of the gradient is correct by using a similar procedure and the <code>torch.autograd.gradgradcheck</code> function.</p> <p>The three cases we need to test are if the initial state, the final state and an arbitrarily selected intermediate state are correctly differentiated. We also need to test these with tighter tolerances than our previous integration as we need to minimise the error accumulated during both the forward and the backward integration.</p> <p>[1] - Finite Difference on Wikipedia</p> In\u00a0[9]: Copied! <pre>def test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp):\n    # Integrates the system with tighter tolerances\n    res = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, init_state, integration_t0, integration_t1, initial_timestep.double()*1e-2, {'atol': atol.double(), 'rtol': rtol.double()*1e-3}, freq, damp)\n    return res\n\ndef test_func_initial_state(init_state, integration_t0, integration_t1, freq, damp):\n    # Integrates the system and returns the initial state stored in the intermediate states\n    res = test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp)\n    return res[2][0]\n\ndef test_func_intermediate_state(init_state, integration_t0, _, freq, damp):\n    # Ideally we'd pick from the intermediate states tensor, but during finite differencing \n    # the time values may change and as a result we will sample the wrong point in the trajectory.\n    # The solution to this is to treat the system as being integrated to some intermediate time and return\n    # the final state.\n    res = test_func_adaptive(init_state, integration_t0, sha_times[sha_times.shape[0]//2], freq, damp)\n    return res[0]\n    \ndef test_func_final_state(init_state, integration_t0, integration_t1, freq, damp):\n    # Return the final state of the integration\n    res = test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp)\n    return res[0]\n</pre> def test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp):     # Integrates the system with tighter tolerances     res = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, init_state, integration_t0, integration_t1, initial_timestep.double()*1e-2, {'atol': atol.double(), 'rtol': rtol.double()*1e-3}, freq, damp)     return res  def test_func_initial_state(init_state, integration_t0, integration_t1, freq, damp):     # Integrates the system and returns the initial state stored in the intermediate states     res = test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp)     return res[2][0]  def test_func_intermediate_state(init_state, integration_t0, _, freq, damp):     # Ideally we'd pick from the intermediate states tensor, but during finite differencing      # the time values may change and as a result we will sample the wrong point in the trajectory.     # The solution to this is to treat the system as being integrated to some intermediate time and return     # the final state.     res = test_func_adaptive(init_state, integration_t0, sha_times[sha_times.shape[0]//2], freq, damp)     return res[0]      def test_func_final_state(init_state, integration_t0, integration_t1, freq, damp):     # Return the final state of the integration     res = test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp)     return res[0] In\u00a0[10]: Copied! <pre>from torch.autograd import gradcheck\n\ntest_variables = [initial_state, initial_time, initial_time.detach().clone()+0.1, frequency, damping]\n# 64-bit floats are required for correct evaluation of finite differences and derivatives\ntest_variables = [i.double() for i in test_variables]\ntest_functions = [test_func_final_state, test_func_initial_state, test_func_intermediate_state]\n\ndef generate_test_vars():\n    # Randomly generate difference integration conditions\n    test_t0 = (torch.rand_like(initial_time.double()) - 1.0)*0.1\n    test_t1 = torch.rand_like(initial_time.double())*0.1 + test_t0.detach()\n    test_x = 2*torch.rand_like(initial_state.double()) - 1.0\n    test_frequency = torch.rand_like(frequency.double())\n    test_damping = torch.rand_like(damping.double())\n    return [i.requires_grad_(True) for i in [test_x, test_t0, test_t1, test_frequency, test_damping]]\n\nnum_tests = 16\n\n# Run test on our initial conditions defined earlier\nprint(f\"[0/{num_tests}] - vars: {[i.detach().cpu().numpy() for i in test_variables]}, success_jacobian: [\", end='')\nfor fn in test_functions:\n    print(gradcheck(fn, [i.detach().clone().requires_grad_(True) for i in test_variables]), end=', ' if fn != test_functions[-1] else '')\nprint(']')\n      \n# Run test on the randomly generated states\nfor iter_idx in range(num_tests):\n    variables = generate_test_vars()\n    print(f\"[{iter_idx+1}/{num_tests}] - vars: {[i.detach().cpu().numpy() for i in variables]}, success: [\", end='')\n    for fn in test_functions:\n        print(gradcheck(fn, variables), end=', ' if fn != test_functions[-1] else '')\n    print(']')\n</pre> from torch.autograd import gradcheck  test_variables = [initial_state, initial_time, initial_time.detach().clone()+0.1, frequency, damping] # 64-bit floats are required for correct evaluation of finite differences and derivatives test_variables = [i.double() for i in test_variables] test_functions = [test_func_final_state, test_func_initial_state, test_func_intermediate_state]  def generate_test_vars():     # Randomly generate difference integration conditions     test_t0 = (torch.rand_like(initial_time.double()) - 1.0)*0.1     test_t1 = torch.rand_like(initial_time.double())*0.1 + test_t0.detach()     test_x = 2*torch.rand_like(initial_state.double()) - 1.0     test_frequency = torch.rand_like(frequency.double())     test_damping = torch.rand_like(damping.double())     return [i.requires_grad_(True) for i in [test_x, test_t0, test_t1, test_frequency, test_damping]]  num_tests = 16  # Run test on our initial conditions defined earlier print(f\"[0/{num_tests}] - vars: {[i.detach().cpu().numpy() for i in test_variables]}, success_jacobian: [\", end='') for fn in test_functions:     print(gradcheck(fn, [i.detach().clone().requires_grad_(True) for i in test_variables]), end=', ' if fn != test_functions[-1] else '') print(']')        # Run test on the randomly generated states for iter_idx in range(num_tests):     variables = generate_test_vars()     print(f\"[{iter_idx+1}/{num_tests}] - vars: {[i.detach().cpu().numpy() for i in variables]}, success: [\", end='')     for fn in test_functions:         print(gradcheck(fn, variables), end=', ' if fn != test_functions[-1] else '')     print(']') <pre>[0/16] - vars: [array([1., 0.]), array(0.), array(0.1), array(1.), array(0.25)], success_jacobian: [True, True, True]\n[1/16] - vars: [array([-0.53149497, -0.64580155]), array(-0.09389467), array(-0.07143922), array(0.55606806), array(0.10944385)], success: [True, True, True]\n[2/16] - vars: [array([ 0.15955251, -0.00666538]), array(-0.05390871), array(0.01692782), array(0.51037517), array(0.32953764)], success: [True, True, True]\n[3/16] - vars: [array([-0.8204064 , -0.76508816]), array(-0.02817939), array(0.01027172), array(0.64023946), array(0.19676658)], success: [True, True, True]\n[4/16] - vars: [array([0.84974453, 0.999399  ]), array(-0.04875527), array(0.02242855), array(0.89273028), array(0.87671972)], success: [True, True, True]\n[5/16] - vars: [array([-0.65892713,  0.96839645]), array(-0.01550284), array(-5.80456654e-05), array(0.81270555), array(0.43584903)], success: [True, True, True]\n[6/16] - vars: [array([0.51552335, 0.84502525]), array(-0.05856788), array(-0.01572713), array(0.964327), array(0.17601831)], success: [True, True, True]\n[7/16] - vars: [array([-0.09120461, -0.40889585]), array(-0.0046106), array(0.02672735), array(0.18750747), array(0.24325766)], success: [True, True, True]\n[8/16] - vars: [array([-0.18625443, -0.42812303]), array(-0.06507037), array(-0.02066313), array(0.8035932), array(0.32176585)], success: [True, True, True]\n[9/16] - vars: [array([ 0.32706173, -0.48966606]), array(-0.06360974), array(-0.03375875), array(0.41437192), array(0.83955493)], success: [True, True, True]\n[10/16] - vars: [array([5.85717787e-01, 2.32655082e-04]), array(-0.02581675), array(0.00283237), array(0.89774049), array(0.10512458)], success: [True, True, True]\n[11/16] - vars: [array([-0.73695231, -0.52172514]), array(-0.04190862), array(0.05675738), array(0.30468405), array(0.51584484)], success: [True, True, True]\n[12/16] - vars: [array([ 0.0601311 , -0.47056019]), array(-0.05485591), array(-0.00556665), array(0.16711785), array(0.54819128)], success: [True, True, True]\n[13/16] - vars: [array([-0.11568781,  0.29077878]), array(-0.07620483), array(-0.02246851), array(0.53756922), array(0.22447995)], success: [True, True, True]\n[14/16] - vars: [array([-0.97824889, -0.43864284]), array(-0.03368136), array(0.05070647), array(0.93011158), array(0.54379786)], success: [True, True, True]\n[15/16] - vars: [array([0.46151627, 0.98484136]), array(-0.01876734), array(0.05872961), array(0.72818875), array(0.23283437)], success: [True, True, True]\n[16/16] - vars: [array([-0.15990223,  0.08383269]), array(-2.5330212e-05), array(0.05537509), array(0.86417502), array(0.43124712)], success: [True, True, True]\n</pre> In\u00a0[11]: Copied! <pre># Generate reference trajectory for optimisation/learning\nwith torch.no_grad():\n    _, _, sha_states_ref, sha_times_ref, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, integrator_kwargs, frequency, damping)\nsha_states_ref, sha_times_ref = sha_states_ref.detach(), sha_times_ref.detach()\n</pre> # Generate reference trajectory for optimisation/learning with torch.no_grad():     _, _, sha_states_ref, sha_times_ref, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, integrator_kwargs, frequency, damping) sha_states_ref, sha_times_ref = sha_states_ref.detach(), sha_times_ref.detach() In\u00a0[12]: Copied! <pre>state_dataset = sha_states_ref[1:].clone()\ntime_dataset  = sha_times_ref[1:].clone()\n\n# Next, we'll define a closure function whose sole purpose is to\n# zero the gradients and compute the error. This is useful as it allows switching to other\n# optimizers such as LBFGS or anything that re-evaluates the error without\n# computing its gradient\ndef sha_closure(rhs: neuralode.integrators.signatures.integration_fn_signature, parameters: list[torch.Tensor], minibatch: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the error on a minibatch of states and times according to the given function `rhs` and its parameters.\n    :param rhs: The function to integrate\n    :param parameters: The parameters wrt. which the gradients should be computed\n    :param minibatch: The minibatch to compute the error on\n    :return: The error of the integration on the minibatch\n    \"\"\"\n    current_state = initial_state.detach().clone()\n    current_time  = initial_time.detach().clone()\n    optimiser.zero_grad()\n    error = 0.0\n    \n    times = minibatch['times']\n    states = minibatch['states']\n    \n    # We need to sort both times and states simultaneously, so we'll use `argsort`\n    sorted_time_indices = torch.argsort(times)\n    times, states = times[sorted_time_indices], states[sorted_time_indices]\n    \n    for sample_state, sample_time in zip(states, times):\n        dt = torch.minimum(initial_timestep, sample_time - current_time).detach()\n        current_state, current_time, _, _, _ = adaptive_rk45_integrator.apply(rhs, current_state, current_time, sample_time, dt, integrator_kwargs, *parameters)\n        error = error + torch.linalg.norm(sample_state - current_state)/times.shape[0]\n    if error.requires_grad:\n        error.backward()\n    return error\n\n# We need to set the size of our mini-batches\nbatch_size = 4\n\n# Now we need an optimisation `loop` where we will take steps to minimise the error\nnumber_of_gd_steps = 128\n</pre> state_dataset = sha_states_ref[1:].clone() time_dataset  = sha_times_ref[1:].clone()  # Next, we'll define a closure function whose sole purpose is to # zero the gradients and compute the error. This is useful as it allows switching to other # optimizers such as LBFGS or anything that re-evaluates the error without # computing its gradient def sha_closure(rhs: neuralode.integrators.signatures.integration_fn_signature, parameters: list[torch.Tensor], minibatch: dict[str, torch.Tensor]) -&gt; torch.Tensor:     \"\"\"     Computes the error on a minibatch of states and times according to the given function `rhs` and its parameters.     :param rhs: The function to integrate     :param parameters: The parameters wrt. which the gradients should be computed     :param minibatch: The minibatch to compute the error on     :return: The error of the integration on the minibatch     \"\"\"     current_state = initial_state.detach().clone()     current_time  = initial_time.detach().clone()     optimiser.zero_grad()     error = 0.0          times = minibatch['times']     states = minibatch['states']          # We need to sort both times and states simultaneously, so we'll use `argsort`     sorted_time_indices = torch.argsort(times)     times, states = times[sorted_time_indices], states[sorted_time_indices]          for sample_state, sample_time in zip(states, times):         dt = torch.minimum(initial_timestep, sample_time - current_time).detach()         current_state, current_time, _, _, _ = adaptive_rk45_integrator.apply(rhs, current_state, current_time, sample_time, dt, integrator_kwargs, *parameters)         error = error + torch.linalg.norm(sample_state - current_state)/times.shape[0]     if error.requires_grad:         error.backward()     return error  # We need to set the size of our mini-batches batch_size = 4  # Now we need an optimisation `loop` where we will take steps to minimise the error number_of_gd_steps = 128 In\u00a0[13]: Copied! <pre># We reinitialise our variables\noptimised_frequency = torch.tensor(0.1, requires_grad=True)\noptimised_damping = torch.tensor(1.0, requires_grad=True)\n# As damping needs to be a strictly positive quantity, we log-encode it\nlog_encoded_damping = torch.log(optimised_damping.detach()).requires_grad_(True)\n\n# First, we'll create an `optimiser` following pytorch convention\noptimiser = torch.optim.Adam([optimised_frequency, log_encoded_damping], lr=1e-1, amsgrad=True)\n# Whenever the loss plateaus, we can reduce the learning rate to improve convergence\nlr_on_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser)\n\n# We also need to track the best solution thus far\nbest_error = torch.inf\nbest_frequency, best_damping = optimised_frequency.detach().clone(), optimised_damping.detach().clone()\nfor step in range(number_of_gd_steps):\n    epoch_error = 0.0\n    shuffled_indices = torch.randperm(time_dataset.shape[0])\n    for batch_idx in range(0, time_dataset.shape[0], batch_size):\n        batch_dict = {\n            'times': time_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n            'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n        }\n    \n        step_error = optimiser.step(lambda: sha_closure(neuralode.dynamics.simple_harmonic_oscillator, [optimised_frequency, torch.exp(log_encoded_damping)], batch_dict))\n        epoch_error = epoch_error + step_error.item()*batch_dict['times'].shape[0]\n        print(f\"[{step+1}/{number_of_gd_steps}]/[{batch_idx}/{time_dataset.shape[0]}] Batch Error: {step_error:.6f}, Current Frequency: {optimised_frequency.item():.4f}, Current Damping: {torch.exp(log_encoded_damping).item():.4f}\", end='\\r')\n    epoch_error = epoch_error/time_dataset.shape[0]\n    if epoch_error &lt; best_error:\n        best_error = epoch_error\n        best_frequency = optimised_frequency.detach().clone()\n        best_damping = torch.exp(log_encoded_damping.detach().clone())\n    lr_on_plateau.step(epoch_error)\n    print(\" \"*128, end=\"\\r\")\n    print(f\"[{step+1}/{number_of_gd_steps} - lr: {lr_on_plateau.get_last_lr()[0]:.4e}] Epoch Error: {epoch_error:.6f}, Current Frequency: {optimised_frequency.item():.6f}, Current Damping: {torch.exp(log_encoded_damping).item():.6f}\")\n    # If the step size is too small, then we can interrupt the\n    # training as it will not lead to significant improvements\n    if lr_on_plateau.get_last_lr()[0] &lt; 1e-6:\n        break\n\nrel_err = torch.mean(torch.abs(1 - best_frequency / frequency)).item()\nmae_err = torch.mean(torch.abs(frequency - best_frequency)).item()\nprint(f\"Best frequency: {best_frequency.item():.6f}, relative error: {rel_err:.6%}, mean absolute error: {mae_err:.6f}\")\nrel_err = torch.mean(torch.abs(1 - best_damping / damping)).item()\nmae_err = torch.mean(torch.abs(damping - best_damping)).item()\nprint(f\"Best damping:   {best_damping.item():.6f}, relative error: {rel_err:.6%}, mean absolute error: {mae_err:.6f}\")\n</pre> # We reinitialise our variables optimised_frequency = torch.tensor(0.1, requires_grad=True) optimised_damping = torch.tensor(1.0, requires_grad=True) # As damping needs to be a strictly positive quantity, we log-encode it log_encoded_damping = torch.log(optimised_damping.detach()).requires_grad_(True)  # First, we'll create an `optimiser` following pytorch convention optimiser = torch.optim.Adam([optimised_frequency, log_encoded_damping], lr=1e-1, amsgrad=True) # Whenever the loss plateaus, we can reduce the learning rate to improve convergence lr_on_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser)  # We also need to track the best solution thus far best_error = torch.inf best_frequency, best_damping = optimised_frequency.detach().clone(), optimised_damping.detach().clone() for step in range(number_of_gd_steps):     epoch_error = 0.0     shuffled_indices = torch.randperm(time_dataset.shape[0])     for batch_idx in range(0, time_dataset.shape[0], batch_size):         batch_dict = {             'times': time_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],             'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],         }              step_error = optimiser.step(lambda: sha_closure(neuralode.dynamics.simple_harmonic_oscillator, [optimised_frequency, torch.exp(log_encoded_damping)], batch_dict))         epoch_error = epoch_error + step_error.item()*batch_dict['times'].shape[0]         print(f\"[{step+1}/{number_of_gd_steps}]/[{batch_idx}/{time_dataset.shape[0]}] Batch Error: {step_error:.6f}, Current Frequency: {optimised_frequency.item():.4f}, Current Damping: {torch.exp(log_encoded_damping).item():.4f}\", end='\\r')     epoch_error = epoch_error/time_dataset.shape[0]     if epoch_error &lt; best_error:         best_error = epoch_error         best_frequency = optimised_frequency.detach().clone()         best_damping = torch.exp(log_encoded_damping.detach().clone())     lr_on_plateau.step(epoch_error)     print(\" \"*128, end=\"\\r\")     print(f\"[{step+1}/{number_of_gd_steps} - lr: {lr_on_plateau.get_last_lr()[0]:.4e}] Epoch Error: {epoch_error:.6f}, Current Frequency: {optimised_frequency.item():.6f}, Current Damping: {torch.exp(log_encoded_damping).item():.6f}\")     # If the step size is too small, then we can interrupt the     # training as it will not lead to significant improvements     if lr_on_plateau.get_last_lr()[0] &lt; 1e-6:         break  rel_err = torch.mean(torch.abs(1 - best_frequency / frequency)).item() mae_err = torch.mean(torch.abs(frequency - best_frequency)).item() print(f\"Best frequency: {best_frequency.item():.6f}, relative error: {rel_err:.6%}, mean absolute error: {mae_err:.6f}\") rel_err = torch.mean(torch.abs(1 - best_damping / damping)).item() mae_err = torch.mean(torch.abs(damping - best_damping)).item() print(f\"Best damping:   {best_damping.item():.6f}, relative error: {rel_err:.6%}, mean absolute error: {mae_err:.6f}\") <pre>[1/128 - lr: 1.0000e-01] Epoch Error: 0.321610, Current Frequency: 1.251675, Current Damping: 0.337432                          \n[2/128 - lr: 1.0000e-01] Epoch Error: 0.077131, Current Frequency: 0.995374, Current Damping: 0.271970                          \n[3/128 - lr: 1.0000e-01] Epoch Error: 0.012469, Current Frequency: 1.007481, Current Damping: 0.247264                          \n[4/128 - lr: 1.0000e-01] Epoch Error: 0.010313, Current Frequency: 0.993685, Current Damping: 0.251497                          \n[5/128 - lr: 1.0000e-01] Epoch Error: 0.015036, Current Frequency: 0.987936, Current Damping: 0.255128                          \n[6/128 - lr: 1.0000e-01] Epoch Error: 0.015201, Current Frequency: 0.992519, Current Damping: 0.247674                          \n[7/128 - lr: 1.0000e-01] Epoch Error: 0.014721, Current Frequency: 1.003300, Current Damping: 0.246292                          \n[8/128 - lr: 1.0000e-01] Epoch Error: 0.010783, Current Frequency: 1.003838, Current Damping: 0.252907                          \n[9/128 - lr: 1.0000e-01] Epoch Error: 0.005755, Current Frequency: 0.997486, Current Damping: 0.244679                          \n[10/128 - lr: 1.0000e-01] Epoch Error: 0.009421, Current Frequency: 0.995029, Current Damping: 0.253222                         \n[11/128 - lr: 1.0000e-01] Epoch Error: 0.018796, Current Frequency: 0.999096, Current Damping: 0.247862                         \n[12/128 - lr: 1.0000e-01] Epoch Error: 0.012960, Current Frequency: 1.006188, Current Damping: 0.246765                         \n[13/128 - lr: 1.0000e-01] Epoch Error: 0.012631, Current Frequency: 0.997460, Current Damping: 0.251612                         \n[14/128 - lr: 1.0000e-01] Epoch Error: 0.012186, Current Frequency: 1.004606, Current Damping: 0.252483                         \n[15/128 - lr: 1.0000e-01] Epoch Error: 0.007520, Current Frequency: 1.009415, Current Damping: 0.247402                         \n[16/128 - lr: 1.0000e-01] Epoch Error: 0.011840, Current Frequency: 0.986589, Current Damping: 0.253592                         \n[17/128 - lr: 1.0000e-01] Epoch Error: 0.013378, Current Frequency: 1.000095, Current Damping: 0.243079                         \n[18/128 - lr: 1.0000e-01] Epoch Error: 0.013211, Current Frequency: 1.008167, Current Damping: 0.250376                         \n[19/128 - lr: 1.0000e-01] Epoch Error: 0.009024, Current Frequency: 0.999350, Current Damping: 0.247020                         \n[20/128 - lr: 1.0000e-02] Epoch Error: 0.006457, Current Frequency: 0.997652, Current Damping: 0.254450                         \n[21/128 - lr: 1.0000e-02] Epoch Error: 0.002457, Current Frequency: 1.000375, Current Damping: 0.249883                         \n[22/128 - lr: 1.0000e-02] Epoch Error: 0.001674, Current Frequency: 1.001694, Current Damping: 0.250331                         \n[23/128 - lr: 1.0000e-02] Epoch Error: 0.001102, Current Frequency: 1.000206, Current Damping: 0.249461                         \n[24/128 - lr: 1.0000e-02] Epoch Error: 0.000884, Current Frequency: 0.999689, Current Damping: 0.249995                         \n[25/128 - lr: 1.0000e-02] Epoch Error: 0.000733, Current Frequency: 1.000436, Current Damping: 0.249912                         \n[26/128 - lr: 1.0000e-02] Epoch Error: 0.001635, Current Frequency: 0.997968, Current Damping: 0.250234                         \n[27/128 - lr: 1.0000e-02] Epoch Error: 0.000655, Current Frequency: 0.999234, Current Damping: 0.249902                         \n[28/128 - lr: 1.0000e-02] Epoch Error: 0.000835, Current Frequency: 1.001496, Current Damping: 0.249199                         \n[29/128 - lr: 1.0000e-02] Epoch Error: 0.001248, Current Frequency: 1.000008, Current Damping: 0.250116                         \n[30/128 - lr: 1.0000e-02] Epoch Error: 0.000784, Current Frequency: 1.000849, Current Damping: 0.249335                         \n[31/128 - lr: 1.0000e-02] Epoch Error: 0.001683, Current Frequency: 0.997040, Current Damping: 0.250993                         \n[32/128 - lr: 1.0000e-02] Epoch Error: 0.001396, Current Frequency: 1.001091, Current Damping: 0.249821                         \n[33/128 - lr: 1.0000e-02] Epoch Error: 0.000768, Current Frequency: 1.000053, Current Damping: 0.250131                         \n[34/128 - lr: 1.0000e-02] Epoch Error: 0.001088, Current Frequency: 0.999780, Current Damping: 0.248895                         \n[35/128 - lr: 1.0000e-02] Epoch Error: 0.000714, Current Frequency: 1.000858, Current Damping: 0.249915                         \n[36/128 - lr: 1.0000e-02] Epoch Error: 0.001056, Current Frequency: 0.998587, Current Damping: 0.250181                         \n[37/128 - lr: 1.0000e-02] Epoch Error: 0.001231, Current Frequency: 1.001789, Current Damping: 0.249745                         \n[38/128 - lr: 1.0000e-03] Epoch Error: 0.000752, Current Frequency: 0.999486, Current Damping: 0.249796                         \n[39/128 - lr: 1.0000e-03] Epoch Error: 0.000264, Current Frequency: 0.999915, Current Damping: 0.250041                         \n[40/128 - lr: 1.0000e-03] Epoch Error: 0.000137, Current Frequency: 0.999792, Current Damping: 0.249997                         \n[41/128 - lr: 1.0000e-03] Epoch Error: 0.000192, Current Frequency: 1.000024, Current Damping: 0.249953                         \n[42/128 - lr: 1.0000e-03] Epoch Error: 0.000080, Current Frequency: 1.000043, Current Damping: 0.250012                         \n[43/128 - lr: 1.0000e-03] Epoch Error: 0.000099, Current Frequency: 0.999943, Current Damping: 0.250036                         \n[44/128 - lr: 1.0000e-03] Epoch Error: 0.000119, Current Frequency: 1.000001, Current Damping: 0.250088                         \n[45/128 - lr: 1.0000e-03] Epoch Error: 0.000089, Current Frequency: 0.999957, Current Damping: 0.249983                         \n[46/128 - lr: 1.0000e-03] Epoch Error: 0.000088, Current Frequency: 1.000017, Current Damping: 0.249934                         \n[47/128 - lr: 1.0000e-03] Epoch Error: 0.000085, Current Frequency: 0.999884, Current Damping: 0.249989                         \n[48/128 - lr: 1.0000e-03] Epoch Error: 0.000158, Current Frequency: 0.999842, Current Damping: 0.250018                         \n[49/128 - lr: 1.0000e-03] Epoch Error: 0.000154, Current Frequency: 1.000107, Current Damping: 0.249976                         \n[50/128 - lr: 1.0000e-03] Epoch Error: 0.000074, Current Frequency: 0.999905, Current Damping: 0.250041                         \n[51/128 - lr: 1.0000e-03] Epoch Error: 0.000072, Current Frequency: 0.999941, Current Damping: 0.250007                         \n[52/128 - lr: 1.0000e-03] Epoch Error: 0.000119, Current Frequency: 0.999948, Current Damping: 0.249982                         \n[53/128 - lr: 1.0000e-03] Epoch Error: 0.000127, Current Frequency: 0.999979, Current Damping: 0.249996                         \n[54/128 - lr: 1.0000e-03] Epoch Error: 0.000129, Current Frequency: 0.999846, Current Damping: 0.250023                         \n[55/128 - lr: 1.0000e-03] Epoch Error: 0.000169, Current Frequency: 0.999789, Current Damping: 0.250029                         \n[56/128 - lr: 1.0000e-03] Epoch Error: 0.000269, Current Frequency: 1.000029, Current Damping: 0.250036                         \n[57/128 - lr: 1.0000e-03] Epoch Error: 0.000136, Current Frequency: 0.999884, Current Damping: 0.250002                         \n[58/128 - lr: 1.0000e-03] Epoch Error: 0.000187, Current Frequency: 1.000121, Current Damping: 0.250004                         \n[59/128 - lr: 1.0000e-03] Epoch Error: 0.000164, Current Frequency: 0.999913, Current Damping: 0.249983                         \n[60/128 - lr: 1.0000e-03] Epoch Error: 0.000108, Current Frequency: 0.999945, Current Damping: 0.250092                         \n[61/128 - lr: 1.0000e-03] Epoch Error: 0.000076, Current Frequency: 1.000125, Current Damping: 0.249983                         \n[62/128 - lr: 1.0000e-04] Epoch Error: 0.000149, Current Frequency: 0.999997, Current Damping: 0.250033                         \n[63/128 - lr: 1.0000e-04] Epoch Error: 0.000012, Current Frequency: 1.000014, Current Damping: 0.250001                         \n[64/128 - lr: 1.0000e-04] Epoch Error: 0.000013, Current Frequency: 1.000021, Current Damping: 0.249993                         \n[65/128 - lr: 1.0000e-04] Epoch Error: 0.000013, Current Frequency: 1.000002, Current Damping: 0.249991                         \n[66/128 - lr: 1.0000e-04] Epoch Error: 0.000013, Current Frequency: 1.000010, Current Damping: 0.250002                         \n[67/128 - lr: 1.0000e-04] Epoch Error: 0.000014, Current Frequency: 1.000012, Current Damping: 0.249998                         \n[68/128 - lr: 1.0000e-04] Epoch Error: 0.000007, Current Frequency: 1.000012, Current Damping: 0.250006                         \n[69/128 - lr: 1.0000e-04] Epoch Error: 0.000010, Current Frequency: 1.000016, Current Damping: 0.250000                         \n[70/128 - lr: 1.0000e-04] Epoch Error: 0.000018, Current Frequency: 0.999979, Current Damping: 0.249999                         \n[71/128 - lr: 1.0000e-04] Epoch Error: 0.000014, Current Frequency: 0.999994, Current Damping: 0.249999                         \n[72/128 - lr: 1.0000e-04] Epoch Error: 0.000016, Current Frequency: 1.000012, Current Damping: 0.250004                         \n[73/128 - lr: 1.0000e-04] Epoch Error: 0.000021, Current Frequency: 1.000002, Current Damping: 0.250002                         \n[74/128 - lr: 1.0000e-04] Epoch Error: 0.000016, Current Frequency: 0.999998, Current Damping: 0.249997                         \n[75/128 - lr: 1.0000e-04] Epoch Error: 0.000010, Current Frequency: 0.999992, Current Damping: 0.250010                         \n[76/128 - lr: 1.0000e-04] Epoch Error: 0.000013, Current Frequency: 1.000000, Current Damping: 0.250005                         \n[77/128 - lr: 1.0000e-04] Epoch Error: 0.000007, Current Frequency: 0.999988, Current Damping: 0.250002                         \n[78/128 - lr: 1.0000e-04] Epoch Error: 0.000013, Current Frequency: 0.999991, Current Damping: 0.250005                         \n[79/128 - lr: 1.0000e-04] Epoch Error: 0.000016, Current Frequency: 0.999998, Current Damping: 0.249999                         \n[80/128 - lr: 1.0000e-04] Epoch Error: 0.000012, Current Frequency: 0.999999, Current Damping: 0.249996                         \n[81/128 - lr: 1.0000e-04] Epoch Error: 0.000011, Current Frequency: 0.999985, Current Damping: 0.250004                         \n[82/128 - lr: 1.0000e-04] Epoch Error: 0.000022, Current Frequency: 0.999975, Current Damping: 0.250006                         \n[83/128 - lr: 1.0000e-04] Epoch Error: 0.000026, Current Frequency: 0.999980, Current Damping: 0.250006                         \n[84/128 - lr: 1.0000e-04] Epoch Error: 0.000018, Current Frequency: 1.000034, Current Damping: 0.249991                         \n[85/128 - lr: 1.0000e-04] Epoch Error: 0.000018, Current Frequency: 0.999967, Current Damping: 0.250005                         \n[86/128 - lr: 1.0000e-04] Epoch Error: 0.000020, Current Frequency: 0.999997, Current Damping: 0.250000                         \n[87/128 - lr: 1.0000e-04] Epoch Error: 0.000006, Current Frequency: 1.000013, Current Damping: 0.249999                         \n[88/128 - lr: 1.0000e-04] Epoch Error: 0.000021, Current Frequency: 1.000005, Current Damping: 0.249997                         \n[89/128 - lr: 1.0000e-04] Epoch Error: 0.000014, Current Frequency: 1.000006, Current Damping: 0.249997                         \n[90/128 - lr: 1.0000e-04] Epoch Error: 0.000006, Current Frequency: 0.999994, Current Damping: 0.249995                         \n[91/128 - lr: 1.0000e-04] Epoch Error: 0.000010, Current Frequency: 0.999995, Current Damping: 0.250004                         \n[92/128 - lr: 1.0000e-04] Epoch Error: 0.000006, Current Frequency: 0.999996, Current Damping: 0.249999                         \n[93/128 - lr: 1.0000e-04] Epoch Error: 0.000009, Current Frequency: 1.000001, Current Damping: 0.250003                         \n[94/128 - lr: 1.0000e-04] Epoch Error: 0.000012, Current Frequency: 1.000007, Current Damping: 0.249999                         \n[95/128 - lr: 1.0000e-04] Epoch Error: 0.000011, Current Frequency: 1.000000, Current Damping: 0.249996                         \n[96/128 - lr: 1.0000e-04] Epoch Error: 0.000015, Current Frequency: 0.999975, Current Damping: 0.250005                         \n[97/128 - lr: 1.0000e-04] Epoch Error: 0.000017, Current Frequency: 0.999996, Current Damping: 0.250002                         \n[98/128 - lr: 1.0000e-04] Epoch Error: 0.000007, Current Frequency: 1.000005, Current Damping: 0.249999                         \n[99/128 - lr: 1.0000e-04] Epoch Error: 0.000007, Current Frequency: 1.000005, Current Damping: 0.249999                         \n[100/128 - lr: 1.0000e-04] Epoch Error: 0.000008, Current Frequency: 0.999997, Current Damping: 0.250001                        \n[101/128 - lr: 1.0000e-05] Epoch Error: 0.000010, Current Frequency: 1.000004, Current Damping: 0.250001                        \n[102/128 - lr: 1.0000e-05] Epoch Error: 0.000002, Current Frequency: 1.000000, Current Damping: 0.250001                        \n[103/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 0.999999, Current Damping: 0.250000                        \n[104/128 - lr: 1.0000e-05] Epoch Error: 0.000002, Current Frequency: 0.999998, Current Damping: 0.250001                        \n[105/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000001, Current Damping: 0.250000                        \n[106/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[107/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000001, Current Damping: 0.250000                        \n[108/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[109/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000001, Current Damping: 0.250000                        \n[110/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 0.999999, Current Damping: 0.250000                        \n[111/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000001, Current Damping: 0.250000                        \n[112/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000001, Current Damping: 0.250000                        \n[113/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[114/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[115/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000001, Current Damping: 0.250000                        \n[116/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[117/128 - lr: 1.0000e-05] Epoch Error: 0.000001, Current Frequency: 1.000001, Current Damping: 0.250000                        \n[118/128 - lr: 1.0000e-06] Epoch Error: 0.000001, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[119/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[120/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[121/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[122/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[123/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[124/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[125/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[126/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[127/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \n[128/128 - lr: 1.0000e-06] Epoch Error: 0.000000, Current Frequency: 1.000000, Current Damping: 0.250000                        \nBest frequency: 1.000000, relative error: 0.000001%, mean absolute error: 0.000000\nBest damping:   0.250000, relative error: 0.000023%, mean absolute error: 0.000000\n</pre> In\u00a0[14]: Copied! <pre>fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\")\nfig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")\n\n_, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, integrator_kwargs, best_frequency, best_damping)\n_ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\")\n_ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\")\n</pre> fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\") fig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")  _, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, integrator_kwargs, best_frequency, best_damping) _ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\") _ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\") <p>This replicates the prior results and actually improves on them by a small margin.</p> <p>Now let's add a neural network into the mix! This will be the simplest network possible aka a matrix multiplied with the input vector. While this may seem simple, you'll see that our simple harmonic oscillator can also be expressed as a matrix multiplied by the input. It has a very specific structure that arises from the fact that it was a second order equation. I've written this matrix below:</p> <p>$$ \\begin{bmatrix}     x^{(1)} \\\\     v^{(1)} \\end{bmatrix} =  \\mathbf{A} \\begin{bmatrix}     x \\\\     v \\end{bmatrix} $$</p> <p>where</p> <p>$$ \\mathbf{A} = \\begin{bmatrix}     0 &amp; 1 \\\\     -\\omega^2 &amp; -2\\zeta\\omega \\end{bmatrix} $$</p> <p>Given that matrix multiplication underlies most neural networks, we can try to learn this $\\mathbf{A}$-matrix and at the same time introduce some of the Neural Network machinery in PyTorch. We will revisit these later when learning more interesting/complex systems.</p> In\u00a0[15]: Copied! <pre># we define our network as a subclass of torch.nn.Module\n# This allows PyTorch to appropriately track parameters\nclass OscillatorNet(torch.nn.Module):\n    def __init__(self):\n        # First we initialise the superclass, `torch.nn.Module`\n        super().__init__()\n        # Then we define the actual neural network\n        # Most Neural Networks operate sequentially so they can be wrapped\n        # inside a torch.nn.Sequential which takes each layer\n        # as an argument.\n        # Since we're only learning one matrix, we have\n        # one layer, the `torch.nn.Linear`.\n        # `torch.nn.Linear` stores a matrix and a bias which actually makes it\n        # an Affine transformation rather than a purely linear transformation\n        self.internal_net = torch.nn.Sequential(\n            torch.nn.Linear(2, 2),\n        )\n    \n    def forward(self, x, t):\n        # Our network only depends on x, but since it could also depend on t, we have\n        # included it for completeness\n        # Additionally, PyTorch layers and modules expect a batched tensor\n        # ie. a tensor where the first dimension is over different samples\n        # Since we don't depend on batches, we check if the input is 1-dimensional\n        # And add a batch dimension as needed for the internal module\n        if x.dim() == 1:\n            return self.internal_net(x[None])[0]\n        else:\n            return self.internal_net(x)\n\n# And then instantiate the weights of the network itself\ndef init_weights(m):\n    # For each layer type, we can define how we initialise its values\n    if isinstance(m, torch.nn.Linear):\n        # A linear equation with a positive coefficient\n        # translates to exponential growth and a negative coefficient\n        # to exponential decay. In order to preserve stability we sample a matrix\n        # that is biased to be negative in its entries thus ensuring\n        # that our initial system is of exponential decay.\n        m.weight.data.normal_(0.0, 0.1)\n        if m.bias is not None:\n            m.bias.data.normal_(0.0, 0.1)\n</pre> # we define our network as a subclass of torch.nn.Module # This allows PyTorch to appropriately track parameters class OscillatorNet(torch.nn.Module):     def __init__(self):         # First we initialise the superclass, `torch.nn.Module`         super().__init__()         # Then we define the actual neural network         # Most Neural Networks operate sequentially so they can be wrapped         # inside a torch.nn.Sequential which takes each layer         # as an argument.         # Since we're only learning one matrix, we have         # one layer, the `torch.nn.Linear`.         # `torch.nn.Linear` stores a matrix and a bias which actually makes it         # an Affine transformation rather than a purely linear transformation         self.internal_net = torch.nn.Sequential(             torch.nn.Linear(2, 2),         )          def forward(self, x, t):         # Our network only depends on x, but since it could also depend on t, we have         # included it for completeness         # Additionally, PyTorch layers and modules expect a batched tensor         # ie. a tensor where the first dimension is over different samples         # Since we don't depend on batches, we check if the input is 1-dimensional         # And add a batch dimension as needed for the internal module         if x.dim() == 1:             return self.internal_net(x[None])[0]         else:             return self.internal_net(x)  # And then instantiate the weights of the network itself def init_weights(m):     # For each layer type, we can define how we initialise its values     if isinstance(m, torch.nn.Linear):         # A linear equation with a positive coefficient         # translates to exponential growth and a negative coefficient         # to exponential decay. In order to preserve stability we sample a matrix         # that is biased to be negative in its entries thus ensuring         # that our initial system is of exponential decay.         m.weight.data.normal_(0.0, 0.1)         if m.bias is not None:             m.bias.data.normal_(0.0, 0.1) In\u00a0[16]: Copied! <pre># Here we instantiate our network.\ntorch.manual_seed(36)\nsimple_oscillator_net = OscillatorNet()\nsimple_oscillator_net.apply(init_weights)\n\n# `torch.autograd.Function`s track computation on all input tensors.\n# For that reason, we must pass our neural network parameters to the integrator,\n# which will pass it to the derivative function.\n# Since our network is stateful, we don't use these parameters, but define them in \n# the function signature.\ndef sha_nn_fn(x, t, *nn_parameters):\n    return torch.func.functional_call(simple_oscillator_net, {k: p for (k, _), p in zip(simple_oscillator_net.named_parameters(), nn_parameters)}, (x, t))\n\noptimiser = torch.optim.Adam(simple_oscillator_net.parameters(), lr=1e-3, amsgrad=True)\n# OneCycleLR cycles the learning rate from max_lr/25 to max_lr and then back down.\n# This helps the network escape local minima and find better solutions.\n# It is an alternative to ReduceLROnPlateau which can sometimes get stuck in local minima\n# for problems with a small number of degrees of freedom.\none_cycle_lr = torch.optim.lr_scheduler.OneCycleLR(optimiser, max_lr=1e-1, steps_per_epoch=round(time_dataset.shape[0]/batch_size+0.5), epochs=number_of_gd_steps, three_phase=True)\n\nideal_matrix = neuralode.dynamics.get_simple_harmonic_oscillator_matrix(frequency, damping)\nideal_bias = torch.zeros_like(initial_state)\n\nbest_error = torch.inf\n# For pytorch modules, the `state_dict` method allows us to get a copy\n# of all the parameters that define the model, thus enabling us to \n# store the state as well as restore it.\nbest_parameters = simple_oscillator_net.state_dict()\nfor step in range(number_of_gd_steps):\n    epoch_error = 0.0\n    shuffled_indices = torch.randperm(time_dataset.shape[0])\n    for batch_idx in range(0, time_dataset.shape[0], batch_size):\n        batch_dict = {\n            'times': time_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n            'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n        }\n    \n        step_error = optimiser.step(lambda: sha_closure(sha_nn_fn, list(simple_oscillator_net.parameters()), batch_dict))\n        epoch_error = epoch_error + step_error.item()*batch_dict['times'].shape[0]\n        one_cycle_lr.step()\n        # print(f\"[{step+1}/{number_of_gd_steps}]/[{batch_idx}/{time_dataset.shape[0]}] Batch Error: {step_error:.6f} \", end='\\r')\n    epoch_error = epoch_error/time_dataset.shape[0]\n    if epoch_error &lt; best_error:\n        best_error = epoch_error\n        best_parameters = simple_oscillator_net.state_dict()\n    learned_matrix = simple_oscillator_net.state_dict()['internal_net.0.weight']\n    learned_bias = simple_oscillator_net.state_dict()['internal_net.0.bias']\n    # Ideally our matrix is equivalent to our simple harmonic oscillator matrix and our bias goes to zero\n    print(f\"[{step+1}/{number_of_gd_steps} - lr: {one_cycle_lr.get_last_lr()[0]:.4e}] Epoch Error: {epoch_error:.6f}, \\nW={learned_matrix.cpu()}, \\nb={learned_bias.cpu()}\")\n    print()\n    \nsimple_oscillator_net.load_state_dict(best_parameters)\nlearned_matrix = simple_oscillator_net.state_dict()['internal_net.0.weight']\nlearned_bias = simple_oscillator_net.state_dict()['internal_net.0.bias']\n\n# Before we were looking at relative error, but in the case of a matrix with zeros,\n# the relative error is undefined, so we look at another common metric: mean absolute error\nprint(f\"Best matrix: {learned_matrix}, mean absolute error: {torch.mean(torch.abs(ideal_matrix - learned_matrix)).item():.6f}\")\nprint(f\"Best bias:   {learned_bias}, mean absolute error: {torch.mean(torch.abs(ideal_bias - learned_bias)).item():.6f}\")\n</pre> # Here we instantiate our network. torch.manual_seed(36) simple_oscillator_net = OscillatorNet() simple_oscillator_net.apply(init_weights)  # `torch.autograd.Function`s track computation on all input tensors. # For that reason, we must pass our neural network parameters to the integrator, # which will pass it to the derivative function. # Since our network is stateful, we don't use these parameters, but define them in  # the function signature. def sha_nn_fn(x, t, *nn_parameters):     return torch.func.functional_call(simple_oscillator_net, {k: p for (k, _), p in zip(simple_oscillator_net.named_parameters(), nn_parameters)}, (x, t))  optimiser = torch.optim.Adam(simple_oscillator_net.parameters(), lr=1e-3, amsgrad=True) # OneCycleLR cycles the learning rate from max_lr/25 to max_lr and then back down. # This helps the network escape local minima and find better solutions. # It is an alternative to ReduceLROnPlateau which can sometimes get stuck in local minima # for problems with a small number of degrees of freedom. one_cycle_lr = torch.optim.lr_scheduler.OneCycleLR(optimiser, max_lr=1e-1, steps_per_epoch=round(time_dataset.shape[0]/batch_size+0.5), epochs=number_of_gd_steps, three_phase=True)  ideal_matrix = neuralode.dynamics.get_simple_harmonic_oscillator_matrix(frequency, damping) ideal_bias = torch.zeros_like(initial_state)  best_error = torch.inf # For pytorch modules, the `state_dict` method allows us to get a copy # of all the parameters that define the model, thus enabling us to  # store the state as well as restore it. best_parameters = simple_oscillator_net.state_dict() for step in range(number_of_gd_steps):     epoch_error = 0.0     shuffled_indices = torch.randperm(time_dataset.shape[0])     for batch_idx in range(0, time_dataset.shape[0], batch_size):         batch_dict = {             'times': time_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],             'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],         }              step_error = optimiser.step(lambda: sha_closure(sha_nn_fn, list(simple_oscillator_net.parameters()), batch_dict))         epoch_error = epoch_error + step_error.item()*batch_dict['times'].shape[0]         one_cycle_lr.step()         # print(f\"[{step+1}/{number_of_gd_steps}]/[{batch_idx}/{time_dataset.shape[0]}] Batch Error: {step_error:.6f} \", end='\\r')     epoch_error = epoch_error/time_dataset.shape[0]     if epoch_error &lt; best_error:         best_error = epoch_error         best_parameters = simple_oscillator_net.state_dict()     learned_matrix = simple_oscillator_net.state_dict()['internal_net.0.weight']     learned_bias = simple_oscillator_net.state_dict()['internal_net.0.bias']     # Ideally our matrix is equivalent to our simple harmonic oscillator matrix and our bias goes to zero     print(f\"[{step+1}/{number_of_gd_steps} - lr: {one_cycle_lr.get_last_lr()[0]:.4e}] Epoch Error: {epoch_error:.6f}, \\nW={learned_matrix.cpu()}, \\nb={learned_bias.cpu()}\")     print()      simple_oscillator_net.load_state_dict(best_parameters) learned_matrix = simple_oscillator_net.state_dict()['internal_net.0.weight'] learned_bias = simple_oscillator_net.state_dict()['internal_net.0.bias']  # Before we were looking at relative error, but in the case of a matrix with zeros, # the relative error is undefined, so we look at another common metric: mean absolute error print(f\"Best matrix: {learned_matrix}, mean absolute error: {torch.mean(torch.abs(ideal_matrix - learned_matrix)).item():.6f}\") print(f\"Best bias:   {learned_bias}, mean absolute error: {torch.mean(torch.abs(ideal_bias - learned_bias)).item():.6f}\") <pre>[1/128 - lr: 4.1609e-03] Epoch Error: 1.609792, \nW=tensor([[ 0.0641,  0.1550],\n        [-0.0738, -0.1089]]), \nb=tensor([-0.0626, -0.0914])\n\n[2/128 - lr: 4.6425e-03] Epoch Error: 0.720409, \nW=tensor([[ 0.0104,  0.2160],\n        [-0.0536, -0.1961]]), \nb=tensor([-0.1232, -0.0374])\n\n[3/128 - lr: 5.4416e-03] Epoch Error: 0.504084, \nW=tensor([[-0.0127,  0.2244],\n        [-0.0064, -0.2662]]), \nb=tensor([-0.1308,  0.0365])\n\n[4/128 - lr: 6.5528e-03] Epoch Error: 0.483388, \nW=tensor([[-0.0308,  0.2290],\n        [-0.0187, -0.3025]]), \nb=tensor([-0.1357,  0.0223])\n\n[5/128 - lr: 7.9688e-03] Epoch Error: 0.471892, \nW=tensor([[-0.0409,  0.2373],\n        [-0.0125, -0.3169]]), \nb=tensor([-0.1141,  0.0231])\n\n[6/128 - lr: 9.6799e-03] Epoch Error: 0.457693, \nW=tensor([[-0.0528,  0.2441],\n        [-0.0243, -0.3263]]), \nb=tensor([-0.0928, -0.0056])\n\n[7/128 - lr: 1.1675e-02] Epoch Error: 0.451436, \nW=tensor([[-0.0744,  0.2438],\n        [-0.0301, -0.3349]]), \nb=tensor([-0.0837, -0.0036])\n\n[8/128 - lr: 1.3940e-02] Epoch Error: 0.443324, \nW=tensor([[-0.1043,  0.2411],\n        [-0.0366, -0.3436]]), \nb=tensor([-0.0817,  0.0052])\n\n[9/128 - lr: 1.6460e-02] Epoch Error: 0.428885, \nW=tensor([[-0.1330,  0.2390],\n        [-0.0429, -0.3516]]), \nb=tensor([-0.0619, -0.0138])\n\n[10/128 - lr: 1.9219e-02] Epoch Error: 0.423491, \nW=tensor([[-0.1724,  0.2298],\n        [-0.0604, -0.3743]]), \nb=tensor([-0.0684, -0.0139])\n\n[11/128 - lr: 2.2197e-02] Epoch Error: 0.415295, \nW=tensor([[-0.2186,  0.2158],\n        [-0.0939, -0.4081]]), \nb=tensor([-0.0541, -0.0083])\n\n[12/128 - lr: 2.5375e-02] Epoch Error: 0.393295, \nW=tensor([[-0.2631,  0.2108],\n        [-0.1239, -0.4492]]), \nb=tensor([-0.0418, -0.0093])\n\n[13/128 - lr: 2.8732e-02] Epoch Error: 0.378483, \nW=tensor([[-0.3168,  0.2258],\n        [-0.1701, -0.4994]]), \nb=tensor([-0.0218,  0.0029])\n\n[14/128 - lr: 3.2244e-02] Epoch Error: 0.362960, \nW=tensor([[-0.3653,  0.2573],\n        [-0.2244, -0.5573]]), \nb=tensor([-0.0408, -0.0072])\n\n[15/128 - lr: 3.5890e-02] Epoch Error: 0.343125, \nW=tensor([[-0.4226,  0.3115],\n        [-0.3458, -0.6166]]), \nb=tensor([-0.0015, -0.0041])\n\n[16/128 - lr: 3.9643e-02] Epoch Error: 0.315085, \nW=tensor([[-0.4736,  0.4208],\n        [-0.5121, -0.6513]]), \nb=tensor([-0.0264, -0.0381])\n\n[17/128 - lr: 4.3479e-02] Epoch Error: 0.269020, \nW=tensor([[-0.5067,  0.5902],\n        [-0.7364, -0.6283]]), \nb=tensor([-0.0192, -0.0658])\n\n[18/128 - lr: 4.7372e-02] Epoch Error: 0.197346, \nW=tensor([[-0.4880,  0.7967],\n        [-1.0256, -0.3488]]), \nb=tensor([-0.0205, -0.0677])\n\n[19/128 - lr: 5.1296e-02] Epoch Error: 0.107882, \nW=tensor([[-0.4213,  0.8397],\n        [-1.1120, -0.1394]]), \nb=tensor([0.0012, 0.0378])\n\n[20/128 - lr: 5.5224e-02] Epoch Error: 0.089753, \nW=tensor([[-0.3504,  0.8440],\n        [-1.0732, -0.1570]]), \nb=tensor([-0.0072,  0.0150])\n\n[21/128 - lr: 5.9132e-02] Epoch Error: 0.074635, \nW=tensor([[-0.2692,  0.8929],\n        [-1.0202, -0.2536]]), \nb=tensor([0.0095, 0.0032])\n\n[22/128 - lr: 6.2991e-02] Epoch Error: 0.055025, \nW=tensor([[-0.1796,  0.9106],\n        [-0.9909, -0.3446]]), \nb=tensor([ 0.0009, -0.0002])\n\n[23/128 - lr: 6.6777e-02] Epoch Error: 0.042198, \nW=tensor([[-0.0918,  0.9161],\n        [-0.9663, -0.4102]]), \nb=tensor([0.0152, 0.0084])\n\n[24/128 - lr: 7.0463e-02] Epoch Error: 0.030391, \nW=tensor([[-0.0337,  0.9564],\n        [-1.0250, -0.4770]]), \nb=tensor([ 0.0103, -0.0006])\n\n[25/128 - lr: 7.4026e-02] Epoch Error: 0.022276, \nW=tensor([[-0.0159,  1.0148],\n        [-0.9859, -0.4600]]), \nb=tensor([0.0005, 0.0003])\n\n[26/128 - lr: 7.7441e-02] Epoch Error: 0.018522, \nW=tensor([[ 0.0070,  1.0024],\n        [-1.0076, -0.4974]]), \nb=tensor([0.0049, 0.0168])\n\n[27/128 - lr: 8.0686e-02] Epoch Error: 0.020377, \nW=tensor([[ 0.0027,  0.9898],\n        [-0.9873, -0.4936]]), \nb=tensor([-0.0004, -0.0022])\n\n[28/128 - lr: 8.3738e-02] Epoch Error: 0.022518, \nW=tensor([[ 0.0015,  1.0029],\n        [-0.9754, -0.5097]]), \nb=tensor([-0.0057,  0.0114])\n\n[29/128 - lr: 8.6578e-02] Epoch Error: 0.020440, \nW=tensor([[ 0.0074,  1.0234],\n        [-1.0268, -0.5093]]), \nb=tensor([-0.0029,  0.0104])\n\n[30/128 - lr: 8.9186e-02] Epoch Error: 0.027248, \nW=tensor([[-1.7884e-05,  1.0147e+00],\n        [-9.9924e-01, -5.1175e-01]]), \nb=tensor([ 0.0046, -0.0083])\n\n[31/128 - lr: 9.1544e-02] Epoch Error: 0.029168, \nW=tensor([[-3.4838e-04,  1.0066e+00],\n        [-9.7280e-01, -5.1309e-01]]), \nb=tensor([-0.0065,  0.0063])\n\n[32/128 - lr: 9.3637e-02] Epoch Error: 0.030273, \nW=tensor([[ 0.0034,  0.9911],\n        [-1.0255, -0.5189]]), \nb=tensor([0.0078, 0.0182])\n\n[33/128 - lr: 9.5451e-02] Epoch Error: 0.022938, \nW=tensor([[-0.0146,  1.0047],\n        [-1.0120, -0.4691]]), \nb=tensor([-0.0095,  0.0011])\n\n[34/128 - lr: 9.6974e-02] Epoch Error: 0.023357, \nW=tensor([[-0.0071,  1.0359],\n        [-0.9823, -0.5283]]), \nb=tensor([-0.0299, -0.0092])\n\n[35/128 - lr: 9.8196e-02] Epoch Error: 0.028715, \nW=tensor([[ 0.0179,  0.9993],\n        [-1.0036, -0.5020]]), \nb=tensor([0.0063, 0.0142])\n\n[36/128 - lr: 9.9107e-02] Epoch Error: 0.032321, \nW=tensor([[ 0.0182,  0.9964],\n        [-0.9976, -0.5147]]), \nb=tensor([-0.0158, -0.0083])\n\n[37/128 - lr: 9.9703e-02] Epoch Error: 0.020442, \nW=tensor([[-0.0135,  0.9988],\n        [-1.0087, -0.5116]]), \nb=tensor([0.0238, 0.0263])\n\n[38/128 - lr: 9.9979e-02] Epoch Error: 0.037006, \nW=tensor([[-0.0259,  0.9824],\n        [-1.0131, -0.4769]]), \nb=tensor([-0.0046,  0.0186])\n\n[39/128 - lr: 9.9934e-02] Epoch Error: 0.029400, \nW=tensor([[-0.0120,  0.9996],\n        [-1.0392, -0.4779]]), \nb=tensor([ 0.0089, -0.0208])\n\n[40/128 - lr: 9.9567e-02] Epoch Error: 0.033170, \nW=tensor([[-0.0055,  1.0142],\n        [-1.0174, -0.4937]]), \nb=tensor([-0.0042, -0.0404])\n\n[41/128 - lr: 9.8881e-02] Epoch Error: 0.034741, \nW=tensor([[ 0.0095,  0.9889],\n        [-0.9669, -0.5061]]), \nb=tensor([-0.0012, -0.0139])\n\n[42/128 - lr: 9.7881e-02] Epoch Error: 0.025106, \nW=tensor([[-0.0160,  0.9722],\n        [-1.0266, -0.4955]]), \nb=tensor([0.0083, 0.0179])\n\n[43/128 - lr: 9.6573e-02] Epoch Error: 0.022553, \nW=tensor([[-5.3376e-04,  9.7187e-01],\n        [-9.9602e-01, -4.8455e-01]]), \nb=tensor([0.0142, 0.0078])\n\n[44/128 - lr: 9.4967e-02] Epoch Error: 0.030136, \nW=tensor([[ 0.0033,  0.9065],\n        [-1.0110, -0.4894]]), \nb=tensor([-0.0296, -0.0152])\n\n[45/128 - lr: 9.3072e-02] Epoch Error: 0.037048, \nW=tensor([[ 0.0018,  1.0208],\n        [-0.9850, -0.4883]]), \nb=tensor([0.0109, 0.0114])\n\n[46/128 - lr: 9.0902e-02] Epoch Error: 0.015217, \nW=tensor([[-0.0096,  0.9875],\n        [-1.0184, -0.5115]]), \nb=tensor([0.0029, 0.0136])\n\n[47/128 - lr: 8.8471e-02] Epoch Error: 0.037965, \nW=tensor([[-0.0263,  1.0420],\n        [-0.9906, -0.5544]]), \nb=tensor([ 0.0238, -0.0071])\n\n[48/128 - lr: 8.5796e-02] Epoch Error: 0.027586, \nW=tensor([[ 0.0051,  1.0088],\n        [-1.0085, -0.4161]]), \nb=tensor([0.0016, 0.0284])\n\n[49/128 - lr: 8.2894e-02] Epoch Error: 0.036071, \nW=tensor([[-0.0171,  0.9916],\n        [-0.9907, -0.4765]]), \nb=tensor([ 0.0048, -0.0028])\n\n[50/128 - lr: 7.9785e-02] Epoch Error: 0.016789, \nW=tensor([[ 0.0072,  1.0112],\n        [-0.9874, -0.5083]]), \nb=tensor([-0.0076,  0.0059])\n\n[51/128 - lr: 7.6490e-02] Epoch Error: 0.018357, \nW=tensor([[ 8.1218e-04,  1.0239e+00],\n        [-1.0008e+00, -5.0510e-01]]), \nb=tensor([ 0.0013, -0.0125])\n\n[52/128 - lr: 7.3031e-02] Epoch Error: 0.023514, \nW=tensor([[-0.0103,  0.9784],\n        [-1.0374, -0.5118]]), \nb=tensor([ 0.0046, -0.0131])\n\n[53/128 - lr: 6.9430e-02] Epoch Error: 0.021051, \nW=tensor([[-0.0074,  0.9536],\n        [-0.9919, -0.5058]]), \nb=tensor([ 0.0024, -0.0115])\n\n[54/128 - lr: 6.5713e-02] Epoch Error: 0.017005, \nW=tensor([[-0.0068,  1.0226],\n        [-1.0077, -0.4919]]), \nb=tensor([0.0081, 0.0121])\n\n[55/128 - lr: 6.1904e-02] Epoch Error: 0.017439, \nW=tensor([[ 0.0139,  0.9870],\n        [-1.0104, -0.5034]]), \nb=tensor([-0.0074,  0.0184])\n\n[56/128 - lr: 5.8028e-02] Epoch Error: 0.011525, \nW=tensor([[ 0.0023,  1.0026],\n        [-1.0045, -0.4947]]), \nb=tensor([-0.0010,  0.0025])\n\n[57/128 - lr: 5.4112e-02] Epoch Error: 0.012064, \nW=tensor([[-0.0022,  1.0142],\n        [-0.9994, -0.5060]]), \nb=tensor([-0.0066,  0.0103])\n\n[58/128 - lr: 5.0182e-02] Epoch Error: 0.012457, \nW=tensor([[-0.0036,  1.0150],\n        [-1.0009, -0.4884]]), \nb=tensor([0.0092, 0.0004])\n\n[59/128 - lr: 4.6264e-02] Epoch Error: 0.010368, \nW=tensor([[-0.0141,  0.9839],\n        [-0.9979, -0.4968]]), \nb=tensor([-0.0075, -0.0010])\n\n[60/128 - lr: 4.2385e-02] Epoch Error: 0.007346, \nW=tensor([[-0.0030,  1.0039],\n        [-0.9966, -0.4866]]), \nb=tensor([ 0.0062, -0.0022])\n\n[61/128 - lr: 3.8570e-02] Epoch Error: 0.009046, \nW=tensor([[ 0.0055,  0.9880],\n        [-0.9992, -0.4988]]), \nb=tensor([-0.0058, -0.0106])\n\n[62/128 - lr: 3.4845e-02] Epoch Error: 0.008171, \nW=tensor([[ 0.0029,  1.0003],\n        [-1.0060, -0.5129]]), \nb=tensor([ 0.0022, -0.0007])\n\n[63/128 - lr: 3.1235e-02] Epoch Error: 0.008128, \nW=tensor([[-0.0015,  0.9900],\n        [-0.9928, -0.4823]]), \nb=tensor([-0.0033,  0.0060])\n\n[64/128 - lr: 2.7764e-02] Epoch Error: 0.010931, \nW=tensor([[ 0.0035,  1.0005],\n        [-0.9982, -0.4849]]), \nb=tensor([-0.0046, -0.0011])\n\n[65/128 - lr: 2.4456e-02] Epoch Error: 0.010787, \nW=tensor([[-0.0014,  1.0096],\n        [-1.0108, -0.5066]]), \nb=tensor([ 0.0035, -0.0010])\n\n[66/128 - lr: 2.1332e-02] Epoch Error: 0.010705, \nW=tensor([[-0.0018,  1.0000],\n        [-0.9986, -0.5067]]), \nb=tensor([0.0024, 0.0030])\n\n[67/128 - lr: 1.8414e-02] Epoch Error: 0.005952, \nW=tensor([[-0.0019,  0.9892],\n        [-1.0084, -0.5000]]), \nb=tensor([-0.0002, -0.0005])\n\n[68/128 - lr: 1.5721e-02] Epoch Error: 0.005751, \nW=tensor([[-0.0027,  0.9980],\n        [-0.9947, -0.4958]]), \nb=tensor([-0.0012,  0.0012])\n\n[69/128 - lr: 1.3271e-02] Epoch Error: 0.004823, \nW=tensor([[-0.0019,  1.0019],\n        [-0.9976, -0.4980]]), \nb=tensor([0.0009, 0.0048])\n\n[70/128 - lr: 1.1081e-02] Epoch Error: 0.003509, \nW=tensor([[ 6.6505e-04,  9.9884e-01],\n        [-1.0029e+00, -4.9987e-01]]), \nb=tensor([0.0005, 0.0010])\n\n[71/128 - lr: 9.1657e-03] Epoch Error: 0.002061, \nW=tensor([[ 4.3162e-05,  1.0008e+00],\n        [-1.0001e+00, -4.9976e-01]]), \nb=tensor([0.0003, 0.0010])\n\n[72/128 - lr: 7.5372e-03] Epoch Error: 0.002211, \nW=tensor([[-7.0880e-04,  9.9970e-01],\n        [-1.0024e+00, -5.0111e-01]]), \nb=tensor([-0.0001,  0.0015])\n\n[73/128 - lr: 6.2067e-03] Epoch Error: 0.002731, \nW=tensor([[ 3.7402e-04,  9.9696e-01],\n        [-9.9885e-01, -4.9766e-01]]), \nb=tensor([-0.0014, -0.0012])\n\n[74/128 - lr: 5.1832e-03] Epoch Error: 0.001516, \nW=tensor([[-4.3274e-04,  9.9969e-01],\n        [-9.9975e-01, -4.9795e-01]]), \nb=tensor([-0.0005, -0.0004])\n\n[75/128 - lr: 4.4736e-03] Epoch Error: 0.001567, \nW=tensor([[-1.5633e-04,  9.9906e-01],\n        [-9.9966e-01, -4.9970e-01]]), \nb=tensor([-0.0006, -0.0003])\n\n[76/128 - lr: 4.0827e-03] Epoch Error: 0.002169, \nW=tensor([[-3.6453e-04,  9.9956e-01],\n        [-1.0023e+00, -4.9796e-01]]), \nb=tensor([ 0.0005, -0.0007])\n\n[77/128 - lr: 3.9997e-03] Epoch Error: 0.001693, \nW=tensor([[-3.3162e-04,  9.9971e-01],\n        [-1.0013e+00, -4.9860e-01]]), \nb=tensor([-7.7370e-05, -6.7228e-04])\n\n[78/128 - lr: 3.9938e-03] Epoch Error: 0.001061, \nW=tensor([[-2.3620e-04,  9.9928e-01],\n        [-9.9912e-01, -4.9921e-01]]), \nb=tensor([0.0004, 0.0001])\n\n[79/128 - lr: 3.9804e-03] Epoch Error: 0.001350, \nW=tensor([[ 6.0312e-05,  1.0014e+00],\n        [-1.0004e+00, -5.0052e-01]]), \nb=tensor([-0.0007,  0.0010])\n\n[80/128 - lr: 3.9596e-03] Epoch Error: 0.001763, \nW=tensor([[ 1.9713e-04,  9.9968e-01],\n        [-9.9843e-01, -5.0022e-01]]), \nb=tensor([ 0.0005, -0.0008])\n\n[81/128 - lr: 3.9314e-03] Epoch Error: 0.001147, \nW=tensor([[-2.6483e-04,  1.0007e+00],\n        [-1.0007e+00, -4.9960e-01]]), \nb=tensor([0.0015, 0.0001])\n\n[82/128 - lr: 3.8960e-03] Epoch Error: 0.001231, \nW=tensor([[ 2.3433e-04,  1.0013e+00],\n        [-1.0011e+00, -5.0177e-01]]), \nb=tensor([ 0.0004, -0.0007])\n\n[83/128 - lr: 3.8534e-03] Epoch Error: 0.000781, \nW=tensor([[ 4.1967e-04,  9.9999e-01],\n        [-1.0002e+00, -5.0061e-01]]), \nb=tensor([-0.0002,  0.0004])\n\n[84/128 - lr: 3.8039e-03] Epoch Error: 0.000817, \nW=tensor([[-2.7855e-04,  1.0003e+00],\n        [-9.9841e-01, -4.9858e-01]]), \nb=tensor([-0.0010, -0.0007])\n\n[85/128 - lr: 3.7476e-03] Epoch Error: 0.000948, \nW=tensor([[ 2.9369e-04,  1.0004e+00],\n        [-1.0010e+00, -4.9932e-01]]), \nb=tensor([-0.0001,  0.0002])\n\n[86/128 - lr: 3.6847e-03] Epoch Error: 0.001242, \nW=tensor([[-7.7710e-04,  9.9899e-01],\n        [-9.9972e-01, -5.0095e-01]]), \nb=tensor([-0.0008, -0.0001])\n\n[87/128 - lr: 3.6155e-03] Epoch Error: 0.000979, \nW=tensor([[ 2.2082e-04,  9.9973e-01],\n        [-1.0007e+00, -5.0043e-01]]), \nb=tensor([0.0007, 0.0010])\n\n[88/128 - lr: 3.5403e-03] Epoch Error: 0.000985, \nW=tensor([[ 4.5567e-04,  9.9948e-01],\n        [-1.0007e+00, -5.0107e-01]]), \nb=tensor([-0.0004, -0.0003])\n\n[89/128 - lr: 3.4592e-03] Epoch Error: 0.000793, \nW=tensor([[-4.3191e-04,  9.9916e-01],\n        [-9.9958e-01, -4.9898e-01]]), \nb=tensor([-0.0001,  0.0006])\n\n[90/128 - lr: 3.3727e-03] Epoch Error: 0.000918, \nW=tensor([[ 2.3193e-04,  9.9881e-01],\n        [-9.9952e-01, -5.0025e-01]]), \nb=tensor([-0.0006, -0.0004])\n\n[91/128 - lr: 3.2810e-03] Epoch Error: 0.001123, \nW=tensor([[ 1.3298e-04,  1.0004e+00],\n        [-1.0008e+00, -5.0098e-01]]), \nb=tensor([-0.0008,  0.0009])\n\n[92/128 - lr: 3.1845e-03] Epoch Error: 0.001331, \nW=tensor([[ 4.5332e-04,  1.0006e+00],\n        [-9.9945e-01, -4.9805e-01]]), \nb=tensor([-0.0003, -0.0008])\n\n[93/128 - lr: 3.0835e-03] Epoch Error: 0.001310, \nW=tensor([[-2.6666e-04,  1.0002e+00],\n        [-1.0006e+00, -4.9925e-01]]), \nb=tensor([-1.0954e-03,  2.1377e-05])\n\n[94/128 - lr: 2.9785e-03] Epoch Error: 0.001219, \nW=tensor([[ 7.3422e-05,  9.9906e-01],\n        [-1.0004e+00, -5.0057e-01]]), \nb=tensor([-0.0003, -0.0002])\n\n[95/128 - lr: 2.8698e-03] Epoch Error: 0.000698, \nW=tensor([[ 2.7087e-04,  9.9979e-01],\n        [-9.9974e-01, -4.9882e-01]]), \nb=tensor([ 0.0002, -0.0001])\n\n[96/128 - lr: 2.7578e-03] Epoch Error: 0.001001, \nW=tensor([[-1.4277e-04,  1.0007e+00],\n        [-1.0007e+00, -5.0149e-01]]), \nb=tensor([-8.3528e-05,  1.0019e-04])\n\n[97/128 - lr: 2.6430e-03] Epoch Error: 0.001115, \nW=tensor([[ 3.0692e-04,  9.9981e-01],\n        [-9.9941e-01, -4.9819e-01]]), \nb=tensor([ 0.0003, -0.0009])\n\n[98/128 - lr: 2.5258e-03] Epoch Error: 0.001035, \nW=tensor([[-2.2373e-04,  1.0001e+00],\n        [-1.0002e+00, -5.0032e-01]]), \nb=tensor([0.0002, 0.0002])\n\n[99/128 - lr: 2.4065e-03] Epoch Error: 0.000735, \nW=tensor([[-6.8873e-04,  9.9979e-01],\n        [-1.0008e+00, -4.9984e-01]]), \nb=tensor([0.0004, 0.0006])\n\n[100/128 - lr: 2.2858e-03] Epoch Error: 0.000692, \nW=tensor([[-4.0295e-04,  1.0013e+00],\n        [-9.9915e-01, -5.0024e-01]]), \nb=tensor([4.8592e-05, 9.7183e-05])\n\n[101/128 - lr: 2.1640e-03] Epoch Error: 0.000592, \nW=tensor([[-4.4167e-05,  9.9992e-01],\n        [-1.0011e+00, -5.0021e-01]]), \nb=tensor([-1.5593e-04, -6.6904e-05])\n\n[102/128 - lr: 2.0416e-03] Epoch Error: 0.000730, \nW=tensor([[-6.3255e-05,  1.0001e+00],\n        [-9.9936e-01, -5.0025e-01]]), \nb=tensor([ 0.0002, -0.0004])\n\n[103/128 - lr: 1.9190e-03] Epoch Error: 0.000489, \nW=tensor([[-1.0249e-04,  1.0001e+00],\n        [-1.0002e+00, -5.0025e-01]]), \nb=tensor([ 2.0460e-04, -2.3722e-05])\n\n[104/128 - lr: 1.7967e-03] Epoch Error: 0.000276, \nW=tensor([[ 9.1129e-06,  1.0003e+00],\n        [-9.9994e-01, -4.9993e-01]]), \nb=tensor([-1.5875e-04,  4.5437e-05])\n\n[105/128 - lr: 1.6752e-03] Epoch Error: 0.000311, \nW=tensor([[-1.8016e-04,  9.9989e-01],\n        [-9.9976e-01, -5.0008e-01]]), \nb=tensor([-6.5294e-06,  6.9909e-05])\n\n[106/128 - lr: 1.5549e-03] Epoch Error: 0.000256, \nW=tensor([[ 1.8151e-04,  9.9986e-01],\n        [-9.9995e-01, -5.0058e-01]]), \nb=tensor([-0.0001, -0.0003])\n\n[107/128 - lr: 1.4363e-03] Epoch Error: 0.000459, \nW=tensor([[-2.8963e-05,  1.0004e+00],\n        [-9.9995e-01, -5.0030e-01]]), \nb=tensor([-6.7238e-05, -8.7982e-05])\n\n[108/128 - lr: 1.3198e-03] Epoch Error: 0.000367, \nW=tensor([[ 1.3832e-04,  9.9923e-01],\n        [-1.0001e+00, -4.9991e-01]]), \nb=tensor([ 5.8027e-05, -5.0988e-05])\n\n[109/128 - lr: 1.2059e-03] Epoch Error: 0.000400, \nW=tensor([[-1.0852e-04,  1.0000e+00],\n        [-9.9959e-01, -4.9979e-01]]), \nb=tensor([-1.4766e-04,  1.1857e-05])\n\n[110/128 - lr: 1.0949e-03] Epoch Error: 0.000225, \nW=tensor([[-1.1258e-04,  9.9941e-01],\n        [-1.0000e+00, -5.0016e-01]]), \nb=tensor([-9.7096e-05,  4.7644e-04])\n\n[111/128 - lr: 9.8735e-04] Epoch Error: 0.000410, \nW=tensor([[ 6.0554e-06,  9.9996e-01],\n        [-9.9976e-01, -4.9961e-01]]), \nb=tensor([-0.0002,  0.0002])\n\n[112/128 - lr: 8.8360e-04] Epoch Error: 0.000480, \nW=tensor([[-8.7386e-05,  1.0002e+00],\n        [-9.9986e-01, -4.9996e-01]]), \nb=tensor([0.0002, 0.0001])\n\n[113/128 - lr: 7.8405e-04] Epoch Error: 0.000378, \nW=tensor([[-1.1393e-04,  9.9990e-01],\n        [-1.0001e+00, -5.0009e-01]]), \nb=tensor([ 1.0459e-05, -3.1913e-04])\n\n[114/128 - lr: 6.8907e-04] Epoch Error: 0.000355, \nW=tensor([[-2.1793e-04,  1.0000e+00],\n        [-9.9990e-01, -5.0007e-01]]), \nb=tensor([ 0.0001, -0.0002])\n\n[115/128 - lr: 5.9901e-04] Epoch Error: 0.000216, \nW=tensor([[-1.0103e-04,  1.0000e+00],\n        [-1.0001e+00, -4.9990e-01]]), \nb=tensor([ 1.8183e-04, -2.1772e-05])\n\n[116/128 - lr: 5.1422e-04] Epoch Error: 0.000152, \nW=tensor([[-7.8162e-05,  1.0000e+00],\n        [-1.0001e+00, -5.0001e-01]]), \nb=tensor([ 9.9333e-05, -5.6128e-05])\n\n[117/128 - lr: 4.3501e-04] Epoch Error: 0.000081, \nW=tensor([[ 3.8341e-05,  9.9996e-01],\n        [-1.0002e+00, -4.9998e-01]]), \nb=tensor([-1.5643e-05,  7.2027e-05])\n\n[118/128 - lr: 3.6169e-04] Epoch Error: 0.000105, \nW=tensor([[-5.7335e-05,  1.0001e+00],\n        [-9.9995e-01, -4.9999e-01]]), \nb=tensor([-2.6819e-05, -7.5429e-06])\n\n[119/128 - lr: 2.9452e-04] Epoch Error: 0.000100, \nW=tensor([[-1.2137e-05,  9.9996e-01],\n        [-1.0000e+00, -4.9986e-01]]), \nb=tensor([2.3959e-05, 1.1439e-04])\n\n[120/128 - lr: 2.3376e-04] Epoch Error: 0.000074, \nW=tensor([[-1.0361e-05,  9.9995e-01],\n        [-9.9993e-01, -4.9996e-01]]), \nb=tensor([-4.1451e-05, -3.1818e-06])\n\n[121/128 - lr: 1.7964e-04] Epoch Error: 0.000091, \nW=tensor([[ 6.0607e-06,  1.0000e+00],\n        [-1.0000e+00, -4.9995e-01]]), \nb=tensor([1.6299e-05, 7.9060e-05])\n\n[122/128 - lr: 1.3235e-04] Epoch Error: 0.000048, \nW=tensor([[-1.5443e-06,  1.0001e+00],\n        [-9.9996e-01, -5.0007e-01]]), \nb=tensor([2.9580e-06, 1.5721e-05])\n\n[123/128 - lr: 9.2093e-05] Epoch Error: 0.000021, \nW=tensor([[ 3.2709e-06,  1.0000e+00],\n        [-1.0000e+00, -4.9999e-01]]), \nb=tensor([-1.2055e-05,  3.4754e-06])\n\n[124/128 - lr: 5.9001e-05] Epoch Error: 0.000014, \nW=tensor([[-1.9627e-06,  9.9999e-01],\n        [-1.0000e+00, -5.0000e-01]]), \nb=tensor([-8.8738e-06, -9.2938e-06])\n\n[125/128 - lr: 3.3203e-05] Epoch Error: 0.000012, \nW=tensor([[-1.9953e-06,  9.9999e-01],\n        [-1.0000e+00, -4.9999e-01]]), \nb=tensor([ 5.6471e-06, -1.0543e-06])\n\n[126/128 - lr: 1.4797e-05] Epoch Error: 0.000006, \nW=tensor([[-4.6119e-06,  1.0000e+00],\n        [-9.9999e-01, -5.0000e-01]]), \nb=tensor([-1.9640e-06, -5.5545e-06])\n\n[127/128 - lr: 3.8508e-06] Epoch Error: 0.000004, \nW=tensor([[-9.6412e-07,  1.0000e+00],\n        [-1.0000e+00, -5.0000e-01]]), \nb=tensor([-1.0393e-06, -1.7788e-07])\n\n[128/128 - lr: 4.0653e-07] Epoch Error: 0.000001, \nW=tensor([[-5.6339e-07,  1.0000e+00],\n        [-1.0000e+00, -5.0000e-01]]), \nb=tensor([-1.1311e-07, -3.7789e-07])\n\nBest matrix: tensor([[-5.6339e-07,  1.0000e+00],\n        [-1.0000e+00, -5.0000e-01]]), mean absolute error: 0.000001\nBest bias:   tensor([-1.1311e-07, -3.7789e-07]), mean absolute error: 0.000000\n</pre> In\u00a0[17]: Copied! <pre>fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\")\nfig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")\n\nsimple_oscillator_net.load_state_dict(best_parameters)\n_, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(sha_nn_fn, initial_state, initial_time, final_time, initial_timestep, integrator_kwargs)\n_ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\")\n_ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\")\n</pre> fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\") fig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")  simple_oscillator_net.load_state_dict(best_parameters) _, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(sha_nn_fn, initial_state, initial_time, final_time, initial_timestep, integrator_kwargs) _ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\") _ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\") <p>And we can see that the neural network is able to effectively learn the dynamics of this system, but it would not extend to other systems with different frequency and damping as they would have a different matrix.</p> <p>In the coming notebooks, we will extend our network to learn the general dynamics by passing frequency and damping as a parameter. Further, we will learn more complex system dynamics and how to manipulate these systems.</p>"},{"location":"03-the-adjoint-method/#the-adjoint-method","title":"The Adjoint Method\u00b6","text":"<p>In this notebook, we will go through the process of implementing the adjoint method for computing gradients arising from a numerical integration.</p>"},{"location":"04-driven-harmonic-oscillator/","title":"The Driven Harmonic Oscillator","text":"In\u00a0[1]: Copied! <pre># We restrict the number of PyTorch CPU threads to 1 as cpu training is\n# slower with multiple threads due to the small sizes of network we're dealing with\n%env OMP_NUM_THREADS = 1\n%env OPENBLAS_NUM_THREADS = 1\n%env MKL_NUM_THREADS = 1\n%env VECLIB_MAXIMUM_THREADS = 1\n%env NUMEXPR_NUM_THREADS = 1\n</pre> # We restrict the number of PyTorch CPU threads to 1 as cpu training is # slower with multiple threads due to the small sizes of network we're dealing with %env OMP_NUM_THREADS = 1 %env OPENBLAS_NUM_THREADS = 1 %env MKL_NUM_THREADS = 1 %env VECLIB_MAXIMUM_THREADS = 1 %env NUMEXPR_NUM_THREADS = 1 <pre>env: OMP_NUM_THREADS=1\nenv: OPENBLAS_NUM_THREADS=1\nenv: MKL_NUM_THREADS=1\nenv: VECLIB_MAXIMUM_THREADS=1\nenv: NUMEXPR_NUM_THREADS=1\n</pre> In\u00a0[2]: Copied! <pre>import warnings\nimport torch\nimport random\nimport numpy as np\nimport neuralode\nimport copy\n\nwarnings.simplefilter('ignore', RuntimeWarning)\n</pre> import warnings import torch import random import numpy as np import neuralode import copy  warnings.simplefilter('ignore', RuntimeWarning) In\u00a0[3]: Copied! <pre># Again restrict the number of pytorch threads\ntorch.set_num_threads(1)\n# For convenience, we define the default tensor device and dtype here\ntorch.set_default_device('cpu')\n# In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training\ntorch.set_default_dtype(torch.float64)\n# Set the random seeds so that each run of this notebook is reproducible\ntorch.manual_seed(1)\nrandom.seed(1)\nnp.random.seed(1)\n</pre> # Again restrict the number of pytorch threads torch.set_num_threads(1) # For convenience, we define the default tensor device and dtype here torch.set_default_device('cpu') # In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training torch.set_default_dtype(torch.float64) # Set the random seeds so that each run of this notebook is reproducible torch.manual_seed(1) random.seed(1) np.random.seed(1) In\u00a0[4]: Copied! <pre>initial_position = torch.tensor(1.0)\ninitial_velocity = torch.tensor(0.0)\n\nfrequency = (torch.ones_like(initial_position))\ndamping = (torch.ones_like(initial_position)*0.25)\ninitial_state = torch.stack([\n    initial_position,\n    initial_velocity,\n], dim=-1).requires_grad_(True)\n\ninitial_time = torch.tensor(0.0)\nfinal_time   = torch.tensor(25.0)\n\ninitial_timestep = (final_time - initial_time) / 100\n\ncurrent_integrator = neuralode.integrators.AdaptiveRKV87Integrator\n\natol = rtol = torch.tensor(torch.finfo(initial_state.dtype).eps**0.5)\n</pre> initial_position = torch.tensor(1.0) initial_velocity = torch.tensor(0.0)  frequency = (torch.ones_like(initial_position)) damping = (torch.ones_like(initial_position)*0.25) initial_state = torch.stack([     initial_position,     initial_velocity, ], dim=-1).requires_grad_(True)  initial_time = torch.tensor(0.0) final_time   = torch.tensor(25.0)  initial_timestep = (final_time - initial_time) / 100  current_integrator = neuralode.integrators.AdaptiveRKV87Integrator  atol = rtol = torch.tensor(torch.finfo(initial_state.dtype).eps**0.5) In\u00a0[5]: Copied! <pre>final_state, _, sha_states, sha_times, _ = current_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, frequency, damping)\n</pre> final_state, _, sha_states, sha_times, _ = current_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, frequency, damping) In\u00a0[6]: Copied! <pre>fig, axes = neuralode.plot.trajectory.plot_trajectory([(i, j) for i, j in zip(sha_states, sha_times)], method_label=\"RK7(8) - Simple Harmonic Oscillator\")\n</pre> fig, axes = neuralode.plot.trajectory.plot_trajectory([(i, j) for i, j in zip(sha_states, sha_times)], method_label=\"RK7(8) - Simple Harmonic Oscillator\") In\u00a0[7]: Copied! <pre>state_range_min = torch.tensor([-25.0, -5.0])\nstate_range_max = torch.tensor([ 25.0,  5.0])\n\nstate_dataset = torch.rand(8, 2) * (state_range_max - state_range_min)[None] + state_range_min[None]\n\ntarget_state = torch.tensor([1.0, 0.0])\n\ndef damping_closure(rhs, parameters, minibatch, optimiser, integrator_kwargs, integrator=None):\n    \n    if integrator is None:\n        integrator = neuralode.integrators.AdaptiveRK45Integrator\n    optimiser.zero_grad()\n    \n    t_final = minibatch['final_time']\n    t_initial = minibatch['initial_time']\n    initial_dt = minibatch['dt']\n    states = minibatch['states']\n    \n    def damping_loss(state, time):\n        # Measure of the unnormalised cosine alignement between the target state and the current state\n        # When the current state is exactly the same as the target state, then the error is 0.0\n        # When the current state is the opposite of the target state (i.e. -target_state), the error\n        # is at its highest at 2|state|x|target| where |.| denotes the vector 2-norm\n        deviation_from_target = torch.linalg.vector_norm(state.detach(), dim=-1, keepdim=True)*torch.linalg.norm(target_state, dim=-1, keepdim=True) - torch.linalg.vecdot(state, target_state, dim=-1)[...,None]\n        return deviation_from_target\n    \n    def damping_augmented_rhs(state, time, *nn_parameters):\n        return torch.cat([\n            rhs(state[...,:-1], time, *nn_parameters),\n            damping_loss(state[...,-1:], time)\n        ], dim=-1)\n    \n    augmented_states = torch.cat([\n        states,\n        torch.zeros_like(states[...,0,None])\n    ], dim=-1)\n\n    final_augmented_state, _, _, _, _ = integrator.apply(damping_augmented_rhs, augmented_states, t_initial, t_final, initial_dt, integrator_kwargs, *parameters)\n\n    error = (target_state - final_augmented_state[...,:-1]).square().sum(dim=-1).mean() + final_augmented_state[...,-1:].sum(dim=-1).mean()\n\n    if error.requires_grad:\n        error.backward()\n    return error\n</pre> state_range_min = torch.tensor([-25.0, -5.0]) state_range_max = torch.tensor([ 25.0,  5.0])  state_dataset = torch.rand(8, 2) * (state_range_max - state_range_min)[None] + state_range_min[None]  target_state = torch.tensor([1.0, 0.0])  def damping_closure(rhs, parameters, minibatch, optimiser, integrator_kwargs, integrator=None):          if integrator is None:         integrator = neuralode.integrators.AdaptiveRK45Integrator     optimiser.zero_grad()          t_final = minibatch['final_time']     t_initial = minibatch['initial_time']     initial_dt = minibatch['dt']     states = minibatch['states']          def damping_loss(state, time):         # Measure of the unnormalised cosine alignement between the target state and the current state         # When the current state is exactly the same as the target state, then the error is 0.0         # When the current state is the opposite of the target state (i.e. -target_state), the error         # is at its highest at 2|state|x|target| where |.| denotes the vector 2-norm         deviation_from_target = torch.linalg.vector_norm(state.detach(), dim=-1, keepdim=True)*torch.linalg.norm(target_state, dim=-1, keepdim=True) - torch.linalg.vecdot(state, target_state, dim=-1)[...,None]         return deviation_from_target          def damping_augmented_rhs(state, time, *nn_parameters):         return torch.cat([             rhs(state[...,:-1], time, *nn_parameters),             damping_loss(state[...,-1:], time)         ], dim=-1)          augmented_states = torch.cat([         states,         torch.zeros_like(states[...,0,None])     ], dim=-1)      final_augmented_state, _, _, _, _ = integrator.apply(damping_augmented_rhs, augmented_states, t_initial, t_final, initial_dt, integrator_kwargs, *parameters)      error = (target_state - final_augmented_state[...,:-1]).square().sum(dim=-1).mean() + final_augmented_state[...,-1:].sum(dim=-1).mean()      if error.requires_grad:         error.backward()     return error In\u00a0[8]: Copied! <pre>driven_oscillator_net = neuralode.models.oscillator.DrivenOscillatorNet()\ndriven_oscillator_net.apply(neuralode.models.util.init_weights)\nmax_acceleration = torch.tensor(25.0)\n\ndef get_control(x, t, controller, *nn_parameters):\n    x_encoded = torch.stack([x[...,0]/100.0, x[...,1]/100.0], dim=-1)\n    control_u = torch.func.functional_call(controller, {k: p for (k, _), p in zip(controller.named_parameters(), nn_parameters)}, (x_encoded, t))\n    control_u = control_u*max_acceleration\n    return torch.cat([\n        torch.zeros_like(control_u),\n        control_u, # The network can only apply forces, not directly control the velocity of the oscillator\n    ], dim=-1)\n\ndef dha_nn_fn(x, t, frequency, damping, *nn_parameters):\n    # We rescale the input to the [-1, 1] range for better stability of the network\n    control_vec = get_control(x, t, driven_oscillator_net, *nn_parameters)\n    return neuralode.dynamics.simple_harmonic_oscillator(x, t, frequency, damping) + control_vec\n</pre> driven_oscillator_net = neuralode.models.oscillator.DrivenOscillatorNet() driven_oscillator_net.apply(neuralode.models.util.init_weights) max_acceleration = torch.tensor(25.0)  def get_control(x, t, controller, *nn_parameters):     x_encoded = torch.stack([x[...,0]/100.0, x[...,1]/100.0], dim=-1)     control_u = torch.func.functional_call(controller, {k: p for (k, _), p in zip(controller.named_parameters(), nn_parameters)}, (x_encoded, t))     control_u = control_u*max_acceleration     return torch.cat([         torch.zeros_like(control_u),         control_u, # The network can only apply forces, not directly control the velocity of the oscillator     ], dim=-1)  def dha_nn_fn(x, t, frequency, damping, *nn_parameters):     # We rescale the input to the [-1, 1] range for better stability of the network     control_vec = get_control(x, t, driven_oscillator_net, *nn_parameters)     return neuralode.dynamics.simple_harmonic_oscillator(x, t, frequency, damping) + control_vec In\u00a0[9]: Copied! <pre>batch_size = 4\nnumber_of_epochs = 64\n\ndriven_oscillator_optimiser = torch.optim.Adam(driven_oscillator_net.parameters(), lr=1e-4, amsgrad=True)\none_cycle_lr_driven_oscillator = torch.optim.lr_scheduler.OneCycleLR(driven_oscillator_optimiser, max_lr=2e-2, div_factor=200.0, steps_per_epoch=round(state_dataset.shape[0]/batch_size+0.5), epochs=number_of_epochs, three_phase=False)\n</pre> batch_size = 4 number_of_epochs = 64  driven_oscillator_optimiser = torch.optim.Adam(driven_oscillator_net.parameters(), lr=1e-4, amsgrad=True) one_cycle_lr_driven_oscillator = torch.optim.lr_scheduler.OneCycleLR(driven_oscillator_optimiser, max_lr=2e-2, div_factor=200.0, steps_per_epoch=round(state_dataset.shape[0]/batch_size+0.5), epochs=number_of_epochs, three_phase=False) In\u00a0[10]: Copied! <pre>driven_oscillator_net.train()\n\nbest_error = torch.inf\nbest_parameters = copy.deepcopy(driven_oscillator_net.state_dict())\n\ncommon_closure_args = [driven_oscillator_optimiser, {'atol': atol, 'rtol': rtol}, current_integrator]\n\nfor step in range(number_of_epochs):\n    epoch_error = 0.0\n    shuffled_indices = torch.randperm(state_dataset.shape[0])\n    for batch_idx in range(0, state_dataset.shape[0], batch_size):\n        batch_dict = {\n            'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n            'initial_time': initial_time.detach().clone(),\n            'final_time': initial_time.detach().clone() + 2.5,\n            'dt': initial_timestep.detach().clone()\n        }\n\n        step_error = driven_oscillator_optimiser.step(lambda: damping_closure(dha_nn_fn, [frequency, torch.zeros_like(damping)] + list(driven_oscillator_net.parameters()), batch_dict, *common_closure_args))\n        one_cycle_lr_driven_oscillator.step()\n        epoch_error = epoch_error + step_error.item()*batch_dict['states'].shape[0]\n        print(f\"[{step+1}/{number_of_epochs} - lr: {one_cycle_lr_driven_oscillator.get_last_lr()[0]:.4e}]/[{batch_idx}/{state_dataset.shape[0]}] Batch Error: {step_error:.4f} \", end='\\r', flush=True)\n    epoch_error = epoch_error/state_dataset.shape[0]\n    if epoch_error &lt; best_error:\n        best_error = epoch_error\n        best_parameters = copy.deepcopy(driven_oscillator_net.state_dict())\n    print(\" \"*128, end='\\r', flush=True)\n    print(f\"[{step+1}/{number_of_epochs} - lr: {one_cycle_lr_driven_oscillator.get_last_lr()[0]:.4e}] Epoch Error: {epoch_error:.6}\")\n    print()\n</pre> driven_oscillator_net.train()  best_error = torch.inf best_parameters = copy.deepcopy(driven_oscillator_net.state_dict())  common_closure_args = [driven_oscillator_optimiser, {'atol': atol, 'rtol': rtol}, current_integrator]  for step in range(number_of_epochs):     epoch_error = 0.0     shuffled_indices = torch.randperm(state_dataset.shape[0])     for batch_idx in range(0, state_dataset.shape[0], batch_size):         batch_dict = {             'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],             'initial_time': initial_time.detach().clone(),             'final_time': initial_time.detach().clone() + 2.5,             'dt': initial_timestep.detach().clone()         }          step_error = driven_oscillator_optimiser.step(lambda: damping_closure(dha_nn_fn, [frequency, torch.zeros_like(damping)] + list(driven_oscillator_net.parameters()), batch_dict, *common_closure_args))         one_cycle_lr_driven_oscillator.step()         epoch_error = epoch_error + step_error.item()*batch_dict['states'].shape[0]         print(f\"[{step+1}/{number_of_epochs} - lr: {one_cycle_lr_driven_oscillator.get_last_lr()[0]:.4e}]/[{batch_idx}/{state_dataset.shape[0]}] Batch Error: {step_error:.4f} \", end='\\r', flush=True)     epoch_error = epoch_error/state_dataset.shape[0]     if epoch_error &lt; best_error:         best_error = epoch_error         best_parameters = copy.deepcopy(driven_oscillator_net.state_dict())     print(\" \"*128, end='\\r', flush=True)     print(f\"[{step+1}/{number_of_epochs} - lr: {one_cycle_lr_driven_oscillator.get_last_lr()[0]:.4e}] Epoch Error: {epoch_error:.6}\")     print() <pre>[1/64 - lr: 2.4008e-04] Epoch Error: 151.945                                                                                    \n\n[2/64 - lr: 6.5639e-04] Epoch Error: 151.252                                                                                    \n\n[3/64 - lr: 1.3372e-03] Epoch Error: 149.567                                                                                    \n\n[4/64 - lr: 2.2633e-03] Epoch Error: 146.498                                                                                    \n\n[5/64 - lr: 3.4087e-03] Epoch Error: 139.44                                                                                     \n\n[6/64 - lr: 4.7411e-03] Epoch Error: 140.579                                                                                    \n\n[7/64 - lr: 6.2230e-03] Epoch Error: 128.389                                                                                    \n\n[8/64 - lr: 7.8126e-03] Epoch Error: 117.897                                                                                    \n\n[9/64 - lr: 9.4653e-03] Epoch Error: 94.8791                                                                                    \n\n[10/64 - lr: 1.1134e-02] Epoch Error: 74.8794                                                                                   \n\n[11/64 - lr: 1.2773e-02] Epoch Error: 42.4302                                                                                   \n\n[12/64 - lr: 1.4335e-02] Epoch Error: 33.4042                                                                                   \n\n[13/64 - lr: 1.5776e-02] Epoch Error: 31.0913                                                                                   \n\n[14/64 - lr: 1.7056e-02] Epoch Error: 19.6234                                                                                   \n\n[15/64 - lr: 1.8139e-02] Epoch Error: 7.62874                                                                                   \n\n[16/64 - lr: 1.8994e-02] Epoch Error: 4.02601                                                                                   \n\n[17/64 - lr: 1.9597e-02] Epoch Error: 1.27341                                                                                   \n\n[18/64 - lr: 1.9931e-02] Epoch Error: 2.04025                                                                                   \n\n[19/64 - lr: 1.9998e-02] Epoch Error: 2.61187                                                                                   \n\n[20/64 - lr: 1.9958e-02] Epoch Error: 1.27375                                                                                   \n\n[21/64 - lr: 1.9870e-02] Epoch Error: 0.852886                                                                                  \n\n[22/64 - lr: 1.9733e-02] Epoch Error: 0.332227                                                                                  \n\n[23/64 - lr: 1.9549e-02] Epoch Error: 0.525715                                                                                  \n\n[24/64 - lr: 1.9317e-02] Epoch Error: 0.386378                                                                                  \n\n[25/64 - lr: 1.9040e-02] Epoch Error: 0.451681                                                                                  \n\n[26/64 - lr: 1.8718e-02] Epoch Error: 0.369986                                                                                  \n\n[27/64 - lr: 1.8353e-02] Epoch Error: 0.171453                                                                                  \n\n[28/64 - lr: 1.7948e-02] Epoch Error: 0.126298                                                                                  \n\n[29/64 - lr: 1.7503e-02] Epoch Error: 0.096483                                                                                  \n\n[30/64 - lr: 1.7021e-02] Epoch Error: 0.0432904                                                                                 \n\n[31/64 - lr: 1.6505e-02] Epoch Error: 0.111354                                                                                  \n\n[32/64 - lr: 1.5957e-02] Epoch Error: 0.12867                                                                                   \n\n[33/64 - lr: 1.5380e-02] Epoch Error: 0.0925516                                                                                 \n\n[34/64 - lr: 1.4776e-02] Epoch Error: 0.130442                                                                                  \n\n[35/64 - lr: 1.4148e-02] Epoch Error: 0.0731119                                                                                 \n\n[36/64 - lr: 1.3501e-02] Epoch Error: 0.0871143                                                                                 \n\n[37/64 - lr: 1.2836e-02] Epoch Error: 0.0568891                                                                                 \n\n[38/64 - lr: 1.2157e-02] Epoch Error: 0.0849734                                                                                 \n\n[39/64 - lr: 1.1467e-02] Epoch Error: 0.0672761                                                                                 \n\n[40/64 - lr: 1.0771e-02] Epoch Error: 0.0692755                                                                                 \n\n[41/64 - lr: 1.0070e-02] Epoch Error: 0.0560489                                                                                 \n\n[42/64 - lr: 9.3693e-03] Epoch Error: 0.0513731                                                                                 \n\n[43/64 - lr: 8.6716e-03] Epoch Error: 0.0604199                                                                                 \n\n[44/64 - lr: 7.9804e-03] Epoch Error: 0.0440056                                                                                 \n\n[45/64 - lr: 7.2991e-03] Epoch Error: 0.0508528                                                                                 \n\n[46/64 - lr: 6.6311e-03] Epoch Error: 0.0547071                                                                                 \n\n[47/64 - lr: 5.9797e-03] Epoch Error: 0.0434798                                                                                 \n\n[48/64 - lr: 5.3480e-03] Epoch Error: 0.0451367                                                                                 \n\n[49/64 - lr: 4.7392e-03] Epoch Error: 0.047124                                                                                  \n\n[50/64 - lr: 4.1562e-03] Epoch Error: 0.0455598                                                                                 \n\n[51/64 - lr: 3.6020e-03] Epoch Error: 0.0439628                                                                                 \n\n[52/64 - lr: 3.0793e-03] Epoch Error: 0.0436574                                                                                 \n\n[53/64 - lr: 2.5905e-03] Epoch Error: 0.044996                                                                                  \n\n[54/64 - lr: 2.1382e-03] Epoch Error: 0.0455611                                                                                 \n\n[55/64 - lr: 1.7245e-03] Epoch Error: 0.0442852                                                                                 \n\n[56/64 - lr: 1.3515e-03] Epoch Error: 0.0433767                                                                                 \n\n[57/64 - lr: 1.0210e-03] Epoch Error: 0.043196                                                                                  \n\n[58/64 - lr: 7.3461e-04] Epoch Error: 0.0436175                                                                                 \n\n[59/64 - lr: 4.9379e-04] Epoch Error: 0.0432106                                                                                 \n\n[60/64 - lr: 2.9970e-04] Epoch Error: 0.0431902                                                                                 \n\n[61/64 - lr: 1.5329e-04] Epoch Error: 0.0432194                                                                                 \n\n[62/64 - lr: 5.5281e-05] Epoch Error: 0.0432728                                                                                 \n\n[63/64 - lr: 6.1562e-06] Epoch Error: 0.043241                                                                                  \n\n[64/64 - lr: 6.1562e-06] Epoch Error: 0.0432406                                                                                 \n\n</pre> In\u00a0[11]: Copied! <pre>test_range_min = torch.tensor([-256, -32.0])\ntest_range_max = torch.tensor([ 256,  32.0])\n\ntest_state_dataset = torch.rand(64, 2) * (test_range_max - test_range_min)[None] + test_range_min[None]\n</pre> test_range_min = torch.tensor([-256, -32.0]) test_range_max = torch.tensor([ 256,  32.0])  test_state_dataset = torch.rand(64, 2) * (test_range_max - test_range_min)[None] + test_range_min[None] In\u00a0[12]: Copied! <pre>driven_oscillator_net.load_state_dict(best_parameters)\ndriven_oscillator_net.eval()\n\ntest_final_time = final_time*2.0\n\n_, _, dha_states_optimised, dha_times_optimised, _ = current_integrator.apply(dha_nn_fn, initial_state, initial_time, test_final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, frequency, torch.zeros_like(damping))\n\nfig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None)\nfig_ref_position.suptitle(\"Position\")\nfig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None)\nfig_ref_velocity.suptitle(\"Velocity\")\n\n_, _, integrated_test_states, integrated_test_times, _ = current_integrator.apply(dha_nn_fn, test_state_dataset, initial_time, test_final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, frequency, torch.zeros_like(damping))\n\n_ = neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_position, method_label=None)\n_ = neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_velocity, method_label=None)\n\nprint(\"Training results on test data:\")\nprint(f\"\\tMean position at t={test_final_time.item():.2f}s: {integrated_test_states[-1, ..., 0].mean().item():.4e} \u00b1 {integrated_test_states[-1, ..., 0].std().item():.4e}\")\nprint(f\"\\tMean position error at t={test_final_time.item():.2f}s: {(target_state - integrated_test_states)[-1, ..., 0].mean().item():.4e} \u00b1 {(target_state - integrated_test_states)[-1, ..., 0].std().item():.4e}\")\nprint()\nprint(f\"\\tMean velocity at t={test_final_time.item():.2f}s: {integrated_test_states[-1, ..., 1].mean().item():.4e} \u00b1 {integrated_test_states[-1, ..., 1].std().item():.4e}\")\nprint(f\"\\tMean velocity error at t={test_final_time.item():.2f}s: {(target_state - integrated_test_states)[-1, ..., 1].mean().item():.4e} \u00b1 {(target_state - integrated_test_states)[-1, ..., 1].std().item():.4e}\")\n\n\nfig_ref_position_closeup, axes_ref_position_closeup = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None)\nfig_ref_velocity_closeup, axes_ref_velocity_closeup = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None)\n\n_ = neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_position_closeup, method_label=None)\nfig_ref_position_closeup.suptitle(\"Close-Up of Position\")\naxes_ref_position_closeup[0].set_xlim(test_final_time.item() - 5.0, test_final_time.item())\naxes_ref_position_closeup[0].set_ylim(target_state[0].item() - 1.0, target_state[0].item() + 1.0)\n_ = neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_velocity_closeup, method_label=None)\nfig_ref_velocity_closeup.suptitle(\"Close-Up of Velocity\")\naxes_ref_velocity_closeup[0].set_xlim(test_final_time.item() - 5.0, test_final_time.item())\naxes_ref_velocity_closeup[0].set_ylim(target_state[1].item() - 1.0, target_state[1].item() + 1.0)\n</pre> driven_oscillator_net.load_state_dict(best_parameters) driven_oscillator_net.eval()  test_final_time = final_time*2.0  _, _, dha_states_optimised, dha_times_optimised, _ = current_integrator.apply(dha_nn_fn, initial_state, initial_time, test_final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, frequency, torch.zeros_like(damping))  fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None) fig_ref_position.suptitle(\"Position\") fig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None) fig_ref_velocity.suptitle(\"Velocity\")  _, _, integrated_test_states, integrated_test_times, _ = current_integrator.apply(dha_nn_fn, test_state_dataset, initial_time, test_final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, frequency, torch.zeros_like(damping))  _ = neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_position, method_label=None) _ = neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_velocity, method_label=None)  print(\"Training results on test data:\") print(f\"\\tMean position at t={test_final_time.item():.2f}s: {integrated_test_states[-1, ..., 0].mean().item():.4e} \u00b1 {integrated_test_states[-1, ..., 0].std().item():.4e}\") print(f\"\\tMean position error at t={test_final_time.item():.2f}s: {(target_state - integrated_test_states)[-1, ..., 0].mean().item():.4e} \u00b1 {(target_state - integrated_test_states)[-1, ..., 0].std().item():.4e}\") print() print(f\"\\tMean velocity at t={test_final_time.item():.2f}s: {integrated_test_states[-1, ..., 1].mean().item():.4e} \u00b1 {integrated_test_states[-1, ..., 1].std().item():.4e}\") print(f\"\\tMean velocity error at t={test_final_time.item():.2f}s: {(target_state - integrated_test_states)[-1, ..., 1].mean().item():.4e} \u00b1 {(target_state - integrated_test_states)[-1, ..., 1].std().item():.4e}\")   fig_ref_position_closeup, axes_ref_position_closeup = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None) fig_ref_velocity_closeup, axes_ref_velocity_closeup = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None)  _ = neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_position_closeup, method_label=None) fig_ref_position_closeup.suptitle(\"Close-Up of Position\") axes_ref_position_closeup[0].set_xlim(test_final_time.item() - 5.0, test_final_time.item()) axes_ref_position_closeup[0].set_ylim(target_state[0].item() - 1.0, target_state[0].item() + 1.0) _ = neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_velocity_closeup, method_label=None) fig_ref_velocity_closeup.suptitle(\"Close-Up of Velocity\") axes_ref_velocity_closeup[0].set_xlim(test_final_time.item() - 5.0, test_final_time.item()) axes_ref_velocity_closeup[0].set_ylim(target_state[1].item() - 1.0, target_state[1].item() + 1.0) <pre>C:\\Users\\ekin4\\PycharmProjects\\ReCoDE-NeuralODEs\\neuralode\\plot\\trajectory.py:79: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\nC:\\Users\\ekin4\\PycharmProjects\\ReCoDE-NeuralODEs\\neuralode\\plot\\trajectory.py:79: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\n</pre> <pre>Training results on test data:\n\tMean position at t=50.00s: 9.3575e-01 \u00b1 3.4454e-11\n\tMean position error at t=50.00s: 6.4254e-02 \u00b1 3.4454e-11\n\n\tMean velocity at t=50.00s: -2.4343e-11 \u00b1 2.2306e-10\n\tMean velocity error at t=50.00s: 2.4343e-11 \u00b1 2.2306e-10\n</pre> Out[12]: <pre>(-1.0, 1.0)</pre> <p>Here we can see that the network has learned how to dampen the oscillations of the system and nudge it towards the target state at the end of the trajectory. Although the damping isn't perfect (i.e. critically damped), the network is able to damp configurations that it has not encountered before within 20 seconds.</p> In\u00a0[13]: Copied! <pre># Generate reference trajectory for optimisation/learning\nwith torch.no_grad():\n    _, _, sha_states_ref, sha_times_ref, _ = current_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, {'atol': torch.zeros_like(atol), 'rtol': rtol}, frequency, damping)\nsha_states_ref, sha_times_ref = sha_states_ref.detach(), sha_times_ref.detach()\n</pre> # Generate reference trajectory for optimisation/learning with torch.no_grad():     _, _, sha_states_ref, sha_times_ref, _ = current_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, {'atol': torch.zeros_like(atol), 'rtol': rtol}, frequency, damping) sha_states_ref, sha_times_ref = sha_states_ref.detach(), sha_times_ref.detach() In\u00a0[14]: Copied! <pre>torch.manual_seed(36)\ndynamics_network = neuralode.models.oscillator.OscillatorNet()\ncontrol_network = neuralode.models.oscillator.DrivenOscillatorNet()\n\ndynamics_network.apply(neuralode.models.util.init_weights)\ncontrol_network.apply(neuralode.models.util.init_weights)\n\nmax_acceleration = torch.tensor(25.0)\n\ndef learned_dynamics_rhs(x, t, *nn_parameters):\n    if len(nn_parameters) &gt; 0:\n        return torch.func.functional_call(dynamics_network, {k: p for (k, _), p in zip(dynamics_network.named_parameters(), nn_parameters)}, (x, t))\n    else:\n        return dynamics_network(x, t)\n\ndef controlled_dynamics_rhs(x, t, *nn_parameters):\n    control_vec = get_control(x, t, control_network, *nn_parameters)\n    return learned_dynamics_rhs(x, t) + control_vec\n</pre> torch.manual_seed(36) dynamics_network = neuralode.models.oscillator.OscillatorNet() control_network = neuralode.models.oscillator.DrivenOscillatorNet()  dynamics_network.apply(neuralode.models.util.init_weights) control_network.apply(neuralode.models.util.init_weights)  max_acceleration = torch.tensor(25.0)  def learned_dynamics_rhs(x, t, *nn_parameters):     if len(nn_parameters) &gt; 0:         return torch.func.functional_call(dynamics_network, {k: p for (k, _), p in zip(dynamics_network.named_parameters(), nn_parameters)}, (x, t))     else:         return dynamics_network(x, t)  def controlled_dynamics_rhs(x, t, *nn_parameters):     control_vec = get_control(x, t, control_network, *nn_parameters)     return learned_dynamics_rhs(x, t) + control_vec In\u00a0[15]: Copied! <pre>number_of_epochs = 32\nnumber_of_dynamics_cycles = 8\nnumber_of_control_cycles = 4\n\ndynamics_optimiser = torch.optim.Adam(dynamics_network.parameters(), lr=1e-3, amsgrad=True)\ncontrol_optimiser = torch.optim.Adam(control_network.parameters(), lr=1e-3, amsgrad=True)\n\none_cycle_lr_dynamics = torch.optim.lr_scheduler.OneCycleLR(dynamics_optimiser, max_lr=1e-1, div_factor=50.0, steps_per_epoch=round(sha_states_ref.shape[0]/batch_size+0.5), epochs=number_of_epochs*number_of_dynamics_cycles, three_phase=True)\none_cycle_lr_control = torch.optim.lr_scheduler.OneCycleLR(control_optimiser, max_lr=2e-2, div_factor=200.0, steps_per_epoch=round(state_dataset.shape[0]/batch_size+0.5), epochs=number_of_epochs*number_of_control_cycles, three_phase=False)\n\nideal_matrix = neuralode.dynamics.get_simple_harmonic_oscillator_matrix(frequency, damping)\nideal_bias = torch.zeros_like(initial_state)\n</pre> number_of_epochs = 32 number_of_dynamics_cycles = 8 number_of_control_cycles = 4  dynamics_optimiser = torch.optim.Adam(dynamics_network.parameters(), lr=1e-3, amsgrad=True) control_optimiser = torch.optim.Adam(control_network.parameters(), lr=1e-3, amsgrad=True)  one_cycle_lr_dynamics = torch.optim.lr_scheduler.OneCycleLR(dynamics_optimiser, max_lr=1e-1, div_factor=50.0, steps_per_epoch=round(sha_states_ref.shape[0]/batch_size+0.5), epochs=number_of_epochs*number_of_dynamics_cycles, three_phase=True) one_cycle_lr_control = torch.optim.lr_scheduler.OneCycleLR(control_optimiser, max_lr=2e-2, div_factor=200.0, steps_per_epoch=round(state_dataset.shape[0]/batch_size+0.5), epochs=number_of_epochs*number_of_control_cycles, three_phase=False)  ideal_matrix = neuralode.dynamics.get_simple_harmonic_oscillator_matrix(frequency, damping) ideal_bias = torch.zeros_like(initial_state) In\u00a0[16]: Copied! <pre>best_dynamics_error = torch.inf\nbest_control_error = torch.inf\n\nbest_dynamics_parameters = copy.deepcopy(dynamics_network.state_dict())\nbest_control_parameters = copy.deepcopy(control_network.state_dict())\n\ncontrol_network.train()\ndynamics_network.train()\n\ncommon_dyn_closure_args = [dynamics_optimiser, {'atol': atol, 'rtol': rtol}, current_integrator]\ncommon_ctrl_closure_args = [control_optimiser, {'atol': atol, 'rtol': rtol}, current_integrator]\n\nfor epoch in range(number_of_epochs):\n    # First phase is to train the dynamics as it wouldn't make sense to learn control of\n    # random dynamics.\n    dynamics_network.train()\n    for cycle in range(number_of_dynamics_cycles):\n        dynamics_epoch_error = 0.0\n        shuffled_indices = torch.randperm(sha_times_ref.shape[0])\n        for batch_idx in range(0, sha_times_ref.shape[0], batch_size):\n            batch_dict = {\n                'times': sha_times_ref[shuffled_indices][batch_idx:batch_idx+batch_size],\n                'states': sha_states_ref[shuffled_indices][batch_idx:batch_idx+batch_size],\n                'initial_state': initial_state.detach().clone(),\n                'initial_time': initial_time.detach().clone(),\n                'dt': initial_timestep.detach().clone(),\n            }\n    \n            step_error = dynamics_optimiser.step(lambda: neuralode.closures.dynamics_closure(learned_dynamics_rhs, list(dynamics_network.parameters()), batch_dict, *common_dyn_closure_args))\n            one_cycle_lr_dynamics.step()\n            \n            dynamics_epoch_error = dynamics_epoch_error + step_error.item()*batch_dict['times'].shape[0]\n            print(f\"(Dynamics) Batch: [{batch_idx}/{sha_times_ref.shape[0]}] Error: {step_error:.6f} \", end='\\r', flush=True)\n        dynamics_epoch_error = dynamics_epoch_error/sha_times_ref.shape[0]\n        if dynamics_epoch_error &lt; best_dynamics_error:\n            best_dynamics_error = dynamics_epoch_error\n            best_dynamics_parameters = copy.deepcopy(dynamics_network.state_dict())\n        learned_matrix = dynamics_network.state_dict()['internal_net.0.weight']\n        learned_bias = dynamics_network.state_dict()['internal_net.0.bias']\n        print(\" \"*128, end='\\r', flush=True)\n        print(f\"(Dynamics) Cycle: [{cycle+1}/{number_of_dynamics_cycles} - lr: {one_cycle_lr_dynamics.get_last_lr()[0]:.4e}] Error: {dynamics_epoch_error:.6f}\")\n\n    # Second phase is to train the control of the dynamics\n    # We would like to learn the controls on the best representation of the dynamics,\n    # but continue the optimisation as normal.\n    # So we save the current dynamics network state and load the best state\n    current_dynamics_parameters = copy.deepcopy(dynamics_network.state_dict())\n    dynamics_network.load_state_dict(best_dynamics_parameters)\n    dynamics_network.eval()\n    \n    for cycle in range(number_of_control_cycles):\n        control_epoch_error = 0.0\n        shuffled_indices = torch.randperm(state_dataset.shape[0])\n        for batch_idx in range(0, state_dataset.shape[0], batch_size):\n            batch_dict = {\n                'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n                'initial_time': initial_time.detach().clone(),\n                'final_time': initial_time.detach().clone() + 2.5,\n                'dt': initial_timestep.detach().clone(),\n            }\n    \n            step_error = control_optimiser.step(lambda: damping_closure(controlled_dynamics_rhs, list(control_network.parameters()), batch_dict, *common_ctrl_closure_args))\n            one_cycle_lr_control.step()\n            \n            control_epoch_error = control_epoch_error + step_error.item()*batch_dict['states'].shape[0]\n            print(f\"(Control) Batch: [{batch_idx}/{state_dataset.shape[0]}] Error: {step_error:.4f} \", end='\\r', flush=True)\n        control_epoch_error = control_epoch_error/state_dataset.shape[0]\n        if control_epoch_error &lt; best_control_error:\n            best_control_error = control_epoch_error\n            best_control_parameters = copy.deepcopy(control_network.state_dict())\n        print(\" \"*128, end='\\r', flush=True)\n        print(f\"(Control) Cycle: [{cycle+1}/{number_of_control_cycles} - lr: {one_cycle_lr_control.get_last_lr()[0]:.4e}] Error: {control_epoch_error:.4f}\")\n    print(f\"[{epoch+1}/{number_of_epochs}] Best Error: (Dynamics) {best_dynamics_error:.6f}/(Control) {best_control_error:.6f}\")\n    print()\n    # And then restore the state to continue the optimisation\n    # (If we reset to the best every epoch, the optimisation may get stuck in a local minimum)\n    dynamics_network.load_state_dict(current_dynamics_parameters)\n</pre> best_dynamics_error = torch.inf best_control_error = torch.inf  best_dynamics_parameters = copy.deepcopy(dynamics_network.state_dict()) best_control_parameters = copy.deepcopy(control_network.state_dict())  control_network.train() dynamics_network.train()  common_dyn_closure_args = [dynamics_optimiser, {'atol': atol, 'rtol': rtol}, current_integrator] common_ctrl_closure_args = [control_optimiser, {'atol': atol, 'rtol': rtol}, current_integrator]  for epoch in range(number_of_epochs):     # First phase is to train the dynamics as it wouldn't make sense to learn control of     # random dynamics.     dynamics_network.train()     for cycle in range(number_of_dynamics_cycles):         dynamics_epoch_error = 0.0         shuffled_indices = torch.randperm(sha_times_ref.shape[0])         for batch_idx in range(0, sha_times_ref.shape[0], batch_size):             batch_dict = {                 'times': sha_times_ref[shuffled_indices][batch_idx:batch_idx+batch_size],                 'states': sha_states_ref[shuffled_indices][batch_idx:batch_idx+batch_size],                 'initial_state': initial_state.detach().clone(),                 'initial_time': initial_time.detach().clone(),                 'dt': initial_timestep.detach().clone(),             }                  step_error = dynamics_optimiser.step(lambda: neuralode.closures.dynamics_closure(learned_dynamics_rhs, list(dynamics_network.parameters()), batch_dict, *common_dyn_closure_args))             one_cycle_lr_dynamics.step()                          dynamics_epoch_error = dynamics_epoch_error + step_error.item()*batch_dict['times'].shape[0]             print(f\"(Dynamics) Batch: [{batch_idx}/{sha_times_ref.shape[0]}] Error: {step_error:.6f} \", end='\\r', flush=True)         dynamics_epoch_error = dynamics_epoch_error/sha_times_ref.shape[0]         if dynamics_epoch_error &lt; best_dynamics_error:             best_dynamics_error = dynamics_epoch_error             best_dynamics_parameters = copy.deepcopy(dynamics_network.state_dict())         learned_matrix = dynamics_network.state_dict()['internal_net.0.weight']         learned_bias = dynamics_network.state_dict()['internal_net.0.bias']         print(\" \"*128, end='\\r', flush=True)         print(f\"(Dynamics) Cycle: [{cycle+1}/{number_of_dynamics_cycles} - lr: {one_cycle_lr_dynamics.get_last_lr()[0]:.4e}] Error: {dynamics_epoch_error:.6f}\")      # Second phase is to train the control of the dynamics     # We would like to learn the controls on the best representation of the dynamics,     # but continue the optimisation as normal.     # So we save the current dynamics network state and load the best state     current_dynamics_parameters = copy.deepcopy(dynamics_network.state_dict())     dynamics_network.load_state_dict(best_dynamics_parameters)     dynamics_network.eval()          for cycle in range(number_of_control_cycles):         control_epoch_error = 0.0         shuffled_indices = torch.randperm(state_dataset.shape[0])         for batch_idx in range(0, state_dataset.shape[0], batch_size):             batch_dict = {                 'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],                 'initial_time': initial_time.detach().clone(),                 'final_time': initial_time.detach().clone() + 2.5,                 'dt': initial_timestep.detach().clone(),             }                  step_error = control_optimiser.step(lambda: damping_closure(controlled_dynamics_rhs, list(control_network.parameters()), batch_dict, *common_ctrl_closure_args))             one_cycle_lr_control.step()                          control_epoch_error = control_epoch_error + step_error.item()*batch_dict['states'].shape[0]             print(f\"(Control) Batch: [{batch_idx}/{state_dataset.shape[0]}] Error: {step_error:.4f} \", end='\\r', flush=True)         control_epoch_error = control_epoch_error/state_dataset.shape[0]         if control_epoch_error &lt; best_control_error:             best_control_error = control_epoch_error             best_control_parameters = copy.deepcopy(control_network.state_dict())         print(\" \"*128, end='\\r', flush=True)         print(f\"(Control) Cycle: [{cycle+1}/{number_of_control_cycles} - lr: {one_cycle_lr_control.get_last_lr()[0]:.4e}] Error: {control_epoch_error:.4f}\")     print(f\"[{epoch+1}/{number_of_epochs}] Best Error: (Dynamics) {best_dynamics_error:.6f}/(Control) {best_control_error:.6f}\")     print()     # And then restore the state to continue the optimisation     # (If we reset to the best every epoch, the optimisation may get stuck in a local minimum)     dynamics_network.load_state_dict(current_dynamics_parameters) <pre>(Dynamics) Cycle: [1/8 - lr: 2.0411e-03] Error: 1.631016                                                                        \n(Dynamics) Cycle: [2/8 - lr: 2.1642e-03] Error: 1.084869                                                                        \n(Dynamics) Cycle: [3/8 - lr: 2.3692e-03] Error: 0.713909                                                                        \n(Dynamics) Cycle: [4/8 - lr: 2.6557e-03] Error: 0.534566                                                                        \n(Dynamics) Cycle: [5/8 - lr: 3.0232e-03] Error: 0.421840                                                                        \n(Dynamics) Cycle: [6/8 - lr: 3.4712e-03] Error: 0.396048                                                                        \n(Dynamics) Cycle: [7/8 - lr: 3.9988e-03] Error: 0.388483                                                                        \n(Dynamics) Cycle: [8/8 - lr: 4.6052e-03] Error: 0.368714                                                                        \n(Control) Cycle: [1/4 - lr: 1.3416e-04] Error: 197.5658                                                                         \n(Control) Cycle: [2/4 - lr: 2.3642e-04] Error: 196.5516                                                                         \n(Control) Cycle: [3/4 - lr: 4.0607e-04] Error: 195.6410                                                                         \n(Control) Cycle: [4/4 - lr: 6.4194e-04] Error: 194.0422                                                                         \n[1/32] Best Error: (Dynamics) 0.368714/(Control) 194.042170\n\n(Dynamics) Cycle: [1/8 - lr: 5.2894e-03] Error: 0.357290                                                                        \n(Dynamics) Cycle: [2/8 - lr: 6.0502e-03] Error: 0.341688                                                                        \n(Dynamics) Cycle: [3/8 - lr: 6.8863e-03] Error: 0.328375                                                                        \n(Dynamics) Cycle: [4/8 - lr: 7.7964e-03] Error: 0.308969                                                                        \n(Dynamics) Cycle: [5/8 - lr: 8.7789e-03] Error: 0.296022                                                                        \n(Dynamics) Cycle: [6/8 - lr: 9.8322e-03] Error: 0.278045                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.0954e-02] Error: 0.261383                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.2144e-02] Error: 0.244250                                                                        \n(Control) Cycle: [1/4 - lr: 9.4242e-04] Error: 113.7902                                                                         \n(Control) Cycle: [2/4 - lr: 1.3054e-03] Error: 110.8693                                                                         \n(Control) Cycle: [3/4 - lr: 1.7285e-03] Error: 108.8710                                                                         \n(Control) Cycle: [4/4 - lr: 2.2087e-03] Error: 105.6138                                                                         \n[2/32] Best Error: (Dynamics) 0.244250/(Control) 105.613768\n\n(Dynamics) Cycle: [1/8 - lr: 1.3398e-02] Error: 0.228030                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.4716e-02] Error: 0.216600                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.6094e-02] Error: 0.209134                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.7531e-02] Error: 0.202860                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.9024e-02] Error: 0.193912                                                                        \n(Dynamics) Cycle: [6/8 - lr: 2.0571e-02] Error: 0.191886                                                                        \n(Dynamics) Cycle: [7/8 - lr: 2.2168e-02] Error: 0.187635                                                                        \n(Dynamics) Cycle: [8/8 - lr: 2.3814e-02] Error: 0.182537                                                                        \n(Control) Cycle: [1/4 - lr: 2.7428e-03] Error: 57.6988                                                                          \n(Control) Cycle: [2/4 - lr: 3.3270e-03] Error: 60.4677                                                                          \n(Control) Cycle: [3/4 - lr: 3.9574e-03] Error: 61.5710                                                                          \n(Control) Cycle: [4/4 - lr: 4.6297e-03] Error: 58.6384                                                                          \n[3/32] Best Error: (Dynamics) 0.182537/(Control) 57.698777\n\n(Dynamics) Cycle: [1/8 - lr: 2.5506e-02] Error: 0.177405                                                                        \n(Dynamics) Cycle: [2/8 - lr: 2.7240e-02] Error: 0.175327                                                                        \n(Dynamics) Cycle: [3/8 - lr: 2.9014e-02] Error: 0.173151                                                                        \n(Dynamics) Cycle: [4/8 - lr: 3.0825e-02] Error: 0.175517                                                                        \n(Dynamics) Cycle: [5/8 - lr: 3.2670e-02] Error: 0.167917                                                                        \n(Dynamics) Cycle: [6/8 - lr: 3.4545e-02] Error: 0.168342                                                                        \n(Dynamics) Cycle: [7/8 - lr: 3.6448e-02] Error: 0.173596                                                                        \n(Dynamics) Cycle: [8/8 - lr: 3.8376e-02] Error: 0.167835                                                                        \n(Control) Cycle: [1/4 - lr: 5.3392e-03] Error: 32.6097                                                                          \n(Control) Cycle: [2/4 - lr: 6.0810e-03] Error: 29.8663                                                                          \n(Control) Cycle: [3/4 - lr: 6.8501e-03] Error: 32.3584                                                                          \n(Control) Cycle: [4/4 - lr: 7.6411e-03] Error: 33.1556                                                                          \n[4/32] Best Error: (Dynamics) 0.167835/(Control) 29.866251\n\n(Dynamics) Cycle: [1/8 - lr: 4.0324e-02] Error: 0.161230                                                                        \n(Dynamics) Cycle: [2/8 - lr: 4.2291e-02] Error: 0.158924                                                                        \n(Dynamics) Cycle: [3/8 - lr: 4.4272e-02] Error: 0.157252                                                                        \n(Dynamics) Cycle: [4/8 - lr: 4.6264e-02] Error: 0.158818                                                                        \n(Dynamics) Cycle: [5/8 - lr: 4.8264e-02] Error: 0.155971                                                                        \n(Dynamics) Cycle: [6/8 - lr: 5.0269e-02] Error: 0.151703                                                                        \n(Dynamics) Cycle: [7/8 - lr: 5.2275e-02] Error: 0.152077                                                                        \n(Dynamics) Cycle: [8/8 - lr: 5.4279e-02] Error: 0.147333                                                                        \n(Control) Cycle: [1/4 - lr: 8.4487e-03] Error: 16.9703                                                                          \n(Control) Cycle: [2/4 - lr: 9.2673e-03] Error: 16.6878                                                                          \n(Control) Cycle: [3/4 - lr: 1.0091e-02] Error: 17.3312                                                                          \n(Control) Cycle: [4/4 - lr: 1.0915e-02] Error: 16.9633                                                                          \n[5/32] Best Error: (Dynamics) 0.147333/(Control) 16.687759\n\n(Dynamics) Cycle: [1/8 - lr: 5.6278e-02] Error: 0.145760                                                                        \n(Dynamics) Cycle: [2/8 - lr: 5.8267e-02] Error: 0.143774                                                                        \n(Dynamics) Cycle: [3/8 - lr: 6.0245e-02] Error: 0.142286                                                                        \n(Dynamics) Cycle: [4/8 - lr: 6.2207e-02] Error: 0.142686                                                                        \n(Dynamics) Cycle: [5/8 - lr: 6.4150e-02] Error: 0.140096                                                                        \n(Dynamics) Cycle: [6/8 - lr: 6.6071e-02] Error: 0.139707                                                                        \n(Dynamics) Cycle: [7/8 - lr: 6.7967e-02] Error: 0.136047                                                                        \n(Dynamics) Cycle: [8/8 - lr: 6.9834e-02] Error: 0.134965                                                                        \n(Control) Cycle: [1/4 - lr: 1.1733e-02] Error: 10.8345                                                                          \n(Control) Cycle: [2/4 - lr: 1.2539e-02] Error: 11.2427                                                                          \n(Control) Cycle: [3/4 - lr: 1.3328e-02] Error: 10.9508                                                                          \n(Control) Cycle: [4/4 - lr: 1.4095e-02] Error: 10.3681                                                                          \n[6/32] Best Error: (Dynamics) 0.134965/(Control) 10.368097\n\n(Dynamics) Cycle: [1/8 - lr: 7.1670e-02] Error: 0.132654                                                                        \n(Dynamics) Cycle: [2/8 - lr: 7.3471e-02] Error: 0.131947                                                                        \n(Dynamics) Cycle: [3/8 - lr: 7.5235e-02] Error: 0.128349                                                                        \n(Dynamics) Cycle: [4/8 - lr: 7.6958e-02] Error: 0.127554                                                                        \n(Dynamics) Cycle: [5/8 - lr: 7.8637e-02] Error: 0.123253                                                                        \n(Dynamics) Cycle: [6/8 - lr: 8.0270e-02] Error: 0.121095                                                                        \n(Dynamics) Cycle: [7/8 - lr: 8.1854e-02] Error: 0.118037                                                                        \n(Dynamics) Cycle: [8/8 - lr: 8.3386e-02] Error: 0.115236                                                                        \n(Control) Cycle: [1/4 - lr: 1.4833e-02] Error: 8.8873                                                                           \n(Control) Cycle: [2/4 - lr: 1.5539e-02] Error: 8.4264                                                                           \n(Control) Cycle: [3/4 - lr: 1.6208e-02] Error: 6.8011                                                                           \n(Control) Cycle: [4/4 - lr: 1.6834e-02] Error: 6.7792                                                                           \n[7/32] Best Error: (Dynamics) 0.115236/(Control) 6.779247\n\n(Dynamics) Cycle: [1/8 - lr: 8.4864e-02] Error: 0.110782                                                                        \n(Dynamics) Cycle: [2/8 - lr: 8.6286e-02] Error: 0.107762                                                                        \n(Dynamics) Cycle: [3/8 - lr: 8.7648e-02] Error: 0.103760                                                                        \n(Dynamics) Cycle: [4/8 - lr: 8.8948e-02] Error: 0.100746                                                                        \n(Dynamics) Cycle: [5/8 - lr: 9.0185e-02] Error: 0.094112                                                                        \n(Dynamics) Cycle: [6/8 - lr: 9.1357e-02] Error: 0.082775                                                                        \n(Dynamics) Cycle: [7/8 - lr: 9.2461e-02] Error: 0.068870                                                                        \n(Dynamics) Cycle: [8/8 - lr: 9.3495e-02] Error: 0.053022                                                                        \n(Control) Cycle: [1/4 - lr: 1.7413e-02] Error: 12.4582                                                                          \n(Control) Cycle: [2/4 - lr: 1.7942e-02] Error: 5.6166                                                                           \n(Control) Cycle: [3/4 - lr: 1.8416e-02] Error: 2.0667                                                                           \n(Control) Cycle: [4/4 - lr: 1.8834e-02] Error: 1.8554                                                                           \n[8/32] Best Error: (Dynamics) 0.053022/(Control) 1.855380\n\n(Dynamics) Cycle: [1/8 - lr: 9.4458e-02] Error: 0.047605                                                                        \n(Dynamics) Cycle: [2/8 - lr: 9.5348e-02] Error: 0.045129                                                                        \n(Dynamics) Cycle: [3/8 - lr: 9.6164e-02] Error: 0.044022                                                                        \n(Dynamics) Cycle: [4/8 - lr: 9.6904e-02] Error: 0.041730                                                                        \n(Dynamics) Cycle: [5/8 - lr: 9.7567e-02] Error: 0.040621                                                                        \n(Dynamics) Cycle: [6/8 - lr: 9.8152e-02] Error: 0.037731                                                                        \n(Dynamics) Cycle: [7/8 - lr: 9.8658e-02] Error: 0.036201                                                                        \n(Dynamics) Cycle: [8/8 - lr: 9.9085e-02] Error: 0.034634                                                                        \n(Control) Cycle: [1/4 - lr: 1.9190e-02] Error: 8.4843                                                                           \n(Control) Cycle: [2/4 - lr: 1.9485e-02] Error: 3.5692                                                                           \n(Control) Cycle: [3/4 - lr: 1.9714e-02] Error: 3.6376                                                                           \n(Control) Cycle: [4/4 - lr: 1.9877e-02] Error: 1.6553                                                                           \n[9/32] Best Error: (Dynamics) 0.034634/(Control) 1.655267\n\n(Dynamics) Cycle: [1/8 - lr: 9.9430e-02] Error: 0.033063                                                                        \n(Dynamics) Cycle: [2/8 - lr: 9.9695e-02] Error: 0.030263                                                                        \n(Dynamics) Cycle: [3/8 - lr: 9.9877e-02] Error: 0.031640                                                                        \n(Dynamics) Cycle: [4/8 - lr: 9.9978e-02] Error: 0.029122                                                                        \n(Dynamics) Cycle: [5/8 - lr: 9.9997e-02] Error: 0.025567                                                                        \n(Dynamics) Cycle: [6/8 - lr: 9.9934e-02] Error: 0.024702                                                                        \n(Dynamics) Cycle: [7/8 - lr: 9.9788e-02] Error: 0.021571                                                                        \n(Dynamics) Cycle: [8/8 - lr: 9.9561e-02] Error: 0.021457                                                                        \n(Control) Cycle: [1/4 - lr: 1.9972e-02] Error: 2.0148                                                                           \n(Control) Cycle: [2/4 - lr: 2.0000e-02] Error: 0.7153                                                                           \n(Control) Cycle: [3/4 - lr: 1.9993e-02] Error: 0.4305                                                                           \n(Control) Cycle: [4/4 - lr: 1.9973e-02] Error: 0.3249                                                                           \n[10/32] Best Error: (Dynamics) 0.021457/(Control) 0.324926\n\n(Dynamics) Cycle: [1/8 - lr: 9.9253e-02] Error: 0.018149                                                                        \n(Dynamics) Cycle: [2/8 - lr: 9.8863e-02] Error: 0.020014                                                                        \n(Dynamics) Cycle: [3/8 - lr: 9.8393e-02] Error: 0.016999                                                                        \n(Dynamics) Cycle: [4/8 - lr: 9.7844e-02] Error: 0.015486                                                                        \n(Dynamics) Cycle: [5/8 - lr: 9.7217e-02] Error: 0.012047                                                                        \n(Dynamics) Cycle: [6/8 - lr: 9.6512e-02] Error: 0.014024                                                                        \n(Dynamics) Cycle: [7/8 - lr: 9.5730e-02] Error: 0.009492                                                                        \n(Dynamics) Cycle: [8/8 - lr: 9.4874e-02] Error: 0.008979                                                                        \n(Control) Cycle: [1/4 - lr: 1.9941e-02] Error: 0.4691                                                                           \n(Control) Cycle: [2/4 - lr: 1.9897e-02] Error: 0.5185                                                                           \n(Control) Cycle: [3/4 - lr: 1.9841e-02] Error: 0.4165                                                                           \n(Control) Cycle: [4/4 - lr: 1.9772e-02] Error: 0.3335                                                                           \n[11/32] Best Error: (Dynamics) 0.008979/(Control) 0.324926\n\n(Dynamics) Cycle: [1/8 - lr: 9.3944e-02] Error: 0.007531                                                                        \n(Dynamics) Cycle: [2/8 - lr: 9.2942e-02] Error: 0.008360                                                                        \n(Dynamics) Cycle: [3/8 - lr: 9.1870e-02] Error: 0.007104                                                                        \n(Dynamics) Cycle: [4/8 - lr: 9.0729e-02] Error: 0.004836                                                                        \n(Dynamics) Cycle: [5/8 - lr: 8.9522e-02] Error: 0.008515                                                                        \n(Dynamics) Cycle: [6/8 - lr: 8.8250e-02] Error: 0.008518                                                                        \n(Dynamics) Cycle: [7/8 - lr: 8.6916e-02] Error: 0.008236                                                                        \n(Dynamics) Cycle: [8/8 - lr: 8.5521e-02] Error: 0.004738                                                                        \n(Control) Cycle: [1/4 - lr: 1.9692e-02] Error: 0.1379                                                                           \n(Control) Cycle: [2/4 - lr: 1.9599e-02] Error: 0.1403                                                                           \n(Control) Cycle: [3/4 - lr: 1.9495e-02] Error: 0.1123                                                                           \n(Control) Cycle: [4/4 - lr: 1.9379e-02] Error: 0.1116                                                                           \n[12/32] Best Error: (Dynamics) 0.004738/(Control) 0.111606\n\n(Dynamics) Cycle: [1/8 - lr: 8.4069e-02] Error: 0.010618                                                                        \n(Dynamics) Cycle: [2/8 - lr: 8.2561e-02] Error: 0.007935                                                                        \n(Dynamics) Cycle: [3/8 - lr: 8.1001e-02] Error: 0.006500                                                                        \n(Dynamics) Cycle: [4/8 - lr: 7.9390e-02] Error: 0.006584                                                                        \n(Dynamics) Cycle: [5/8 - lr: 7.7731e-02] Error: 0.002766                                                                        \n(Dynamics) Cycle: [6/8 - lr: 7.6028e-02] Error: 0.004383                                                                        \n(Dynamics) Cycle: [7/8 - lr: 7.4282e-02] Error: 0.003150                                                                        \n(Dynamics) Cycle: [8/8 - lr: 7.2498e-02] Error: 0.005326                                                                        \n(Control) Cycle: [1/4 - lr: 1.9252e-02] Error: 0.1311                                                                           \n(Control) Cycle: [2/4 - lr: 1.9113e-02] Error: 0.0898                                                                           \n(Control) Cycle: [3/4 - lr: 1.8964e-02] Error: 0.0637                                                                           \n(Control) Cycle: [4/4 - lr: 1.8803e-02] Error: 0.0758                                                                           \n[13/32] Best Error: (Dynamics) 0.002766/(Control) 0.063721\n\n(Dynamics) Cycle: [1/8 - lr: 7.0678e-02] Error: 0.004473                                                                        \n(Dynamics) Cycle: [2/8 - lr: 6.8824e-02] Error: 0.007433                                                                        \n(Dynamics) Cycle: [3/8 - lr: 6.6941e-02] Error: 0.003469                                                                        \n(Dynamics) Cycle: [4/8 - lr: 6.5031e-02] Error: 0.003518                                                                        \n(Dynamics) Cycle: [5/8 - lr: 6.3098e-02] Error: 0.004494                                                                        \n(Dynamics) Cycle: [6/8 - lr: 6.1144e-02] Error: 0.004412                                                                        \n(Dynamics) Cycle: [7/8 - lr: 5.9173e-02] Error: 0.007177                                                                        \n(Dynamics) Cycle: [8/8 - lr: 5.7189e-02] Error: 0.003617                                                                        \n(Control) Cycle: [1/4 - lr: 1.8631e-02] Error: 0.0641                                                                           \n(Control) Cycle: [2/4 - lr: 1.8449e-02] Error: 0.0616                                                                           \n(Control) Cycle: [3/4 - lr: 1.8256e-02] Error: 0.0469                                                                           \n(Control) Cycle: [4/4 - lr: 1.8053e-02] Error: 0.0617                                                                           \n[14/32] Best Error: (Dynamics) 0.002766/(Control) 0.046933\n\n(Dynamics) Cycle: [1/8 - lr: 5.5194e-02] Error: 0.003768                                                                        \n(Dynamics) Cycle: [2/8 - lr: 5.3192e-02] Error: 0.005125                                                                        \n(Dynamics) Cycle: [3/8 - lr: 5.1186e-02] Error: 0.004564                                                                        \n(Dynamics) Cycle: [4/8 - lr: 4.9180e-02] Error: 0.007029                                                                        \n(Dynamics) Cycle: [5/8 - lr: 4.7178e-02] Error: 0.009791                                                                        \n(Dynamics) Cycle: [6/8 - lr: 4.5181e-02] Error: 0.006955                                                                        \n(Dynamics) Cycle: [7/8 - lr: 4.3195e-02] Error: 0.003950                                                                        \n(Dynamics) Cycle: [8/8 - lr: 4.1221e-02] Error: 0.005212                                                                        \n(Control) Cycle: [1/4 - lr: 1.7840e-02] Error: 0.0419                                                                           \n(Control) Cycle: [2/4 - lr: 1.7618e-02] Error: 0.0482                                                                           \n(Control) Cycle: [3/4 - lr: 1.7386e-02] Error: 0.0378                                                                           \n(Control) Cycle: [4/4 - lr: 1.7145e-02] Error: 0.0391                                                                           \n[15/32] Best Error: (Dynamics) 0.002766/(Control) 0.037761\n\n(Dynamics) Cycle: [1/8 - lr: 3.9264e-02] Error: 0.002857                                                                        \n(Dynamics) Cycle: [2/8 - lr: 3.7326e-02] Error: 0.002244                                                                        \n(Dynamics) Cycle: [3/8 - lr: 3.5412e-02] Error: 0.002740                                                                        \n(Dynamics) Cycle: [4/8 - lr: 3.3523e-02] Error: 0.001380                                                                        \n(Dynamics) Cycle: [5/8 - lr: 3.1664e-02] Error: 0.002367                                                                        \n(Dynamics) Cycle: [6/8 - lr: 2.9838e-02] Error: 0.002010                                                                        \n(Dynamics) Cycle: [7/8 - lr: 2.8046e-02] Error: 0.002192                                                                        \n(Dynamics) Cycle: [8/8 - lr: 2.6293e-02] Error: 0.002703                                                                        \n(Control) Cycle: [1/4 - lr: 1.6895e-02] Error: 0.0339                                                                           \n(Control) Cycle: [2/4 - lr: 1.6637e-02] Error: 0.0314                                                                           \n(Control) Cycle: [3/4 - lr: 1.6371e-02] Error: 0.0317                                                                           \n(Control) Cycle: [4/4 - lr: 1.6097e-02] Error: 0.0290                                                                           \n[16/32] Best Error: (Dynamics) 0.001380/(Control) 0.029003\n\n(Dynamics) Cycle: [1/8 - lr: 2.4582e-02] Error: 0.002062                                                                        \n(Dynamics) Cycle: [2/8 - lr: 2.2915e-02] Error: 0.001393                                                                        \n(Dynamics) Cycle: [3/8 - lr: 2.1295e-02] Error: 0.002654                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.9725e-02] Error: 0.001584                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.8207e-02] Error: 0.001202                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.6744e-02] Error: 0.001402                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.5339e-02] Error: 0.001286                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.3993e-02] Error: 0.000670                                                                        \n(Control) Cycle: [1/4 - lr: 1.5815e-02] Error: 0.0395                                                                           \n(Control) Cycle: [2/4 - lr: 1.5527e-02] Error: 0.0269                                                                           \n(Control) Cycle: [3/4 - lr: 1.5231e-02] Error: 0.0310                                                                           \n(Control) Cycle: [4/4 - lr: 1.4929e-02] Error: 0.0281                                                                           \n[17/32] Best Error: (Dynamics) 0.000670/(Control) 0.026862\n\n(Dynamics) Cycle: [1/8 - lr: 1.2709e-02] Error: 0.000749                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.1490e-02] Error: 0.001233                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.0337e-02] Error: 0.000934                                                                        \n(Dynamics) Cycle: [4/8 - lr: 9.2517e-03] Error: 0.000848                                                                        \n(Dynamics) Cycle: [5/8 - lr: 8.2367e-03] Error: 0.000517                                                                        \n(Dynamics) Cycle: [6/8 - lr: 7.2933e-03] Error: 0.000334                                                                        \n(Dynamics) Cycle: [7/8 - lr: 6.4232e-03] Error: 0.000463                                                                        \n(Dynamics) Cycle: [8/8 - lr: 5.6278e-03] Error: 0.000418                                                                        \n(Control) Cycle: [1/4 - lr: 1.4621e-02] Error: 0.0280                                                                           \n(Control) Cycle: [2/4 - lr: 1.4307e-02] Error: 0.0245                                                                           \n(Control) Cycle: [3/4 - lr: 1.3988e-02] Error: 0.0247                                                                           \n(Control) Cycle: [4/4 - lr: 1.3664e-02] Error: 0.0232                                                                           \n[18/32] Best Error: (Dynamics) 0.000334/(Control) 0.023191\n\n(Dynamics) Cycle: [1/8 - lr: 4.9084e-03] Error: 0.000251                                                                        \n(Dynamics) Cycle: [2/8 - lr: 4.2663e-03] Error: 0.000312                                                                        \n(Dynamics) Cycle: [3/8 - lr: 3.7026e-03] Error: 0.000282                                                                        \n(Dynamics) Cycle: [4/8 - lr: 3.2181e-03] Error: 0.000175                                                                        \n(Dynamics) Cycle: [5/8 - lr: 2.8137e-03] Error: 0.000131                                                                        \n(Dynamics) Cycle: [6/8 - lr: 2.4901e-03] Error: 0.000142                                                                        \n(Dynamics) Cycle: [7/8 - lr: 2.2478e-03] Error: 0.000099                                                                        \n(Dynamics) Cycle: [8/8 - lr: 2.0872e-03] Error: 0.000172                                                                        \n(Control) Cycle: [1/4 - lr: 1.3336e-02] Error: 0.0223                                                                           \n(Control) Cycle: [2/4 - lr: 1.3003e-02] Error: 0.0247                                                                           \n(Control) Cycle: [3/4 - lr: 1.2667e-02] Error: 0.0209                                                                           \n(Control) Cycle: [4/4 - lr: 1.2328e-02] Error: 0.0217                                                                           \n[19/32] Best Error: (Dynamics) 0.000099/(Control) 0.020869\n\n(Dynamics) Cycle: [1/8 - lr: 2.0086e-03] Error: 0.000175                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.9999e-03] Error: 0.000222                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.9989e-03] Error: 0.000174                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.9970e-03] Error: 0.000165                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.9941e-03] Error: 0.000137                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.9903e-03] Error: 0.000148                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.9856e-03] Error: 0.000133                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.9800e-03] Error: 0.000121                                                                        \n(Control) Cycle: [1/4 - lr: 1.1985e-02] Error: 0.0258                                                                           \n(Control) Cycle: [2/4 - lr: 1.1640e-02] Error: 0.0208                                                                           \n(Control) Cycle: [3/4 - lr: 1.1294e-02] Error: 0.0198                                                                           \n(Control) Cycle: [4/4 - lr: 1.0945e-02] Error: 0.0197                                                                           \n[20/32] Best Error: (Dynamics) 0.000099/(Control) 0.019681\n\n(Dynamics) Cycle: [1/8 - lr: 1.9734e-03] Error: 0.000144                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.9659e-03] Error: 0.000083                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.9575e-03] Error: 0.000082                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.9482e-03] Error: 0.000136                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.9380e-03] Error: 0.000115                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.9270e-03] Error: 0.000073                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.9150e-03] Error: 0.000117                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.9023e-03] Error: 0.000139                                                                        \n(Control) Cycle: [1/4 - lr: 1.0596e-02] Error: 0.0190                                                                           \n(Control) Cycle: [2/4 - lr: 1.0245e-02] Error: 0.0196                                                                           \n(Control) Cycle: [3/4 - lr: 9.8948e-03] Error: 0.0190                                                                           \n(Control) Cycle: [4/4 - lr: 9.5444e-03] Error: 0.0198                                                                           \n[21/32] Best Error: (Dynamics) 0.000073/(Control) 0.018959\n\n(Dynamics) Cycle: [1/8 - lr: 1.8886e-03] Error: 0.000135                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.8741e-03] Error: 0.000176                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.8588e-03] Error: 0.000112                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.8427e-03] Error: 0.000150                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.8258e-03] Error: 0.000199                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.8082e-03] Error: 0.000216                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.7897e-03] Error: 0.000183                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.7706e-03] Error: 0.000162                                                                        \n(Control) Cycle: [1/4 - lr: 9.1944e-03] Error: 0.0180                                                                           \n(Control) Cycle: [2/4 - lr: 8.8455e-03] Error: 0.0182                                                                           \n(Control) Cycle: [3/4 - lr: 8.4980e-03] Error: 0.0177                                                                           \n(Control) Cycle: [4/4 - lr: 8.1524e-03] Error: 0.0199                                                                           \n[22/32] Best Error: (Dynamics) 0.000073/(Control) 0.017730\n\n(Dynamics) Cycle: [1/8 - lr: 1.7507e-03] Error: 0.000166                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.7301e-03] Error: 0.000087                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.7088e-03] Error: 0.000127                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.6868e-03] Error: 0.000198                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.6642e-03] Error: 0.000187                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.6410e-03] Error: 0.000125                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.6171e-03] Error: 0.000097                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.5927e-03] Error: 0.000066                                                                        \n(Control) Cycle: [1/4 - lr: 7.8090e-03] Error: 0.0176                                                                           \n(Control) Cycle: [2/4 - lr: 7.4683e-03] Error: 0.0172                                                                           \n(Control) Cycle: [3/4 - lr: 7.1307e-03] Error: 0.0170                                                                           \n(Control) Cycle: [4/4 - lr: 6.7967e-03] Error: 0.0166                                                                           \n[23/32] Best Error: (Dynamics) 0.000066/(Control) 0.016637\n\n(Dynamics) Cycle: [1/8 - lr: 1.5678e-03] Error: 0.000089                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.5423e-03] Error: 0.000112                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.5163e-03] Error: 0.000087                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.4898e-03] Error: 0.000122                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.4628e-03] Error: 0.000119                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.4354e-03] Error: 0.000155                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.4076e-03] Error: 0.000264                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.3795e-03] Error: 0.000272                                                                        \n(Control) Cycle: [1/4 - lr: 6.4666e-03] Error: 0.0164                                                                           \n(Control) Cycle: [2/4 - lr: 6.1408e-03] Error: 0.0163                                                                           \n(Control) Cycle: [3/4 - lr: 5.8198e-03] Error: 0.0159                                                                           \n(Control) Cycle: [4/4 - lr: 5.5039e-03] Error: 0.0165                                                                           \n[24/32] Best Error: (Dynamics) 0.000066/(Control) 0.015885\n\n(Dynamics) Cycle: [1/8 - lr: 1.3509e-03] Error: 0.000164                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.3221e-03] Error: 0.000148                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.2929e-03] Error: 0.000091                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.2634e-03] Error: 0.000084                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.2337e-03] Error: 0.000082                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.2038e-03] Error: 0.000088                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.1737e-03] Error: 0.000101                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.1435e-03] Error: 0.000102                                                                        \n(Control) Cycle: [1/4 - lr: 5.1935e-03] Error: 0.0170                                                                           \n(Control) Cycle: [2/4 - lr: 4.8891e-03] Error: 0.0156                                                                           \n(Control) Cycle: [3/4 - lr: 4.5909e-03] Error: 0.0162                                                                           \n(Control) Cycle: [4/4 - lr: 4.2994e-03] Error: 0.0154                                                                           \n[25/32] Best Error: (Dynamics) 0.000066/(Control) 0.015429\n\n(Dynamics) Cycle: [1/8 - lr: 1.1131e-03] Error: 0.000086                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.0826e-03] Error: 0.000066                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.0520e-03] Error: 0.000073                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.0213e-03] Error: 0.000059                                                                        \n(Dynamics) Cycle: [5/8 - lr: 9.9068e-04] Error: 0.000091                                                                        \n(Dynamics) Cycle: [6/8 - lr: 9.6004e-04] Error: 0.000054                                                                        \n(Dynamics) Cycle: [7/8 - lr: 9.2943e-04] Error: 0.000098                                                                        \n(Dynamics) Cycle: [8/8 - lr: 8.9889e-04] Error: 0.000094                                                                        \n(Control) Cycle: [1/4 - lr: 4.0149e-03] Error: 0.0154                                                                           \n(Control) Cycle: [2/4 - lr: 3.7377e-03] Error: 0.0156                                                                           \n(Control) Cycle: [3/4 - lr: 3.4683e-03] Error: 0.0157                                                                           \n(Control) Cycle: [4/4 - lr: 3.2069e-03] Error: 0.0153                                                                           \n[26/32] Best Error: (Dynamics) 0.000054/(Control) 0.015268\n\n(Dynamics) Cycle: [1/8 - lr: 8.6845e-04] Error: 0.000074                                                                        \n(Dynamics) Cycle: [2/8 - lr: 8.3813e-04] Error: 0.000093                                                                        \n(Dynamics) Cycle: [3/8 - lr: 8.0796e-04] Error: 0.000070                                                                        \n(Dynamics) Cycle: [4/8 - lr: 7.7797e-04] Error: 0.000067                                                                        \n(Dynamics) Cycle: [5/8 - lr: 7.4819e-04] Error: 0.000056                                                                        \n(Dynamics) Cycle: [6/8 - lr: 7.1864e-04] Error: 0.000070                                                                        \n(Dynamics) Cycle: [7/8 - lr: 6.8937e-04] Error: 0.000054                                                                        \n(Dynamics) Cycle: [8/8 - lr: 6.6038e-04] Error: 0.000037                                                                        \n(Control) Cycle: [1/4 - lr: 2.9538e-03] Error: 0.0151                                                                           \n(Control) Cycle: [2/4 - lr: 2.7094e-03] Error: 0.0151                                                                           \n(Control) Cycle: [3/4 - lr: 2.4739e-03] Error: 0.0152                                                                           \n(Control) Cycle: [4/4 - lr: 2.2477e-03] Error: 0.0152                                                                           \n[27/32] Best Error: (Dynamics) 0.000037/(Control) 0.015056\n\n(Dynamics) Cycle: [1/8 - lr: 6.3171e-04] Error: 0.000022                                                                        \n(Dynamics) Cycle: [2/8 - lr: 6.0339e-04] Error: 0.000022                                                                        \n(Dynamics) Cycle: [3/8 - lr: 5.7544e-04] Error: 0.000016                                                                        \n(Dynamics) Cycle: [4/8 - lr: 5.4789e-04] Error: 0.000031                                                                        \n(Dynamics) Cycle: [5/8 - lr: 5.2077e-04] Error: 0.000038                                                                        \n(Dynamics) Cycle: [6/8 - lr: 4.9409e-04] Error: 0.000030                                                                        \n(Dynamics) Cycle: [7/8 - lr: 4.6790e-04] Error: 0.000015                                                                        \n(Dynamics) Cycle: [8/8 - lr: 4.4220e-04] Error: 0.000026                                                                        \n(Control) Cycle: [1/4 - lr: 2.0310e-03] Error: 0.0153                                                                           \n(Control) Cycle: [2/4 - lr: 1.8242e-03] Error: 0.0149                                                                           \n(Control) Cycle: [3/4 - lr: 1.6273e-03] Error: 0.0147                                                                           \n(Control) Cycle: [4/4 - lr: 1.4408e-03] Error: 0.0147                                                                           \n[28/32] Best Error: (Dynamics) 0.000015/(Control) 0.014693\n\n(Dynamics) Cycle: [1/8 - lr: 4.1702e-04] Error: 0.000038                                                                        \n(Dynamics) Cycle: [2/8 - lr: 3.9240e-04] Error: 0.000026                                                                        \n(Dynamics) Cycle: [3/8 - lr: 3.6834e-04] Error: 0.000017                                                                        \n(Dynamics) Cycle: [4/8 - lr: 3.4488e-04] Error: 0.000037                                                                        \n(Dynamics) Cycle: [5/8 - lr: 3.2204e-04] Error: 0.000041                                                                        \n(Dynamics) Cycle: [6/8 - lr: 2.9983e-04] Error: 0.000037                                                                        \n(Dynamics) Cycle: [7/8 - lr: 2.7828e-04] Error: 0.000036                                                                        \n(Dynamics) Cycle: [8/8 - lr: 2.5741e-04] Error: 0.000024                                                                        \n(Control) Cycle: [1/4 - lr: 1.2648e-03] Error: 0.0147                                                                           \n(Control) Cycle: [2/4 - lr: 1.0995e-03] Error: 0.0147                                                                           \n(Control) Cycle: [3/4 - lr: 9.4518e-04] Error: 0.0147                                                                           \n(Control) Cycle: [4/4 - lr: 8.0198e-04] Error: 0.0146                                                                           \n[29/32] Best Error: (Dynamics) 0.000015/(Control) 0.014644\n\n(Dynamics) Cycle: [1/8 - lr: 2.3724e-04] Error: 0.000027                                                                        \n(Dynamics) Cycle: [2/8 - lr: 2.1778e-04] Error: 0.000021                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.9906e-04] Error: 0.000010                                                                        \n(Dynamics) Cycle: [4/8 - lr: 1.8109e-04] Error: 0.000011                                                                        \n(Dynamics) Cycle: [5/8 - lr: 1.6389e-04] Error: 0.000009                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.4748e-04] Error: 0.000010                                                                        \n(Dynamics) Cycle: [7/8 - lr: 1.3187e-04] Error: 0.000005                                                                        \n(Dynamics) Cycle: [8/8 - lr: 1.1708e-04] Error: 0.000004                                                                        \n(Control) Cycle: [1/4 - lr: 6.7008e-04] Error: 0.0146                                                                           \n(Control) Cycle: [2/4 - lr: 5.4965e-04] Error: 0.0147                                                                           \n(Control) Cycle: [3/4 - lr: 4.4084e-04] Error: 0.0146                                                                           \n(Control) Cycle: [4/4 - lr: 3.4378e-04] Error: 0.0146                                                                           \n[30/32] Best Error: (Dynamics) 0.000004/(Control) 0.014585\n\n(Dynamics) Cycle: [1/8 - lr: 1.0311e-04] Error: 0.000006                                                                        \n(Dynamics) Cycle: [2/8 - lr: 8.9989e-05] Error: 0.000007                                                                        \n(Dynamics) Cycle: [3/8 - lr: 7.7723e-05] Error: 0.000005                                                                        \n(Dynamics) Cycle: [4/8 - lr: 6.6323e-05] Error: 0.000006                                                                        \n(Dynamics) Cycle: [5/8 - lr: 5.5801e-05] Error: 0.000007                                                                        \n(Dynamics) Cycle: [6/8 - lr: 4.6166e-05] Error: 0.000008                                                                        \n(Dynamics) Cycle: [7/8 - lr: 3.7428e-05] Error: 0.000005                                                                        \n(Dynamics) Cycle: [8/8 - lr: 2.9595e-05] Error: 0.000003                                                                        \n(Control) Cycle: [1/4 - lr: 2.5859e-04] Error: 0.0146                                                                           \n(Control) Cycle: [2/4 - lr: 1.8538e-04] Error: 0.0146                                                                           \n(Control) Cycle: [3/4 - lr: 1.2423e-04] Error: 0.0145                                                                           \n(Control) Cycle: [4/4 - lr: 7.5215e-05] Error: 0.0145                                                                           \n[31/32] Best Error: (Dynamics) 0.000003/(Control) 0.014533\n\n(Dynamics) Cycle: [1/8 - lr: 2.2674e-05] Error: 0.000002                                                                        \n(Dynamics) Cycle: [2/8 - lr: 1.6671e-05] Error: 0.000002                                                                        \n(Dynamics) Cycle: [3/8 - lr: 1.1593e-05] Error: 0.000001                                                                        \n(Dynamics) Cycle: [4/8 - lr: 7.4438e-06] Error: 0.000000                                                                        \n(Dynamics) Cycle: [5/8 - lr: 4.2275e-06] Error: 0.000000                                                                        \n(Dynamics) Cycle: [6/8 - lr: 1.9473e-06] Error: 0.000000                                                                        \n(Dynamics) Cycle: [7/8 - lr: 6.0516e-07] Error: 0.000000                                                                        \n(Dynamics) Cycle: [8/8 - lr: 2.0240e-07] Error: 0.000000                                                                        \n(Control) Cycle: [1/4 - lr: 3.8403e-05] Error: 0.0145                                                                           \n(Control) Cycle: [2/4 - lr: 1.3837e-05] Error: 0.0145                                                                           \n(Control) Cycle: [3/4 - lr: 1.5467e-06] Error: 0.0145                                                                           \n(Control) Cycle: [4/4 - lr: 1.5467e-06] Error: 0.0145                                                                           \n[32/32] Best Error: (Dynamics) 0.000000/(Control) 0.014524\n\n</pre> In\u00a0[17]: Copied! <pre>dynamics_network.load_state_dict(best_dynamics_parameters)\ncontrol_network.load_state_dict(best_control_parameters)\n\ndynamics_network.eval()\ncontrol_network.eval()\n\nlearned_matrix = dynamics_network.state_dict()['internal_net.0.weight']\nlearned_bias = dynamics_network.state_dict()['internal_net.0.bias']\n\nprint(f\"Best dynamics matrix: {learned_matrix}, mean absolute error: {torch.mean(torch.abs(ideal_matrix - learned_matrix)).item():.6f}\")\nprint(f\"Best dynamics bias:   {learned_bias}, mean absolute error: {torch.mean(torch.abs(ideal_bias - learned_bias)).item():.6f}\")\n</pre> dynamics_network.load_state_dict(best_dynamics_parameters) control_network.load_state_dict(best_control_parameters)  dynamics_network.eval() control_network.eval()  learned_matrix = dynamics_network.state_dict()['internal_net.0.weight'] learned_bias = dynamics_network.state_dict()['internal_net.0.bias']  print(f\"Best dynamics matrix: {learned_matrix}, mean absolute error: {torch.mean(torch.abs(ideal_matrix - learned_matrix)).item():.6f}\") print(f\"Best dynamics bias:   {learned_bias}, mean absolute error: {torch.mean(torch.abs(ideal_bias - learned_bias)).item():.6f}\") <pre>Best dynamics matrix: tensor([[ 1.0136e-07,  1.0000e+00],\n        [-1.0000e+00, -5.0000e-01]]), mean absolute error: 0.000000\nBest dynamics bias:   tensor([ 4.7776e-08, -4.1761e-08]), mean absolute error: 0.000000\n</pre> <p>We see that the matrix closely matches the expected dynamics matrix, and furthermore, the bias is zero as expected for this system.</p> <p>Taking a look at the trajectory below, we see that this matrix translates to a near-perfect match to the reference trajectory. We further see that the control network has learned the appropriate control function to reach the target state on the learned dynamics.</p> In\u00a0[18]: Copied! <pre>_, _, learned_dynamics_states, learned_dynamics_times, _ = current_integrator.apply(learned_dynamics_rhs, initial_state, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol})\n\nig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK7(8) - SHA Position Ref.\")\nfig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK7(8) - SHA Velocity Ref.\")\n\n_ = neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(learned_dynamics_states, learned_dynamics_times)], axes=axes_ref_position, method_label=\"RK7(8) - SHA Position Opt.\")\nfig_ref_position.suptitle(\"Position\")\n_ = neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(learned_dynamics_states, learned_dynamics_times)], axes=axes_ref_velocity, method_label=\"RK7(8) - SHA Velocity Opt.\")\nfig_ref_velocity.suptitle(\"Velocity\")\n\n_, _, integrated_test_states, integrated_test_times, _ = current_integrator.apply(controlled_dynamics_rhs, test_state_dataset, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol})\n\nprint(\"Training results on test data:\")\nprint(f\"\\tMean position at t={final_time.item():.2f}s: {integrated_test_states[-1, ..., 0].mean().item():.4e} \u00b1 {integrated_test_states[-1, ..., 0].std().item():.4e}\")\nprint(f\"\\tMean position error at t={final_time.item():.2f}s: {(target_state - integrated_test_states)[-1, ..., 0].mean().item():.4e} \u00b1 {(target_state - integrated_test_states)[-1, ..., 0].std().item():.4e}\")\nprint()\nprint(f\"\\tMean velocity at t={final_time.item():.2f}s: {integrated_test_states[-1, ..., 1].mean().item():.4e} \u00b1 {integrated_test_states[-1, ..., 1].std().item():.4e}\")\nprint(f\"\\tMean velocity error at t={final_time.item():.2f}s: {(target_state - integrated_test_states)[-1, ..., 1].mean().item():.4e} \u00b1 {(target_state - integrated_test_states)[-1, ..., 1].std().item():.4e}\")\n\nneuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(integrated_test_states, integrated_test_times)], method_label=None)\nneuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(integrated_test_states, integrated_test_times)], method_label=None)\n\nfig_ref_position_closeup, axes_ref_position_closeup = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None)\nfig_ref_velocity_closeup, axes_ref_velocity_closeup = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None)\n\n_ = neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_position_closeup, method_label=None)\nfig_ref_position_closeup.suptitle(\"Close-Up of Position\")\naxes_ref_position_closeup[0].set_xlim(final_time.item() - 5.0, final_time.item())\naxes_ref_position_closeup[0].set_ylim(target_state[0].item() - 1.0, target_state[0].item() + 1.0)\n\n_ = neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_velocity_closeup, method_label=None)\nfig_ref_velocity_closeup.suptitle(\"Close-Up of Velocity\")\naxes_ref_velocity_closeup[0].set_xlim(final_time.item() - 5.0, final_time.item())\naxes_ref_velocity_closeup[0].set_ylim(target_state[1].item() - 1.0, target_state[1].item() + 1.0)\n</pre> _, _, learned_dynamics_states, learned_dynamics_times, _ = current_integrator.apply(learned_dynamics_rhs, initial_state, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol})  ig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK7(8) - SHA Position Ref.\") fig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK7(8) - SHA Velocity Ref.\")  _ = neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(learned_dynamics_states, learned_dynamics_times)], axes=axes_ref_position, method_label=\"RK7(8) - SHA Position Opt.\") fig_ref_position.suptitle(\"Position\") _ = neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(learned_dynamics_states, learned_dynamics_times)], axes=axes_ref_velocity, method_label=\"RK7(8) - SHA Velocity Opt.\") fig_ref_velocity.suptitle(\"Velocity\")  _, _, integrated_test_states, integrated_test_times, _ = current_integrator.apply(controlled_dynamics_rhs, test_state_dataset, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol})  print(\"Training results on test data:\") print(f\"\\tMean position at t={final_time.item():.2f}s: {integrated_test_states[-1, ..., 0].mean().item():.4e} \u00b1 {integrated_test_states[-1, ..., 0].std().item():.4e}\") print(f\"\\tMean position error at t={final_time.item():.2f}s: {(target_state - integrated_test_states)[-1, ..., 0].mean().item():.4e} \u00b1 {(target_state - integrated_test_states)[-1, ..., 0].std().item():.4e}\") print() print(f\"\\tMean velocity at t={final_time.item():.2f}s: {integrated_test_states[-1, ..., 1].mean().item():.4e} \u00b1 {integrated_test_states[-1, ..., 1].std().item():.4e}\") print(f\"\\tMean velocity error at t={final_time.item():.2f}s: {(target_state - integrated_test_states)[-1, ..., 1].mean().item():.4e} \u00b1 {(target_state - integrated_test_states)[-1, ..., 1].std().item():.4e}\")  neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(integrated_test_states, integrated_test_times)], method_label=None) neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(integrated_test_states, integrated_test_times)], method_label=None)  fig_ref_position_closeup, axes_ref_position_closeup = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None) fig_ref_velocity_closeup, axes_ref_velocity_closeup = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(dha_states_optimised, dha_times_optimised)], method_label=None)  _ = neuralode.plot.trajectory.plot_trajectory([(i[...,0], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_position_closeup, method_label=None) fig_ref_position_closeup.suptitle(\"Close-Up of Position\") axes_ref_position_closeup[0].set_xlim(final_time.item() - 5.0, final_time.item()) axes_ref_position_closeup[0].set_ylim(target_state[0].item() - 1.0, target_state[0].item() + 1.0)  _ = neuralode.plot.trajectory.plot_trajectory([(i[...,1], j) for i, j in zip(integrated_test_states, integrated_test_times)], axes=axes_ref_velocity_closeup, method_label=None) fig_ref_velocity_closeup.suptitle(\"Close-Up of Velocity\") axes_ref_velocity_closeup[0].set_xlim(final_time.item() - 5.0, final_time.item()) axes_ref_velocity_closeup[0].set_ylim(target_state[1].item() - 1.0, target_state[1].item() + 1.0) <pre>Training results on test data:\n\tMean position at t=25.00s: 9.4453e-01 \u00b1 9.8263e-12\n\tMean position error at t=25.00s: 5.5466e-02 \u00b1 9.8263e-12\n\n\tMean velocity at t=25.00s: -1.4351e-07 \u00b1 6.6683e-12\n\tMean velocity error at t=25.00s: 1.4351e-07 \u00b1 6.6683e-12\n</pre> <pre>C:\\Users\\ekin4\\PycharmProjects\\ReCoDE-NeuralODEs\\neuralode\\plot\\trajectory.py:79: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n  ax.legend()\n</pre> Out[18]: <pre>(-1.0, 1.0)</pre>"},{"location":"04-driven-harmonic-oscillator/#the-driven-harmonic-oscillator","title":"The Driven Harmonic Oscillator\u00b6","text":"<p>In this notebook, we will go through implementing the driven harmonic system for which we'll train a neural network to learn to control the oscillator state.</p> <p>We will train a separate network to learn the system dynamics as well.</p> <p>A part of this exercise will also show you how to make animations in matplotlib!</p>"},{"location":"04-driven-harmonic-oscillator/#learning-to-fix-the-oscillator-state","title":"Learning to Fix the Oscillator State\u00b6","text":"<p>Previously we learned the dynamics of this oscillator directly, here we will incorporate control of the oscillator. We'll be using the forced harmonic oscillator model where we not only have movement of the oscillator with the dynamics that come from the simple harmonic oscillator but also a driving force that does work on the system.</p> <p>Mathematically, this is equivalent to</p> <p>$$ \\begin{bmatrix}     x^{(1)} \\\\     v^{(1)} \\end{bmatrix} =  \\mathbf{A} \\begin{bmatrix}     x \\\\     v \\end{bmatrix} +  \\begin{bmatrix}     0 \\\\     F \\end{bmatrix} $$</p> <p>where</p> <p>$$ \\mathbf{A} = \\begin{bmatrix}     0 &amp; 1 \\\\     -\\omega^2 &amp; -2\\zeta\\omega \\end{bmatrix} $$</p> <p>If we were to learn both the dynamics and the driving force together, we'd need both driven and undriven oscillator states as the two networks could learn some kind of combined dynamics that integrates the driving force into the RHS. In that fashion, we'd no longer be able to distinguish the dynamics of the oscillator itself from the driving force.</p>"},{"location":"04-driven-harmonic-oscillator/#dataset-and-loss-function-configuration","title":"Dataset and Loss Function Configuration\u00b6","text":"<p>We create a dataset of 8 initial states which cover a range of positions and velocities, and use a loss function that penalises both the deviation from the target state at the end of the integration, but also cumulatively over the whole trajectory.</p>"},{"location":"04-driven-harmonic-oscillator/#network-specification","title":"Network Specification\u00b6","text":"<p>The network is a simple feed-forward network composed of dense layers and non-linearities, outputting a single control value, the acceleration of the oscillator.</p>"},{"location":"04-driven-harmonic-oscillator/#training-configuration","title":"Training Configuration\u00b6","text":"<p>We train for 128 with a batch size of 4 using the Adam optimiser and OneCycleLR learning rate schedule for accelerated convergence.</p>"},{"location":"04-driven-harmonic-oscillator/#training-the-network","title":"Training the Network\u00b6","text":""},{"location":"04-driven-harmonic-oscillator/#testing-the-network-on-unseen-systems","title":"Testing the Network on Unseen Systems\u00b6","text":"<p>In order to test this controller, let's generate several cases that the network hasn't seen and integrate the system. Ideally, we should observe rapid damping of oscillations, akin to critical damping.</p>"},{"location":"04-driven-harmonic-oscillator/#co-learning-dynamics-and-control-of-an-oscillator","title":"Co-Learning Dynamics and Control of an Oscillator\u00b6","text":"<p>Loosely based on https://arxiv.org/html/2401.01836v1.</p> <p>Suppose we didn't know the dynamics of the system either; we had one reference trajectory and many initial states that we'd like to control. One method for solving this would be to have two networks that are trained in an alternating fashion. One network learns the dynamics, like we've seen before, and another network, like the one above, learns the control. We can posit that the same structure applies, the network can only apply forces, but not manipulate the velocity directly which should preserve the physicality of the system.</p>"},{"location":"04-driven-harmonic-oscillator/#generating-reference-trajectory-data","title":"Generating Reference Trajectory Data\u00b6","text":"<p>We take a damped oscillator as previously specified and simply take the points generated along its trajectory as our dataset. With a high-order integrator, this dataset could end up being too sparse.</p>"},{"location":"04-driven-harmonic-oscillator/#network-specification","title":"Network Specification\u00b6","text":"<p>We specify two networks: one dynamics network which is simply a single affine transformation (which we know fits the simple harmonic oscillator and can be easily analysed) and a control network, same as before.</p> <p>The pytorch manual seed is set in order to make the notebook more reproducible, and this seed has been found to converge, otherwise the network can sometimes learn the time-average dynamics which neglects the oscillations.</p>"},{"location":"04-driven-harmonic-oscillator/#training-configuration","title":"Training Configuration\u00b6","text":"<p>Ideally, the dynamics network converges before the control network so that the control network learns controls suitable for the system we're interested in.</p> <p>By running the training in cycles every epoch, we train the dynamics network 4 times more than the control network. These are tunable parameters, and can be tweaked to control the trade-off between convergence of the dynamics and the control network.</p>"},{"location":"04-driven-harmonic-oscillator/#training-the-networks","title":"Training the Networks\u00b6","text":""},{"location":"04-driven-harmonic-oscillator/#testing-the-network","title":"Testing the Network\u00b6","text":"<p>We first look at the dynamics matrix learned by the dynamics network.</p>"},{"location":"05-the-inverted-pendulum/","title":"The Inverted Pendulum (or the Cart-Pole System)","text":"In\u00a0[1]: Copied! <pre>import torch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n\nimport math\nimport copy\n\nfrom IPython.display import HTML\n\nimport neuralode\nimport warnings\nwarnings.simplefilter('ignore', RuntimeWarning)\n</pre> import torch import numpy as np import random import matplotlib.pyplot as plt  import math import copy  from IPython.display import HTML  import neuralode import warnings warnings.simplefilter('ignore', RuntimeWarning) In\u00a0[2]: Copied! <pre># For convenience, we define the default tensor device and dtype here\ntorch.set_default_device('cuda' if torch.cuda.is_available() else 'cpu')\n# In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. On CPUs we'll use float64 as this incurs minimal cost compared to on GPUs where we'll use float32\ntorch.set_default_dtype(torch.float32)\n</pre> # For convenience, we define the default tensor device and dtype here torch.set_default_device('cuda' if torch.cuda.is_available() else 'cpu') # In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. On CPUs we'll use float64 as this incurs minimal cost compared to on GPUs where we'll use float32 torch.set_default_dtype(torch.float32) In\u00a0[3]: Copied! <pre>### The System Parameters\nmass_cart   = torch.tensor(1.0)  # kg\nmass_pole   = torch.tensor(0.1)  # kg\nlength_pole = torch.tensor(1.0)  # m\ngravity     = torch.tensor(9.81) # m/s^2\nfriction_cart = torch.tensor(0.0)\nfriction_pole = torch.tensor(0.0)\n</pre> ### The System Parameters mass_cart   = torch.tensor(1.0)  # kg mass_pole   = torch.tensor(0.1)  # kg length_pole = torch.tensor(1.0)  # m gravity     = torch.tensor(9.81) # m/s^2 friction_cart = torch.tensor(0.0) friction_pole = torch.tensor(0.0) In\u00a0[4]: Copied! <pre>initial_state = torch.tensor([0.0,-1.0,0.0,0.1])\n\ninitial_time = torch.tensor(0.0)\nfinal_time   = torch.tensor(5.0)\n\ninitial_timestep = torch.tensor(1e-4)\n\ncurrent_integrator = neuralode.integrators.AdaptiveRKV76Integrator\n\natol = rtol = 0.01*torch.tensor(torch.finfo(initial_state.dtype).eps**0.5)\ncommon_integrator_parameters = [mass_cart, mass_pole, length_pole, gravity, friction_cart, friction_pole]\n</pre> initial_state = torch.tensor([0.0,-1.0,0.0,0.1])  initial_time = torch.tensor(0.0) final_time   = torch.tensor(5.0)  initial_timestep = torch.tensor(1e-4)  current_integrator = neuralode.integrators.AdaptiveRKV76Integrator  atol = rtol = 0.01*torch.tensor(torch.finfo(initial_state.dtype).eps**0.5) common_integrator_parameters = [mass_cart, mass_pole, length_pole, gravity, friction_cart, friction_pole] In\u00a0[5]: Copied! <pre>final_state, _, pendulum_states, pendulum_times, estimated_errors = current_integrator.apply(neuralode.dynamics.inverted_pendulum, initial_state, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, torch.tensor(0.0), *common_integrator_parameters)\n</pre> final_state, _, pendulum_states, pendulum_times, estimated_errors = current_integrator.apply(neuralode.dynamics.inverted_pendulum, initial_state, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, torch.tensor(0.0), *common_integrator_parameters) In\u00a0[6]: Copied! <pre>pendulum_x = torch.sin(pendulum_states[...,0]) * length_pole + pendulum_states[...,2]\npendulum_y = torch.cos(pendulum_states[...,0]) * length_pole\n\ncart_x, cart_y = pendulum_states[...,2], torch.zeros_like(pendulum_states[...,2])\n\nneuralode.plot.inverted_pendulum.plot_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, pendulum_times, torch.zeros_like(pendulum_times))\nHTML(neuralode.plot.inverted_pendulum.animate_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, pendulum_times, frame_time=1000/60))\n</pre> pendulum_x = torch.sin(pendulum_states[...,0]) * length_pole + pendulum_states[...,2] pendulum_y = torch.cos(pendulum_states[...,0]) * length_pole  cart_x, cart_y = pendulum_states[...,2], torch.zeros_like(pendulum_states[...,2])  neuralode.plot.inverted_pendulum.plot_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, pendulum_times, torch.zeros_like(pendulum_times)) HTML(neuralode.plot.inverted_pendulum.animate_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, pendulum_times, frame_time=1000/60)) Out[6]:    Your browser does not support the video tag.  In\u00a0[7]: Copied! <pre>torch.random.manual_seed(1)\nrandom.seed(1)\nnp.random.seed(1)\n\nmodel_path = None\nmodel_arguments = dict(max_force=20.0, num_hidden_layers=4, num_hidden_neurons=128)\nif model_path is not None:\n    model_arguments, model_parameters = torch.load(model_path, map_device=torch.get_default_device)\ncart_pole_net = neuralode.models.inverted_pendulum.CartPoleNet(**model_arguments)\nif model_path is not None:\n    cart_pole_net.load_state_dict(model_parameters)\nelse:\n    cart_pole_net.apply(neuralode.models.util.init_weights)\n\njitted_dynamics = torch.jit.script(neuralode.dynamics.inverted_pendulum)\n\ndef functional_controller(state, time, controller, *nn_parameters):\n    return torch.func.functional_call(controller, {k: p for (k, _), p in zip(controller.named_parameters(), nn_parameters)}, (state, time))\n\ndef nn_controlled_pendulum(state, time, controller, mc, mp, length, g, mu_c, mu_p, *nn_parameters):\n    force = functional_controller(state, time, controller, *nn_parameters)[...,0]\n    return jitted_dynamics(state, time, force, mc, mp, length, g, mu_c, mu_p)\n</pre> torch.random.manual_seed(1) random.seed(1) np.random.seed(1)  model_path = None model_arguments = dict(max_force=20.0, num_hidden_layers=4, num_hidden_neurons=128) if model_path is not None:     model_arguments, model_parameters = torch.load(model_path, map_device=torch.get_default_device) cart_pole_net = neuralode.models.inverted_pendulum.CartPoleNet(**model_arguments) if model_path is not None:     cart_pole_net.load_state_dict(model_parameters) else:     cart_pole_net.apply(neuralode.models.util.init_weights)  jitted_dynamics = torch.jit.script(neuralode.dynamics.inverted_pendulum)  def functional_controller(state, time, controller, *nn_parameters):     return torch.func.functional_call(controller, {k: p for (k, _), p in zip(controller.named_parameters(), nn_parameters)}, (state, time))  def nn_controlled_pendulum(state, time, controller, mc, mp, length, g, mu_c, mu_p, *nn_parameters):     force = functional_controller(state, time, controller, *nn_parameters)[...,0]     return jitted_dynamics(state, time, force, mc, mp, length, g, mu_c, mu_p) In\u00a0[8]: Copied! <pre># As the goal is balancing the pole on top of the cart, we generate initial states where the pole\n# is near vertical and moving with some random rotational velocity. This way, the neural network observes\n# multiple states near the vertical state where it needs to learn to counter-balance it.\ntorch.random.manual_seed(2)\nnum_examples = 256\nstate_min = torch.tensor([-0.125*torch.pi, -1.0*torch.pi, -10.0, -1.0])\nstate_max = torch.tensor([ 0.125*torch.pi,  1.0*torch.pi,  10.0,  1.0])\nstate_dataset = torch.rand(num_examples, *state_max.shape) * (state_max - state_min)[None] + state_min[None]\n\nfrac_below = 0.25\nif frac_below &gt; 0.0:\n    state_min = torch.tensor([-0.5*torch.pi+torch.pi, -1.0*torch.pi, 0.0, 0.0])\n    state_max = torch.tensor([ 0.5*torch.pi+torch.pi,  1.0*torch.pi, 0.0, 0.0])\n    state_dataset = torch.cat([\n        state_dataset, \n        torch.rand(math.ceil(num_examples*frac_below), *state_max.shape) * (state_max - state_min)[None] + state_min[None]\n    ], dim=0)\n</pre> # As the goal is balancing the pole on top of the cart, we generate initial states where the pole # is near vertical and moving with some random rotational velocity. This way, the neural network observes # multiple states near the vertical state where it needs to learn to counter-balance it. torch.random.manual_seed(2) num_examples = 256 state_min = torch.tensor([-0.125*torch.pi, -1.0*torch.pi, -10.0, -1.0]) state_max = torch.tensor([ 0.125*torch.pi,  1.0*torch.pi,  10.0,  1.0]) state_dataset = torch.rand(num_examples, *state_max.shape) * (state_max - state_min)[None] + state_min[None]  frac_below = 0.25 if frac_below &gt; 0.0:     state_min = torch.tensor([-0.5*torch.pi+torch.pi, -1.0*torch.pi, 0.0, 0.0])     state_max = torch.tensor([ 0.5*torch.pi+torch.pi,  1.0*torch.pi, 0.0, 0.0])     state_dataset = torch.cat([         state_dataset,          torch.rand(math.ceil(num_examples*frac_below), *state_max.shape) * (state_max - state_min)[None] + state_min[None]     ], dim=0) In\u00a0[9]: Copied! <pre># As our network needs to minimize the angle across all timesteps,\n# we can integrate our loss function alongside the system and then minimize its value at the end.\n# While we'd like to minimize all the parameters, we're more interested in the\n# pole angle and velocity so we can use weighting to reduce the weight of the other parameters\nloss_weights = torch.tensor([2.0, 1.0, 1.0, 1.0])\n# Additionally, we add a small force regularisation term to force the network to be sparing with\n# its controls and avoid spurious outputs.\ndef loss_func(x0, t0, controller, *nn_parameters):\n    force = functional_controller(x0, t0, controller, *nn_parameters)\n    return (loss_weights*x0[...,:4]).square().sum(dim=-1, keepdim=True).sqrt() + 0.001*force.square()\n\n# We append one more state variable that will track our loss.\n# Initially it is set to zero as our initial state is\n# outside the control of our neural network\nstate_dataset = torch.cat([\n    state_dataset[...,:initial_state.shape[0]],\n    torch.zeros_like(state_dataset[...,:1]),\n], dim=-1)\n\n\ndef loss_augmented_rhs(x0, t, *nn_parameters):\n    return torch.cat([\n        nn_controlled_pendulum(x0[...,:4], t, cart_pole_net, *nn_parameters),\n        loss_func(x0[...,:4], t, cart_pole_net, *nn_parameters[6:])\n    ], dim=-1)\n\ntraining_integrator_kwargs = {\n    'atol': atol,\n    'rtol': rtol,\n    # Set the adjoint integration tolerances higher as we don't need the gradients to be exact\n    'backward_atol': atol,\n    'backward_rtol': rtol,\n    # Clip the adjoint gradients of the network parameters as these may vary erratically/unstably during the adjoint integration\n    'gradient_clipping': torch.tensor(1.0)\n}\ntesting_integrator_kwargs = {\n    'atol': atol,\n    'rtol': rtol\n}\n\ndef batched_integrator(x0, integration_duration):\n    return current_integrator.apply(loss_augmented_rhs, x0, initial_time, initial_time+integration_duration, initial_timestep, training_integrator_kwargs, *common_integrator_parameters, *cart_pole_net.parameters())\n</pre> # As our network needs to minimize the angle across all timesteps, # we can integrate our loss function alongside the system and then minimize its value at the end. # While we'd like to minimize all the parameters, we're more interested in the # pole angle and velocity so we can use weighting to reduce the weight of the other parameters loss_weights = torch.tensor([2.0, 1.0, 1.0, 1.0]) # Additionally, we add a small force regularisation term to force the network to be sparing with # its controls and avoid spurious outputs. def loss_func(x0, t0, controller, *nn_parameters):     force = functional_controller(x0, t0, controller, *nn_parameters)     return (loss_weights*x0[...,:4]).square().sum(dim=-1, keepdim=True).sqrt() + 0.001*force.square()  # We append one more state variable that will track our loss. # Initially it is set to zero as our initial state is # outside the control of our neural network state_dataset = torch.cat([     state_dataset[...,:initial_state.shape[0]],     torch.zeros_like(state_dataset[...,:1]), ], dim=-1)   def loss_augmented_rhs(x0, t, *nn_parameters):     return torch.cat([         nn_controlled_pendulum(x0[...,:4], t, cart_pole_net, *nn_parameters),         loss_func(x0[...,:4], t, cart_pole_net, *nn_parameters[6:])     ], dim=-1)  training_integrator_kwargs = {     'atol': atol,     'rtol': rtol,     # Set the adjoint integration tolerances higher as we don't need the gradients to be exact     'backward_atol': atol,     'backward_rtol': rtol,     # Clip the adjoint gradients of the network parameters as these may vary erratically/unstably during the adjoint integration     'gradient_clipping': torch.tensor(1.0) } testing_integrator_kwargs = {     'atol': atol,     'rtol': rtol }  def batched_integrator(x0, integration_duration):     return current_integrator.apply(loss_augmented_rhs, x0, initial_time, initial_time+integration_duration, initial_timestep, training_integrator_kwargs, *common_integrator_parameters, *cart_pole_net.parameters()) In\u00a0[10]: Copied! <pre>batch_size = 32\nnumber_of_epochs = 32\nnumber_of_batches = math.ceil(state_dataset.shape[0] / batch_size)\ntotal_steps = number_of_batches * number_of_epochs\n\noptimiser = torch.optim.AdamW(cart_pole_net.parameters(), lr=4e-5, weight_decay=1e-3)\n\ndef pendulum_closure(minibatch, integration_duration):\n    optimiser.zero_grad()\n    \n    states = minibatch['states']\n    \n    final_state, *_ = batched_integrator(states, integration_duration)\n\n    # The error is simply the integrated error over the whole trajectory\n    ## The integrated error is divided by the integration duration to give a time invariant error\n    error = final_state[...,-1].mean()/integration_duration\n    ## Also minimise the final cart position, ideally it will be at the origin at the end of the integration interval\n    error = error + final_state[...,:4].abs().sum(dim=-1).mean()\n    \n    if error.requires_grad:\n        error.backward()\n    return error\n</pre> batch_size = 32 number_of_epochs = 32 number_of_batches = math.ceil(state_dataset.shape[0] / batch_size) total_steps = number_of_batches * number_of_epochs  optimiser = torch.optim.AdamW(cart_pole_net.parameters(), lr=4e-5, weight_decay=1e-3)  def pendulum_closure(minibatch, integration_duration):     optimiser.zero_grad()          states = minibatch['states']          final_state, *_ = batched_integrator(states, integration_duration)      # The error is simply the integrated error over the whole trajectory     ## The integrated error is divided by the integration duration to give a time invariant error     error = final_state[...,-1].mean()/integration_duration     ## Also minimise the final cart position, ideally it will be at the origin at the end of the integration interval     error = error + final_state[...,:4].abs().sum(dim=-1).mean()          if error.requires_grad:         error.backward()     return error In\u00a0[11]: Copied! <pre>training_step = []\ntraining_error = []\n\nbest_error = torch.inf\nbest_params = copy.deepcopy(cart_pole_net.state_dict())\n\ncart_pole_net.train()\n\nintegration_time = torch.tensor(0.5)\nno_improvement = 0\npeak_lr = None\n\nfor step in range(1, number_of_epochs+1):\n    epoch_error = 0.0\n    shuffled_indices = torch.randperm(state_dataset.shape[0])\n    for batch_idx in range(0, state_dataset.shape[0], batch_size):\n        batch_dict = {\n            'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n        }\n        try:\n            step_error = optimiser.step(lambda: pendulum_closure(batch_dict, integration_time))\n        except RuntimeError as e:\n            print(e)\n            print(\"This is likely due to the integrator not converging, will reset network and reduce learning rate\")\n            optimiser.param_groups[0]['lr'] *= 0.25\n            cart_pole_net.load_state_dict(best_params)\n            continue\n        if any(torch.any(~torch.isfinite(i)) for i in cart_pole_net.parameters()) or any(torch.any(~torch.isfinite(i.grad)) for i in cart_pole_net.parameters() if i.requires_grad):\n            raise ValueError(\"Encountered non-finite parameters/parameters gradients\")\n        epoch_error = epoch_error + step_error.item()*batch_dict['states'].shape[0]\n        print(f\"[{step}/{number_of_epochs} - {optimiser.param_groups[0]['lr']:.4e} - horizon: {integration_time:.5f}]/[{batch_idx}/{state_dataset.shape[0]}] Batch Error: {step_error:.6f} / Running Epoch Error: {epoch_error/(batch_idx+batch_size):.6f}\", end='\\r')\n        if step &lt; 8:\n            optimiser.param_groups[0]['lr'] *= 2.0**(1/number_of_batches)\n        elif step &gt; 16:\n            if peak_lr is None:\n                peak_lr = optimiser.param_groups[0]['lr']\n            optimiser.param_groups[0]['lr'] *= (1e-7/peak_lr)**(1/(number_of_batches*(number_of_epochs - 8)))\n    epoch_error = epoch_error/state_dataset.shape[0]\n    if step == number_of_epochs - 4:\n        optimiser.param_groups[0]['lr'] = 1e-5\n    training_step.append(step)\n    training_error.append(epoch_error)\n    print(\" \"*192, end=\"\\r\")\n    print(f\"[{step}/{number_of_epochs} - {optimiser.param_groups[0]['lr']:.4e} - horizon: {integration_time:.5f}] Epoch Error: {epoch_error:.6f}\")\n    if epoch_error &lt; best_error*0.9999:\n        # if best_error &lt; torch.inf:\n        #     integration_time = (integration_time*(best_error/epoch_error)).clamp(max=1.0)\n        best_error = epoch_error\n        best_params = copy.deepcopy(cart_pole_net.state_dict())\n        no_improvement = 0\n    else:\n        no_improvement += 1\n    if no_improvement == 2:\n        optimiser.param_groups[0]['lr'] *= 0.25\n        cart_pole_net.load_state_dict(best_params)\n    if optimiser.param_groups[0]['lr'] &lt;= 1e-7:\n        break\n    torch.cuda.empty_cache()\n</pre> training_step = [] training_error = []  best_error = torch.inf best_params = copy.deepcopy(cart_pole_net.state_dict())  cart_pole_net.train()  integration_time = torch.tensor(0.5) no_improvement = 0 peak_lr = None  for step in range(1, number_of_epochs+1):     epoch_error = 0.0     shuffled_indices = torch.randperm(state_dataset.shape[0])     for batch_idx in range(0, state_dataset.shape[0], batch_size):         batch_dict = {             'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],         }         try:             step_error = optimiser.step(lambda: pendulum_closure(batch_dict, integration_time))         except RuntimeError as e:             print(e)             print(\"This is likely due to the integrator not converging, will reset network and reduce learning rate\")             optimiser.param_groups[0]['lr'] *= 0.25             cart_pole_net.load_state_dict(best_params)             continue         if any(torch.any(~torch.isfinite(i)) for i in cart_pole_net.parameters()) or any(torch.any(~torch.isfinite(i.grad)) for i in cart_pole_net.parameters() if i.requires_grad):             raise ValueError(\"Encountered non-finite parameters/parameters gradients\")         epoch_error = epoch_error + step_error.item()*batch_dict['states'].shape[0]         print(f\"[{step}/{number_of_epochs} - {optimiser.param_groups[0]['lr']:.4e} - horizon: {integration_time:.5f}]/[{batch_idx}/{state_dataset.shape[0]}] Batch Error: {step_error:.6f} / Running Epoch Error: {epoch_error/(batch_idx+batch_size):.6f}\", end='\\r')         if step &lt; 8:             optimiser.param_groups[0]['lr'] *= 2.0**(1/number_of_batches)         elif step &gt; 16:             if peak_lr is None:                 peak_lr = optimiser.param_groups[0]['lr']             optimiser.param_groups[0]['lr'] *= (1e-7/peak_lr)**(1/(number_of_batches*(number_of_epochs - 8)))     epoch_error = epoch_error/state_dataset.shape[0]     if step == number_of_epochs - 4:         optimiser.param_groups[0]['lr'] = 1e-5     training_step.append(step)     training_error.append(epoch_error)     print(\" \"*192, end=\"\\r\")     print(f\"[{step}/{number_of_epochs} - {optimiser.param_groups[0]['lr']:.4e} - horizon: {integration_time:.5f}] Epoch Error: {epoch_error:.6f}\")     if epoch_error &lt; best_error*0.9999:         # if best_error &lt; torch.inf:         #     integration_time = (integration_time*(best_error/epoch_error)).clamp(max=1.0)         best_error = epoch_error         best_params = copy.deepcopy(cart_pole_net.state_dict())         no_improvement = 0     else:         no_improvement += 1     if no_improvement == 2:         optimiser.param_groups[0]['lr'] *= 0.25         cart_pole_net.load_state_dict(best_params)     if optimiser.param_groups[0]['lr'] &lt;= 1e-7:         break     torch.cuda.empty_cache() <pre>[1/32 - 8.0000e-05 - horizon: 0.50000] Epoch Error: 16.809307                                                                                                                                   \n[2/32 - 1.6000e-04 - horizon: 0.50000] Epoch Error: 16.630486                                                                                                                                   \n[3/32 - 3.2000e-04 - horizon: 0.50000] Epoch Error: 16.399066                                                                                                                                   \n[4/32 - 6.4000e-04 - horizon: 0.50000] Epoch Error: 16.164124                                                                                                                                   \n[5/32 - 1.2800e-03 - horizon: 0.50000] Epoch Error: 15.931533                                                                                                                                   \n[6/32 - 2.5600e-03 - horizon: 0.50000] Epoch Error: 15.734209                                                                                                                                   \n[7/32 - 5.1200e-03 - horizon: 0.50000] Epoch Error: 15.354967                                                                                                                                   \n[8/32 - 5.1200e-03 - horizon: 0.50000] Epoch Error: 13.606040                                                                                                                                   \n[9/32 - 5.1200e-03 - horizon: 0.50000] Epoch Error: 14.351519                                                                                                                                   \n[10/32 - 5.1200e-03 - horizon: 0.50000] Epoch Error: 14.450917                                                                                                                                  \n[11/32 - 1.2800e-03 - horizon: 0.50000] Epoch Error: 12.732606                                                                                                                                  \n[12/32 - 1.2800e-03 - horizon: 0.50000] Epoch Error: 13.745735                                                                                                                                  \n[13/32 - 1.2800e-03 - horizon: 0.50000] Epoch Error: 14.614527                                                                                                                                  \n[14/32 - 3.2000e-04 - horizon: 0.50000] Epoch Error: 13.183453                                                                                                                                  \n[15/32 - 3.2000e-04 - horizon: 0.50000] Epoch Error: 13.615922                                                                                                                                  \n[16/32 - 3.2000e-04 - horizon: 0.50000] Epoch Error: 14.117361                                                                                                                                  \n[17/32 - 2.2861e-04 - horizon: 0.50000] Epoch Error: 14.322703                                                                                                                                  \n[18/32 - 1.6333e-04 - horizon: 0.50000] Epoch Error: 14.406934                                                                                                                                  \n[19/32 - 1.1668e-04 - horizon: 0.50000] Epoch Error: 14.509286                                                                                                                                  \n[20/32 - 8.3360e-05 - horizon: 0.50000] Epoch Error: 14.598582                                                                                                                                  \n[21/32 - 5.9554e-05 - horizon: 0.50000] Epoch Error: 14.675578                                                                                                                                  \n[22/32 - 4.2546e-05 - horizon: 0.50000] Epoch Error: 14.726732                                                                                                                                  \n[23/32 - 3.0396e-05 - horizon: 0.50000] Epoch Error: 14.765353                                                                                                                                  \n[24/32 - 2.1715e-05 - horizon: 0.50000] Epoch Error: 14.784968                                                                                                                                  \n[25/32 - 1.5514e-05 - horizon: 0.50000] Epoch Error: 14.801829                                                                                                                                  \n[26/32 - 1.1083e-05 - horizon: 0.50000] Epoch Error: 14.816362                                                                                                                                  \n[27/32 - 7.9181e-06 - horizon: 0.50000] Epoch Error: 14.825682                                                                                                                                  \n[28/32 - 1.0000e-05 - horizon: 0.50000] Epoch Error: 14.829560                                                                                                                                  \n[29/32 - 7.1442e-06 - horizon: 0.50000] Epoch Error: 14.831281                                                                                                                                  \n[30/32 - 5.1039e-06 - horizon: 0.50000] Epoch Error: 14.835590                                                                                                                                  \n[31/32 - 3.6463e-06 - horizon: 0.50000] Epoch Error: 14.839969                                                                                                                                  \n[32/32 - 2.6050e-06 - horizon: 0.50000] Epoch Error: 14.843607                                                                                                                                  \n</pre> In\u00a0[12]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(8, 4))\nfig.suptitle(\"Controller Loss During Training\")\nax.plot(training_step, training_error)\nax.set_ylabel(\"Training Error\")\nax.set_xlabel(\"Epoch\")\n</pre> fig, ax = plt.subplots(1, 1, figsize=(8, 4)) fig.suptitle(\"Controller Loss During Training\") ax.plot(training_step, training_error) ax.set_ylabel(\"Training Error\") ax.set_xlabel(\"Epoch\") Out[12]: <pre>Text(0.5, 0, 'Epoch')</pre> <p>Furthermore, looking at the animated plots below, we can see that the network successfully learns to balance an upturned pole over $5$s despite only being trained over a period of $0.5$s. While the network hasn't truly minimised the error in that the cart is offset from the origin (and leaving!), the pole is stable above the cart which is the primary goal of the training.</p> In\u00a0[13]: Copied! <pre>def test_controller(test_state, test_duration, controller, hide_static_plot=False):\n    final_state, _, controlled_pendulum_states, controlled_pendulum_times, errors_achieved = current_integrator.apply(nn_controlled_pendulum, test_state, initial_time, initial_time+test_duration, initial_timestep, testing_integrator_kwargs, controller, *common_integrator_parameters)\n    cart_x = controlled_pendulum_states[...,2]\n    cart_y = torch.zeros_like(cart_x)\n    \n    pendulum_x = torch.sin(controlled_pendulum_states[...,0]) * length_pole + cart_x\n    pendulum_y = torch.cos(controlled_pendulum_states[...,0]) * length_pole\n    \n    with torch.no_grad():\n        controller_force = controller(controlled_pendulum_states, controlled_pendulum_times)[...,0]\n    if not hide_static_plot:\n        neuralode.plot.inverted_pendulum.plot_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, controlled_pendulum_times, controller_force)\n    return HTML(neuralode.plot.inverted_pendulum.animate_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, controlled_pendulum_times, forces=controller_force, frame_time=1000/60))\n</pre> def test_controller(test_state, test_duration, controller, hide_static_plot=False):     final_state, _, controlled_pendulum_states, controlled_pendulum_times, errors_achieved = current_integrator.apply(nn_controlled_pendulum, test_state, initial_time, initial_time+test_duration, initial_timestep, testing_integrator_kwargs, controller, *common_integrator_parameters)     cart_x = controlled_pendulum_states[...,2]     cart_y = torch.zeros_like(cart_x)          pendulum_x = torch.sin(controlled_pendulum_states[...,0]) * length_pole + cart_x     pendulum_y = torch.cos(controlled_pendulum_states[...,0]) * length_pole          with torch.no_grad():         controller_force = controller(controlled_pendulum_states, controlled_pendulum_times)[...,0]     if not hide_static_plot:         neuralode.plot.inverted_pendulum.plot_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, controlled_pendulum_times, controller_force)     return HTML(neuralode.plot.inverted_pendulum.animate_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, controlled_pendulum_times, forces=controller_force, frame_time=1000/60)) In\u00a0[14]: Copied! <pre>cart_pole_net.eval()\ncart_pole_net.load_state_dict(best_params)\n\ntest_integration_time = 5.0\n\ndisplay(test_controller(initial_state, test_integration_time, cart_pole_net, hide_static_plot=True))\ndisplay(test_controller(state_dataset[0,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True))\ndisplay(test_controller(state_dataset[1,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True))\ndisplay(test_controller(state_dataset[2,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True))\ndisplay(test_controller(state_dataset[-1,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True))\ndisplay(test_controller(state_dataset[-2,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True))\ndisplay(test_controller(state_dataset[-3,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True))\n</pre> cart_pole_net.eval() cart_pole_net.load_state_dict(best_params)  test_integration_time = 5.0  display(test_controller(initial_state, test_integration_time, cart_pole_net, hide_static_plot=True)) display(test_controller(state_dataset[0,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True)) display(test_controller(state_dataset[1,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True)) display(test_controller(state_dataset[2,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True)) display(test_controller(state_dataset[-1,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True)) display(test_controller(state_dataset[-2,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True)) display(test_controller(state_dataset[-3,...,:-1], test_integration_time, cart_pole_net, hide_static_plot=True))    Your browser does not support the video tag.     Your browser does not support the video tag.     Your browser does not support the video tag.     Your browser does not support the video tag.     Your browser does not support the video tag.     Your browser does not support the video tag.     Your browser does not support the video tag.  In\u00a0[15]: Copied! <pre># We can save the network model inside a tuple such that it can be loaded at a later point\ntorch.save((model_arguments, best_params), \"trained-inverted-pendulum.pt\")\n</pre> # We can save the network model inside a tuple such that it can be loaded at a later point torch.save((model_arguments, best_params), \"trained-inverted-pendulum.pt\")"},{"location":"05-the-inverted-pendulum/#the-inverted-pendulum-or-the-cart-pole-system","title":"The Inverted Pendulum (or the Cart-Pole System)\u00b6","text":"<p>In this notebook, we will go through implementing the inverted pendulum system for which we'll train a neural network to learn to control the pendulum state.</p> <p>A part of this exercise will also show you how to make animations in matplotlib!</p>"},{"location":"05-the-inverted-pendulum/#implementing-the-cart-pole-system","title":"Implementing the Cart-Pole System\u00b6","text":""},{"location":"05-the-inverted-pendulum/#the-dynamics","title":"The Dynamics\u00b6","text":"<p>The cart-pole system is a dynamical system comprising of a cart attached to a linear track with one-degree of freedom (the x-axis), and a rigid pole attach to the cart with one-degree of freedom (the rotational axis). While the equations can be complicated due to the interaction between the cart and the pole, the system itself is easy to imagine. Unlike the previous systems, we will not derive the equations of the system in this notebook as they require an understanding of force-balance equations, and instead quote them below[1]:</p> <p>$$ \\theta^{(2)}= \\frac{g\\sin\\theta + \\cos\\theta\\left\\{\\frac{-F_p + d_N\\cos\\theta}{m_T} + g d_N\\right\\}-\\frac{\\mu_p\\theta^{(1)}}{m_p l}}{l\\left\\{\\frac{4}{3}-\\frac{m_p\\cos\\theta}{m_T}\\left[\\cos\\theta - d_N\\right]\\right\\}} $$ and $$ x^{(2)} = \\frac{F_p - m_p l \\theta^{(2)}\\cos\\theta - N_c d_N}{m_T} $$ where</p> <p>$$ m_T = m_c + m_p $$</p> <p>$$ N_c=m_T g-m_p l \\left(\\theta^{(2)}\\sin\\theta +\\left[\\theta^{(1)}\\right]^2\\cos\\theta\\right) $$</p> <p>$$ d_N = \\mu_c\\mathrm{sgn}\\left(N_c x^{(1)}\\right) $$</p> <p>$$ F_p = F + m_p l \\left[\\theta^{(1)}\\right]^2\\sin\\theta $$ If we neglect friction (i.e. $\\mu_p = \\mu_c = 0$) then this simplifies to:</p> <p>$$ \\theta^{(2)}= \\frac{g\\sin\\theta + \\cos\\theta\\frac{-F_p}{m_T}}{l\\left\\{\\frac{4}{3}-\\frac{m_p\\cos^2\\theta}{m_T}\\right\\}} $$ and $$ x^{(2)} = \\frac{F_p - m_p l \\theta^{(2)}\\cos\\theta}{m_T} $$</p> <p>Using the same transformation as before, we can write this as a first-order Ordinary Differential Equation by introducing $\\omega=\\theta^{(1)}$ and $v = x^{(1)}$ as variables and denoting the state $\\vec{x}=\\left[\\theta, \\omega, x, v\\right]$, we get the dynamical equations $$ \\vec{x}^{(1)}= \\begin{bmatrix} \\omega \\\\ \\frac{g\\sin\\theta + \\cos\\theta\\left\\{\\frac{-F_p + d_N\\cos\\theta}{m_T} + g d_N\\right\\}-\\frac{\\mu_p\\theta^{(1)}}{m_p l}}{l\\left\\{\\frac{4}{3}-\\frac{m_p\\cos\\theta}{m_T}\\left[\\cos\\theta - d_N\\right]\\right\\}} \\\\ v \\\\ \\frac{F_p - m_p l \\theta^{(2)}\\cos\\theta - N_c d_N}{m_T} \\end{bmatrix} $$.</p> <p>You'll note that the right-hand side of the equation depends on $\\theta^{(2)}$ through $N_c$ which is also on the left-hand side of our equation. One way to solve this would be to use a non-linear root-finding algorithm to find the exact value of $\\theta^{(2)}$ that satisfies this equation, but this would be computationally expensive. If you look closely, $N_c$ is the normal force applied by the track on the cart and for $\\theta^{(2)}$ we only use its sign to determine the acceleration of the pole. In most cases, because the cart is attached to the track and weighed down, $N_c$ will be positive so we can assume that it is positive and compute $\\theta^{(2)}$ accordingly. Once we've computed $\\theta^{(2)}$, we can check that $N_c$ is still positive. If it isn't, we can recompute $\\theta^{(2)}$. Because in most cases it will be positive, this incurs minimal computational cost in comparison to using a root-finding scheme.</p> <p>The implementation itself can be found in <code>neuralode.dynamics.inverted_pendulum</code>.</p>"},{"location":"05-the-inverted-pendulum/#training-a-network","title":"Training a Network\u00b6","text":""},{"location":"05-the-inverted-pendulum/#the-model","title":"The Model\u00b6","text":"<p>First we define the model. We've left the definition in the module if you'd like to look at the implementation, but this is a simple Multi-Layer Perceptron (MLP) model with a default of 6 layers (4 hidden, 1 input, 1 output) using a non-linear CELU activation function. Unlike many other MLPs, we've opted to use CELU activations due to two desirable properties that assists in the training of the network:</p> <ol> <li>CELU is $C^1$-continuous (the $1^{st}$ derivative is continuous everywhere)</li> <li>CELU only saturates and dies off when the input is highly negative</li> </ol> <p>The first property is desirable because, during the adjoint integration, we are integrating the gradient of the network and discontinuities in the activation function naturally lead to jumps in the adjoint that are difficult to integrate over using explicit integration techniques. The second property is desirable because our inputs have large magnitudes and have periodic properties, and while we could use normalisation, we'd be removing the periodicity. Instead, sinusoids allow the network to learn periodic control representations that are well suited for a dynamical system such as this.</p> <p>We have also restricted the maximum force applied by the network to $20\\mathrm{N}$ as any real controller would be limited in the amount of force it can apply.</p>"},{"location":"05-the-inverted-pendulum/#the-dataset","title":"The Dataset\u00b6","text":"<p>We want our network to learn how to balance the pole over the cart and thus we generate a dataset of nearly balanced poles (within $22.5^\\mathrm{o}$ of the vertical position) in various angular velocity, cart position and cart velocity configurations for the network to learn from.</p> <p>We also add examples, equal to $3.125\\%$ of our nearly balanced examples, for which the pole is hanging below the cart so that the network can learn to swing the pole up and over the cart before balancing it.</p>"},{"location":"05-the-inverted-pendulum/#loss-function","title":"Loss Function\u00b6","text":"<p>Ideally the network balances the pole above the cart and this is equivalent to having the pole angle equal zero at all times. To that end, we use an loss of the angular deviation from zero and integrate it alongside the dynamics. This gives us a measure of the angular deviation over time and is a quantity that we would like to minimise.</p>"},{"location":"05-the-inverted-pendulum/#training-configuration","title":"Training Configuration\u00b6","text":"<p>We select a batch size of 32 and train for 256 epochs. Unlike our previous networks, the learning rate is set very low as a higher learning rate leads to divergence.</p> <p>The closure function we have accepts a variable integration duration and thus we have set it so that the returned error is invariant to this. Thus the loss function we're minimising is the maximum deviation of the final state and the rate of the integrated error (as opposed to the integrated error itself).</p>"},{"location":"05-the-inverted-pendulum/#training-the-network-incrementally","title":"Training the Network Incrementally\u00b6","text":"<p>While we could have the network tackle the full problem where it tries to balance the pole over many seconds, this is a very difficult problem. Instead, we train the network on a set of random states to minimise the error over $0.5$ seconds with a learning rate that initially ramps up quickly and then decays back down. The goal of a ramping learning rate is to escape any local minima the network may get stuck in, in the initial stages of learning.</p> <p>If the network is able to train successfully over $0.5$ seconds, then, given that any time interval can be split into $0.5$ second increments, our network should still be able to function over a longer period of time.</p>"},{"location":"05-the-inverted-pendulum/#results","title":"Results\u00b6","text":"<p>In the loss graph, we can see that the loss function is successfully decreased over the duration of the training.</p>"},{"location":"05-the-inverted-pendulum/#post-training-notes","title":"Post-Training Notes\u00b6","text":"<p>That training, on my computer, took $12.5$ hours to run, and there are several files in the \"notebooks/artifacts\" which are trained models that are all from different runs with the same seed. As can be seen, they result in slightly different networks each time (GPU training is non-deterministic as is CPU training with multiple threads!).</p> <p>From this, we can see that the network learns how to balance the pole vertically, but struggles to learn how to flip the pole up and over the cart when it's hanging below although we see in some cases that it has learned to swing it back up when it's fallen from the vertical position. Furthermore, the controller starts leaving the origin in almost every case despite being explicitly trained to stay at the origin. This is one of the difficulties of adjoint-based methods, where, unlike reinforcement learning which explicitly controls exploitation vs exploration, we're limited to the signal provided by the gradients of the dynamics which can suffer from local minima and instability.</p> <p>Some solutions could be to include a different position encoding where rather than passing the position and velocity directly to the network we use a sum of sinusoids to encode the information, enforcing some kind of translational invariance. The downside of these methods is that the loss function is not translationally invariant and so these encodings would be at odds with the objective of the training. Another alternative is to cap the magnitude of the distance, $x$ passed to the network so that it remains within the domain of the dataset it was trained on.</p> <p>Neural networks are fickle to train, the above parameters that led to a successfully trained network took quite a lot of guided trial-and-error, especially in terms of picking an appropriate learning rate and learning schedule, and dataset sizes. It is absolutely worth exploring these parameters to see their effects on the training, and try to find ways to improve the convergence of the network. Some ideas to experiment with are weighting the integrated error term differently, dynamically adjusting the weight of the integrated error term based on the integration duration, or changing its form from $\\int_{t_0}^{t_1}\\omega\\mathrm{d}t$ to $\\int_{t_0}^{t_1}\\omega t\\mathrm{d}t$ so that deviations from the target angle of $\\omega=0$ at later times have greater weight than earlier times.</p>"},{"location":"05-the-inverted-pendulum/#references","title":"References\u00b6","text":"<p>[1]: https://coneural.org/florian/papers/05_cart_pole.pdf</p>"}]}