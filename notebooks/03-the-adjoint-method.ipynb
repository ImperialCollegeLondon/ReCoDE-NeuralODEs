{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The Adjoint Method\n",
    "\n",
    "In this notebook, we will go through the process of implementing the adjoint method for computing gradients arising from a numerical integration."
   ],
   "id": "a43901d03f99741f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:03.905953Z",
     "start_time": "2024-06-27T22:25:03.151077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import typing\n",
    "import warnings\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import einops\n",
    "import neuralode\n",
    "\n",
    "warnings.simplefilter('ignore', RuntimeWarning)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:03.911111Z",
     "start_time": "2024-06-27T22:25:03.906957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For convenience, we define the default tensor device and dtype here\n",
    "torch.set_default_device('cpu')\n",
    "# In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training\n",
    "torch.set_default_dtype(torch.float64)"
   ],
   "id": "89b673b74ad153fa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We'll be starting with the same function from the previous notebook, but we have tidied up the code by using `neuralode.util.partial_compensated_sum` to track the truncated bits instead of duplicating the code through our integration function.\n",
    "\n",
    "We have moved many of the functions into a submodule. Most importantly, the class creation and finalisation have been moved into `neuralode.integrators.classes`, and the integrators are now subclasses of `neuralode.integrators.classes.Integrator` which enables us to do type checking and signature checking both statically and at runtime if needed. Duplicated functionality in checking the tolerances has been moved into `neuralode.integrators.helpers.ensure_tolerance` function, and consistency checking of the timestep has been put into `neuralode.integrators.helpers.ensure_timestep`. \n",
    "\n",
    "Also, the entire forward integration loop has now been moved into `neuralode.integrators.routines.solve_ivp` which allows us to separate the concerns of `torch.autograd.Function` and the actual integration (e.g. ctx does not need to be in the same scope). This requires passing around several packaged arguments, but gains us conciseness in the code here, allowing us to focus on the adjoint method implementation."
   ],
   "id": "42abcb68b6ce6798"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:03.925567Z",
     "start_time": "2024-06-27T22:25:03.912116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_integrator(integrator_tableau: torch.Tensor, integrator_order: int, use_local_extrapolation: bool = True, integrator_name: str = None) -> torch.autograd.Function:\n",
    "    __integrator_type = neuralode.integrators.classes.create_integrator_class(integrator_tableau, integrator_order, use_local_extrapolation, integrator_name)\n",
    "    def __internal_forward(ctx: torch.autograd.function.FunctionCtx, forward_fn: typing.Callable[[torch.Tensor, torch.Tensor, typing.Any], torch.Tensor], \n",
    "                           x0: torch.Tensor, t0: torch.Tensor, t1: torch.Tensor, dt: torch.Tensor,\n",
    "                           atol: torch.Tensor, rtol: torch.Tensor, *additional_dynamic_args):\n",
    "        \n",
    "        integrator_spec = (\n",
    "            __integrator_type.integrator_tableau.clone().to(x0.device, x0.dtype),\n",
    "            __integrator_type.is_adaptive,\n",
    "            __integrator_type.number_of_stages,\n",
    "            __integrator_type.integrator_order,\n",
    "            __integrator_type.use_local_extrapolation\n",
    "        )\n",
    "        \n",
    "        c_state, c_time, intermediate_states, intermediate_times, error_in_state = neuralode.integrators.routines.solve_ivp(\n",
    "            forward_fn, integrator_spec, x0, t0, t1, dt, atol, rtol, additional_dynamic_args\n",
    "            )\n",
    "\n",
    "\n",
    "        non_differentiable_parameters = [dt]\n",
    "        backward_save_variables = [x0, t0, t1, dt, c_state, c_time, intermediate_states, intermediate_times, *additional_dynamic_args]\n",
    "        if __integrator_type.is_adaptive:\n",
    "            non_differentiable_parameters = non_differentiable_parameters + [atol, rtol]\n",
    "            backward_save_variables = [atol, rtol] + backward_save_variables\n",
    "        ctx.mark_non_differentiable(*non_differentiable_parameters)\n",
    "        ctx.save_for_backward(*backward_save_variables)\n",
    "        \n",
    "        ctx.__internal_forward = forward_fn\n",
    "        \n",
    "        return c_state, c_time, intermediate_states, intermediate_times, error_in_state.detach()\n",
    "    \n",
    "    def __internal_backward(ctx: torch.autograd.function.FunctionCtx, \n",
    "                            d_c_state: torch.Tensor, \n",
    "                            d_c_time: torch.Tensor, \n",
    "                            d_intermediate_states: torch.Tensor, \n",
    "                            d_intermediate_times: torch.Tensor, \n",
    "                            d_error_in_state: torch.Tensor) -> tuple[typing.Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function computes the gradient of the input variables for `__internal_forward` by exploiting the fact\n",
    "        that PyTorch can track the whole graph of operations used to derive a specific result. Thus each time backward is called,\n",
    "        we compute the actual graph of operations and propagate derivatives through it. Unfortunately, this is an exceptionally\n",
    "        slow method of computation that also uses a lot of memory.\n",
    "        \n",
    "        This is implemented here as a demonstration of how we could compute gradients and how these are expected to be propagated back\n",
    "        to the autograd tape. \n",
    "        \n",
    "        :param ctx: \n",
    "        :param d_c_state: \n",
    "        :param d_c_time: \n",
    "        :param d_intermediate_states: \n",
    "        :param d_intermediate_times: \n",
    "        :param d_error_in_state: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        \n",
    "        # First, we retrieve our integration function that we stored in `__internal_forward`\n",
    "        forward_fn: neuralode.integrators.signatures.integration_fn_signature = ctx.__internal_forward\n",
    "        \n",
    "        # Then we retrieve the input variables\n",
    "        if __integrator_type.is_adaptive:\n",
    "            atol, rtol, x0, t0, t1, dt, c_state, c_time, intermediate_states, intermediate_times, *additional_dynamic_args = ctx.saved_tensors\n",
    "            tol_args = [atol, rtol]\n",
    "        else:\n",
    "            x0, t0, t1, dt, c_state, c_time, intermediate_states, intermediate_times, *additional_dynamic_args = ctx.saved_tensors\n",
    "            atol, rtol = torch.inf, torch.inf\n",
    "            tol_args = []\n",
    "            \n",
    "        inputs = forward_fn, x0, t0, t1, dt, atol, rtol, *additional_dynamic_args\n",
    "        input_grads = [None for _ in range(len(inputs))]\n",
    "        \n",
    "        if any(ctx.needs_input_grad):\n",
    "            # Construct the adjoint equation\n",
    "            def adjoint_fn(packed_state: torch.Tensor, adj_time: torch.Tensor, *dynamic_args: tuple[torch.Tensor]) -> torch.Tensor:\n",
    "                # Unpack the state variables\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    packed_state = packed_state.requires_grad_(True)\n",
    "                    adj_lagrange = packed_state[c_state.numel():2*c_state.numel()].reshape(c_state.shape)\n",
    "                    dy = forward_fn(packed_state[:c_state.numel()].reshape(c_state.shape), adj_time, *dynamic_args).ravel()\n",
    "                # We want the product of the jacobian of dy/dt wrt. each of the parameters multiplied by the adjoint variables\n",
    "                # This is the Jacobian-vector product which can be directly computed through PyTorch autograd\n",
    "                # using the `torch.autograd.grad` function and passing the adjoint variables as the incoming gradients\n",
    "\n",
    "                final_grads = neuralode.util.masked_grad(dy, [packed_state] + list(dynamic_args), adj_lagrange, create_graph=True, materialize_grads=True, allow_unused=True)\n",
    "                adj_lagrange_derivatives = final_grads[0][:c_state.numel()]\n",
    "                parameter_derivatives_of_fn = final_grads[1:]\n",
    "\n",
    "                # The alternative is to compute the jacobian explicitly and then do the product, but this requires\n",
    "                # multiple passes through the autodiff graph which is inefficient. Instead, the above achieves\n",
    "                # the same result in one pass instead of one pass per variable (4 variables in this case)\n",
    "\n",
    "                # state_derivatives_of_ode = torch.autograd.functional.jacobian(lambda st: forward_fn(st, adj_time, *parameter_states).ravel(), state)\n",
    "\n",
    "                # adj_lagrange_derivatives = einops.einsum(adj_lagrange, state_derivatives_of_ode, \"i,i ...->...\").ravel()\n",
    "\n",
    "                # parameter_derivatives_of_ode = torch.autograd.functional.jacobian(lambda p: forward_fn(state, adj_time, *p).ravel(), all_parameters)\n",
    "\n",
    "                # parameter_derivatives_of_ode = torch.cat([\n",
    "                #     einops.einsum(adj_lagrange, i, \"i,i ...->...\").ravel() for i in parameter_derivatives_of_ode\n",
    "                #     ], dim=0)\n",
    "\n",
    "                d_adj = torch.cat([\n",
    "                    dy,\n",
    "                    -adj_lagrange_derivatives,\n",
    "                ], dim=0)\n",
    "                if len(parameter_derivatives_of_fn) > 0:\n",
    "                    parameter_derivatives_of_fn = torch.cat([i.ravel() for i in parameter_derivatives_of_fn])\n",
    "                    d_adj = torch.cat([\n",
    "                        d_adj,\n",
    "                        -parameter_derivatives_of_fn\n",
    "                    ])\n",
    "                return d_adj\n",
    "        \n",
    "            # We ensure that gradients are enabled so that autograd tracks the variable operations\n",
    "            # For pointwise functionals, the initial adjoint state is simply the incoming gradients\n",
    "            parameter_shapes = [i.shape for i in additional_dynamic_args]\n",
    "            packed_reverse_state = torch.cat([\n",
    "                c_state.ravel(),\n",
    "                (d_c_state + d_intermediate_states[-1]).ravel(),\n",
    "            ])\n",
    "            if len(additional_dynamic_args) > 0:\n",
    "                packed_reverse_state = torch.cat([\n",
    "                    packed_reverse_state,\n",
    "                    torch.zeros(sum(map(math.prod, parameter_shapes)), device=c_state.device, dtype=c_state.dtype)\n",
    "                ])\n",
    "\n",
    "            current_adj_time = t1\n",
    "            current_adj_state = packed_reverse_state\n",
    "            \n",
    "            if torch.any(d_intermediate_states != 0.0):\n",
    "                # We only need to account for the incoming gradients if any are non-zero\n",
    "                for next_adj_time, d_inter_state in zip(intermediate_times[1:-1].flip(dims=[0]), d_intermediate_states[1:-1].flip(dims=[0])):\n",
    "                    # The incoming gradients of the intermediate states are the gradients of the state defined at various points in time.\n",
    "                    # For each of these incoming gradients, we need to integrate up to that temporal boundary and add them to adjoint state\n",
    "                    if torch.all(d_inter_state == 0.0):\n",
    "                        # No need to integrate up to the boundary if the incoming gradients are zero\n",
    "                        continue\n",
    "                    current_adj_state, current_adj_time, _, _, _ = __integrator_type.apply(adjoint_fn, current_adj_state, current_adj_time, next_adj_time, -dt, *tol_args, *additional_dynamic_args)\n",
    "                    packed_reverse_state = torch.cat([\n",
    "                        torch.zeros_like(c_state.ravel()),\n",
    "                        d_inter_state.ravel(),\n",
    "                    ])\n",
    "                    if len(additional_dynamic_args) > 0:\n",
    "                        packed_reverse_state = torch.cat([\n",
    "                            packed_reverse_state,\n",
    "                            torch.zeros(sum(map(math.prod, parameter_shapes)), device=c_state.device, dtype=c_state.dtype)\n",
    "                        ])\n",
    "                    current_adj_state = current_adj_state + packed_reverse_state\n",
    "            \n",
    "            final_adj_state, final_adj_time, _, _, _ = __integrator_type.apply(adjoint_fn, current_adj_state, current_adj_time, t0, -dt, *tol_args, *additional_dynamic_args)\n",
    "            \n",
    "            adj_variables = final_adj_state[c_state.numel():2*c_state.numel()].reshape(c_state.shape)\n",
    "            adj_parameter_gradients = final_adj_state[2*c_state.numel():]\n",
    "            \n",
    "            # The gradients of the incoming state are equal to the gradients from the first element of the intermediate state\n",
    "            # plus the lagrange variables\n",
    "            input_grads[1] = adj_variables + d_intermediate_states[0].ravel().reshape(c_state.shape)\n",
    "            \n",
    "            # The gradient of the initial time is equal to the gradient from the first element of the intermediate times\n",
    "            # minus the product of the lagrange variables and the derivative of the system at the initial time\n",
    "            input_grads[2] = d_intermediate_times[0].ravel() - einops.einsum(adj_variables.ravel(), forward_fn(final_adj_state[:c_state.numel()].reshape(c_state.shape), final_adj_time, *additional_dynamic_args).ravel(), \"i,i->\")\n",
    "            # The gradient of the final time is equal to the gradient from the gradient in the final state\n",
    "            # plus the product of the lagrange variables and the derivative of the system at the final time\n",
    "            input_grads[3] = (d_c_time + d_intermediate_times[-1]) + einops.einsum((d_c_state + d_intermediate_states[-1]).ravel(), forward_fn(c_state, c_time, *additional_dynamic_args).ravel(), \"i,i->\")\n",
    "            \n",
    "            parameter_gradients = []\n",
    "            \n",
    "            for p_shape, num_elem in zip(parameter_shapes, map(math.prod, parameter_shapes)): \n",
    "                parameter_gradients.append(None)\n",
    "                adj_parameter_gradients, parameter_gradients[-1] = adj_parameter_gradients[num_elem:], adj_parameter_gradients[:num_elem].reshape(p_shape)\n",
    "            \n",
    "            input_grads[7:] = parameter_gradients\n",
    "            \n",
    "            if not __integrator_type.is_adaptive:\n",
    "                input_grads = input_grads[:5] + input_grads[7:]\n",
    "        return tuple(input_grads)\n",
    "\n",
    "    neuralode.integrators.classes.finalise_integrator_class(__integrator_type, __internal_forward, __internal_backward)\n",
    "    \n",
    "    return __integrator_type"
   ],
   "id": "b54c677cd4e130d4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:03.933486Z",
     "start_time": "2024-06-27T22:25:03.926572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initial_position = torch.tensor(1.0)\n",
    "initial_velocity = torch.tensor(0.0)\n",
    "\n",
    "frequency = (torch.ones_like(initial_position)).requires_grad_(True)\n",
    "damping = (torch.ones_like(initial_position)*0.25).requires_grad_(True)\n",
    "initial_state = torch.stack([\n",
    "    initial_position,\n",
    "    initial_velocity,\n",
    "], dim=-1).requires_grad_(True)\n",
    "\n",
    "initial_time = torch.tensor(0.0).requires_grad_(True)\n",
    "final_time   = torch.tensor(10.0).requires_grad_(True)\n",
    "\n",
    "initial_timestep = (final_time - initial_time) / 100\n",
    "\n",
    "adaptive_rk45_integrator = get_integrator(torch.tensor([\n",
    "    [0.0,       0.0,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],\n",
    "    [1/5,       1/5,         0.0,        0.0,         0.0,      0.0,          0.0,      0.0 ],\n",
    "    [3/10,      3/40,        9/40,       0.0,         0.0,      0.0,          0.0,      0.0 ],\n",
    "    [4/5,       44/45,      -56/15,      32/9,        0.0,      0.0,          0.0,      0.0 ],\n",
    "    [8/9,       19372/6561, -25360/2187, 64448/6561, -212/729,  0.0,          0.0,      0.0 ],\n",
    "    [1.0,       9017/3168,  -355/33,     46732/5247,  49/176,  -5103/18656,   0.0,      0.0 ],\n",
    "    [1.0,       35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],\n",
    "    [torch.inf, 35/384,      0.0,        500/1113,    125/192, -2187/6784,    11/84,    0.0 ],\n",
    "    [torch.inf, 5179/57600,  0.0,        7571/16695,  393/640, -92097/339200, 187/2100, 1/40]\n",
    "], dtype=torch.float64), integrator_order = 5, integrator_name = \"AdaptiveRK45Integrator\")\n",
    "\n",
    "atol = rtol = torch.tensor(5e-8)"
   ],
   "id": "9229a4063f9112b7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:05.291042Z",
     "start_time": "2024-06-27T22:25:03.934491Z"
    }
   },
   "cell_type": "code",
   "source": "final_state, _, sha_states, sha_times, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, atol, rtol, frequency, damping)",
   "id": "d4b047778f31af04",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:05.624748Z",
     "start_time": "2024-06-27T22:25:05.292050Z"
    }
   },
   "cell_type": "code",
   "source": "fig, axes = neuralode.plot.trajectory.plot_trajectory([(i, j) for i, j in zip(sha_states, sha_times)], method_label=\"RK4(5) - Simple Harmonic Oscillator\")",
   "id": "fab5b4c49d43743c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Jacobian is the matrix of gradients of each output with respect to each input. For given vector function $\\vec{f}(\\vec{x})$, the Jacobian is:\n",
    "\n",
    "$$\n",
    "J = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{f_1}}{\\partial{x_1}} & \\cdots & \\frac{\\partial{f_1}}{\\partial{x_n}} \\\\\n",
    "& \\vdots & \\\\\n",
    "\\frac{\\partial{f_m}}{\\partial{x_1}} & \\cdots & \\frac{\\partial{f_m}}{\\partial{x_n}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial{f_m}}{\\partial{x_n}}$ is the (partial) derivative of the $m^{th}$ component of $\\vec{f}$ with respect to the $n^{th}$ component of $\\vec{x}$. What this matrix tells us how $\\vec{f}$ when $\\vec{x}$ changes. \n",
    " \n",
    "If we think of our integration as a function $\\vec{f}$, the Jacobian indicates how the result of our integration changes with respect to the inputs. We can compute it by creating a function that simply returns our trajectory and then using the `pytorch.autograd` functional API as follows."
   ],
   "id": "adeb9cfc0ba6e248"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:10.220268Z",
     "start_time": "2024-06-27T22:25:05.625755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_trajectory_return_final_state(x, t0, t1, f, d):\n",
    "    return adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, x, t0, t1, initial_timestep, atol/1000, rtol/1000, f, d)[0][0]\n",
    "\n",
    "def compute_trajectory_return_intermediate_state(x, t0, t1, f, d):\n",
    "    return adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, x, t0, t1, initial_timestep, atol/1000, rtol/1000, f, d)[2][3][0]\n",
    "\n",
    "test_variables = [initial_state.clone(), initial_time.clone(), final_time.clone(), frequency.clone(), damping.clone()]\n",
    "test_variables = [i.detach().clone().requires_grad_(True) for i in test_variables]\n",
    "\n",
    "jac_wrt_initial_state, jac_wrt_initial_time, jac_wrt_final_time, jac_wrt_freq, jac_wrt_damp = torch.autograd.functional.jacobian(compute_trajectory_return_final_state, tuple(test_variables))\n",
    "print(f\"The Jacobian of x(t1) wrt. x(t0) is: {jac_wrt_initial_state.cpu().numpy()}\")\n",
    "print(f\"The Jacobian of x(t1) wrt. t0 is: {jac_wrt_initial_time.cpu().numpy()}\")\n",
    "print(f\"The Jacobian of x(t1) wrt. t1 is: {jac_wrt_final_time.cpu().numpy()}\")\n",
    "print(f\"The Jacobian of x(t1) wrt. frequency is: {jac_wrt_freq.cpu().numpy()}\")\n",
    "print(f\"The Jacobian of x(t1) wrt. damping is: {jac_wrt_damp.cpu().numpy()}\")\n",
    "\n",
    "jac_wrt_initial_state, jac_wrt_initial_time, jac_wrt_final_time, jac_wrt_freq, jac_wrt_damp = torch.autograd.functional.jacobian(compute_trajectory_return_intermediate_state, tuple(test_variables))\n",
    "print()\n",
    "print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. x(t0) is: {jac_wrt_initial_state.cpu().numpy()}\")\n",
    "print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. t0 is: {jac_wrt_initial_time.cpu().numpy()}\")\n",
    "print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. t1 is: {jac_wrt_final_time.cpu().numpy()}\")\n",
    "print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. frequency is: {jac_wrt_freq.cpu().numpy()}\")\n",
    "print(f\"The Jacobian of x({sha_times[3].item():.4e}) wrt. damping is: {jac_wrt_damp.cpu().numpy()}\")"
   ],
   "id": "633df23f7308c2f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Jacobian of x(t1) wrt. x(t0) is: [-0.08477596 -0.02160443]\n",
      "The Jacobian of x(t1) wrt. t0 is: -0.021604426078305743\n",
      "The Jacobian of x(t1) wrt. t1 is: 0.021604426103785476\n",
      "The Jacobian of x(t1) wrt. frequency is: 0.21604426092022275\n",
      "The Jacobian of x(t1) wrt. damping is: 0.8236204066664241\n",
      "\n",
      "The Jacobian of x(3.6849e-01) wrt. x(t0) is: [0.99497297 0.09847432]\n",
      "The Jacobian of x(3.6849e-01) wrt. t0 is: 0.09847432250120462\n",
      "The Jacobian of x(3.6849e-01) wrt. t1 is: 0.0\n",
      "The Jacobian of x(3.6849e-01) wrt. frequency is: -0.009961476785662705\n",
      "The Jacobian of x(3.6849e-01) wrt. damping is: 0.000336109749325613\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can further use the `torch.autograd.gradcheck` function to test that these Jacobians are correct. The basic overview of `gradcheck` is that it computes the gradient using finite differences^[1] and then compares them to our implementation of the gradient. Within some numerical tolerance, these should be identical and this function can test this. We can also test that the gradient of the gradient is correct by using a similar procedure and the `torch.autograd.gradgradcheck` function.\n",
    "\n",
    "The three cases we need to test are if the initial state, the final state and an arbitrarily selected intermediate state are correctly differentiated. We also need to test these with tighter tolerances than our previous integration as we need to minimise the error accumulated during both the forward _and_ the backward integration.\n",
    "\n",
    "\n",
    "[[1] - Finite Difference on Wikipedia](https://en.wikipedia.org/wiki/Finite_difference)"
   ],
   "id": "8e96dc1c5c68d7ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T22:25:22.955902Z",
     "start_time": "2024-06-27T22:25:10.220268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fd_stencil_steps = [-1, 0, 1]\n",
    "fd_stencil = [1, -2, 1]\n",
    "fd_step = 1e-4\n",
    "fd_denominator = 1.0 / (fd_step*fd_step)\n",
    "\n",
    "var_to_perturb = 2\n",
    "variables_to_test = [i.detach().clone() for i in [initial_state.clone(), initial_time.clone(), final_time.clone(), frequency.clone(), damping.clone()]]\n",
    "\n",
    "fd_estimate = sum(fdc*compute_trajectory_return_final_state(*[i if idx != var_to_perturb else i + fds*fd_step for idx, i in enumerate(variables_to_test)]) for fdc, fds in zip(fd_stencil, fd_stencil_steps)) * fd_denominator\n",
    "def test_fn(v):\n",
    "    return compute_trajectory_return_final_state(*[i.detach().clone() if idx != var_to_perturb else v for idx, i in enumerate(variables_to_test)])\n",
    "\n",
    "def test_fn2():\n",
    "    fd_stencil_1st = [-1, 0, 1]\n",
    "    fd_denominator_1st = 1.0 / (2.0 * fd_step)\n",
    "    return sum(fdc*torch.autograd.functional.jacobian(test_fn, ((variables_to_test[var_to_perturb].detach() + fds*fd_step).clone().requires_grad_(True),))[0] for fdc, fds in zip(fd_stencil_1st, fd_stencil_steps)) * fd_denominator_1st\n",
    "\n",
    "def test_fn3(v):\n",
    "    return torch.autograd.functional.jacobian(test_fn, (v,), create_graph=True)\n",
    "\n",
    "autograd_estimate = torch.autograd.functional.jacobian(test_fn3, (variables_to_test[var_to_perturb].detach().clone().requires_grad_(True),))\n",
    "alt_fd_estimate = test_fn2()\n",
    "print(fd_estimate, alt_fd_estimate, autograd_estimate)"
   ],
   "id": "e8969c993b122354",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0740, grad_fn=<MulBackward0>) tensor(0.0740) ((tensor(0.0740),),)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T23:06:47.582910Z",
     "start_time": "2024-06-27T23:06:47.570899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp):\n",
    "    # Integrates the system with tighter tolerances\n",
    "    res = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, init_state, integration_t0, integration_t1, initial_timestep.double()*1e-2, torch.zeros_like(atol.double()), torch.ones_like(rtol.double())*1e-6, freq, damp)\n",
    "    return res\n",
    "\n",
    "def test_func_initial_state(init_state, integration_t0, integration_t1, freq, damp):\n",
    "    # Integrates the system and returns the initial state stored in the intermediate states\n",
    "    res = test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp)\n",
    "    return res[2][0]\n",
    "\n",
    "def test_func_intermediate_state(init_state, integration_t0, _, freq, damp):\n",
    "    # Ideally we'd pick from the intermediate states tensor, but during finite differencing \n",
    "    # the time values may change and as a result we will sample the wrong point in the trajectory.\n",
    "    # The solution to this is to treat the system as being integrated to some intermediate time and return\n",
    "    # the final state.\n",
    "    res = test_func_adaptive(init_state, integration_t0, sha_times[sha_times.shape[0]//2], freq, damp)\n",
    "    return res[0]\n",
    "    \n",
    "def test_func_final_state(init_state, integration_t0, integration_t1, freq, damp):\n",
    "    # Return the final state of the integration\n",
    "    res = test_func_adaptive(init_state, integration_t0, integration_t1, freq, damp)\n",
    "    return res[0]"
   ],
   "id": "b182da4d83a7624",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T23:07:02.983307Z",
     "start_time": "2024-06-27T23:06:47.814804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.autograd import gradcheck, gradgradcheck\n",
    "\n",
    "test_variables = [initial_state, initial_time, initial_time.detach().clone()+0.1, frequency, damping]\n",
    "# 64-bit floats are required for correct evaluation of finite differences and derivatives\n",
    "test_variables = [i.double() for i in test_variables]\n",
    "test_functions = [test_func_final_state, test_func_initial_state, test_func_intermediate_state]\n",
    "\n",
    "# The initial and final integration times are not twice differentiable,\n",
    "# so we exclude them from the Hessian tests\n",
    "non_twice_differentiable = []\n",
    "\n",
    "def generate_test_vars():\n",
    "    # Randomly generate difference integration conditions\n",
    "    test_t0 = (torch.rand_like(initial_time.double()) - 1.0)*0.1\n",
    "    test_t1 = torch.rand_like(initial_time.double())*0.1 + test_t0.detach()\n",
    "    test_x = 2*torch.rand_like(initial_state.double()) - 1.0\n",
    "    test_frequency = torch.rand_like(frequency.double())\n",
    "    test_damping = torch.rand_like(damping.double())\n",
    "    return [i.requires_grad_(True) for i in [test_x, test_t0, test_t1, test_frequency, test_damping]]\n",
    "\n",
    "num_tests = 16\n",
    "\n",
    "# Run test on our initial conditions defined earlier\n",
    "print(f\"[0/{num_tests}] - vars: {[i.detach().cpu().numpy() for i in test_variables]}, success_jacobian: [\", end='')\n",
    "for fn in test_functions:\n",
    "    print(gradcheck(fn, [i.detach().clone().requires_grad_(True) for i in test_variables], atol=1e-3, rtol=1e-2), end=', ' if fn != test_functions[-1] else '')\n",
    "print('], success_hessian: [', end='')\n",
    "for fn in test_functions:\n",
    "    print(gradgradcheck(fn, [i.detach().clone().requires_grad_(True) for i in test_variables], atol=1e-3, rtol=1e-2, fast_mode=False), end=', ' if fn != test_functions[-1] else '')\n",
    "print(']')\n",
    "\n",
    "# Run test on the randomly generated states\n",
    "for iter_idx in range(num_tests):\n",
    "    variables = generate_test_vars()\n",
    "    print(f\"[{iter_idx+1}/{num_tests}] - vars: {[i.detach().cpu().numpy() for i in variables]}, success: [\", end='')\n",
    "    for fn in test_functions:\n",
    "        print(gradcheck(fn, variables, atol=1e-3, rtol=1e-2), end=', ' if fn != test_functions[-1] else '')\n",
    "    print('], success_hessian: [', end='')\n",
    "    for fn in test_functions:\n",
    "        print(gradgradcheck(fn, variables, atol=1e-3, rtol=1e-2, fast_mode=False), end=', ' if fn != test_functions[-1] else '')\n",
    "    print(']')"
   ],
   "id": "f93866333301ab8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/16] - vars: [array([1., 0.]), array(0.), array(0.1), array(1.), array(0.25)], success_jacobian: ["
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x00000211666C5A00>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ekin4\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\neuralode-DDywRCkr-py3.12\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "GradcheckError",
     "evalue": "Jacobian mismatch for output 0 with respect to input 4,\nnumerical:tensor([[0.0003, 0.0097]])\nanalytical:tensor([[0.0008, 0.0113]])\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mGradcheckError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[0/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_tests\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] - vars: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m[i\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mi\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mtest_variables]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, success_jacobian: [\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fn \u001B[38;5;129;01min\u001B[39;00m test_functions:\n\u001B[1;32m---> 26\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mgradcheck\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequires_grad_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtest_variables\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43matol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrtol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e-2\u001B[39;49m\u001B[43m)\u001B[49m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m fn \u001B[38;5;241m!=\u001B[39m test_functions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m], success_hessian: [\u001B[39m\u001B[38;5;124m'\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fn \u001B[38;5;129;01min\u001B[39;00m test_functions:\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\neuralode-DDywRCkr-py3.12\\Lib\\site-packages\\torch\\autograd\\gradcheck.py:2049\u001B[0m, in \u001B[0;36mgradcheck\u001B[1;34m(func, inputs, eps, atol, rtol, raise_exception, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode, masked)\u001B[0m\n\u001B[0;32m   2047\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m   2048\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2049\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_gradcheck_helper\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\neuralode-DDywRCkr-py3.12\\Lib\\site-packages\\torch\\autograd\\gradcheck.py:2078\u001B[0m, in \u001B[0;36m_gradcheck_helper\u001B[1;34m(func, inputs, eps, atol, rtol, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode, masked)\u001B[0m\n\u001B[0;32m   2073\u001B[0m _check_outputs(outputs)\n\u001B[0;32m   2075\u001B[0m gradcheck_fn \u001B[38;5;241m=\u001B[39m functools\u001B[38;5;241m.\u001B[39mpartial(\n\u001B[0;32m   2076\u001B[0m     _fast_gradcheck \u001B[38;5;28;01mif\u001B[39;00m fast_mode \u001B[38;5;28;01melse\u001B[39;00m _slow_gradcheck, masked\u001B[38;5;241m=\u001B[39mmasked\n\u001B[0;32m   2077\u001B[0m )\n\u001B[1;32m-> 2078\u001B[0m \u001B[43m_gradcheck_real_imag\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2079\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgradcheck_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2080\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2081\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunc_out\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2082\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtupled_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2083\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2084\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2085\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2086\u001B[0m \u001B[43m    \u001B[49m\u001B[43matol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2087\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheck_grad_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2088\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheck_forward_ad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_forward_ad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2089\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheck_backward_ad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_backward_ad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2090\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnondet_tol\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnondet_tol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2091\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheck_undefined_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheck_undefined_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2092\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2094\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_batched_forward_grad:\n\u001B[0;32m   2095\u001B[0m     _test_batched_grad_forward_ad(func, tupled_inputs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\neuralode-DDywRCkr-py3.12\\Lib\\site-packages\\torch\\autograd\\gradcheck.py:1488\u001B[0m, in \u001B[0;36m_gradcheck_real_imag\u001B[1;34m(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, check_forward_ad, check_backward_ad, nondet_tol, check_undefined_grad)\u001B[0m\n\u001B[0;32m   1475\u001B[0m         gradcheck_fn(\n\u001B[0;32m   1476\u001B[0m             real_fn,\n\u001B[0;32m   1477\u001B[0m             real_func_out,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1485\u001B[0m             complex_indices\u001B[38;5;241m=\u001B[39mcomplex_out_indices,\n\u001B[0;32m   1486\u001B[0m         )\n\u001B[0;32m   1487\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1488\u001B[0m         \u001B[43mgradcheck_fn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1489\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1490\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc_out\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1491\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtupled_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1492\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1493\u001B[0m \u001B[43m            \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1494\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1495\u001B[0m \u001B[43m            \u001B[49m\u001B[43matol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1496\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcheck_grad_dtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1497\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnondet_tol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1498\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m check_forward_ad:\n\u001B[0;32m   1501\u001B[0m     complex_inp_indices \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m   1502\u001B[0m         i\n\u001B[0;32m   1503\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i, inp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tupled_inputs)\n\u001B[0;32m   1504\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m is_tensor_like(inp) \u001B[38;5;129;01mand\u001B[39;00m inp\u001B[38;5;241m.\u001B[39mis_complex()\n\u001B[0;32m   1505\u001B[0m     ]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\neuralode-DDywRCkr-py3.12\\Lib\\site-packages\\torch\\autograd\\gradcheck.py:1629\u001B[0m, in \u001B[0;36m_slow_gradcheck\u001B[1;34m(func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, nondet_tol, use_forward_ad, complex_indices, test_imag, masked)\u001B[0m\n\u001B[0;32m   1627\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m j, (a, n) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(analytical, numerical[i])):\n\u001B[0;32m   1628\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _allclose_with_type_promotion(a, n\u001B[38;5;241m.\u001B[39mto(a\u001B[38;5;241m.\u001B[39mdevice), rtol, atol):\n\u001B[1;32m-> 1629\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m GradcheckError(\n\u001B[0;32m   1630\u001B[0m                     _get_notallclose_msg(a, n, i, j, complex_indices, test_imag)\n\u001B[0;32m   1631\u001B[0m                 )\n\u001B[0;32m   1633\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mGradcheckError\u001B[0m: Jacobian mismatch for output 0 with respect to input 4,\nnumerical:tensor([[0.0003, 0.0097]])\nanalytical:tensor([[0.0008, 0.0113]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate reference trajectory for optimisation/learning\n",
    "with torch.no_grad():\n",
    "    _, _, sha_states_ref, sha_times_ref, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, atol, rtol, frequency, damping)\n",
    "sha_states_ref, sha_times_ref = sha_states_ref.detach(), sha_times_ref.detach()"
   ],
   "id": "30bb5959ef5611bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state_dataset = sha_states_ref.clone()\n",
    "time_dataset  = sha_times_ref.clone()\n",
    "# We skip the first time as we know the initial state\n",
    "\n",
    "# We reinitialise our variables\n",
    "optimised_frequency = torch.tensor(0.1, requires_grad=True)\n",
    "optimised_damping = torch.tensor(1.0, requires_grad=True)\n",
    "# As damping needs to be a strictly positive quantity, we log-encode it\n",
    "log_encoded_damping = torch.log(optimised_damping.detach()).requires_grad_(True)\n",
    "\n",
    "# First, we'll create an `optimiser` following pytorch convention\n",
    "optimiser = torch.optim.Adam([optimised_frequency, log_encoded_damping], lr=1e-1, amsgrad=True)\n",
    "# Whenever the loss plateaus, we can reduce the learning rate to improve convergence\n",
    "lr_on_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser)\n",
    "\n",
    "# Next, we'll define a closure function whose sole purpose is to\n",
    "# zero the gradients and compute the error. This is useful as it allows switching to other\n",
    "# optimisers such as LBFGS or anything that re-evaluates the error without\n",
    "# computing its gradient\n",
    "def sha_closure(minibatch):\n",
    "    current_state = initial_state.detach().clone()\n",
    "    current_time  = initial_time.detach().clone()\n",
    "    optimiser.zero_grad()\n",
    "    error = 0.0\n",
    "    \n",
    "    times = minibatch['times']\n",
    "    states = minibatch['states']\n",
    "    \n",
    "    # We need to sort both times and states simultaneously, so we'll use `argsort`\n",
    "    sorted_time_indices = torch.argsort(times)\n",
    "    times, states = times[sorted_time_indices], states[sorted_time_indices]\n",
    "    \n",
    "    for sample_state, sample_time in zip(states, times):\n",
    "        dt = torch.minimum(initial_timestep, sample_time - current_time).detach()\n",
    "        new_state, new_time, _, _, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, current_state, current_time, sample_time, dt, atol, rtol, optimised_frequency, torch.exp(log_encoded_damping))\n",
    "        error = error + torch.linalg.norm(sample_state - new_state)/times.shape[0]\n",
    "        current_state, current_time = new_state, new_time\n",
    "    if error.requires_grad:\n",
    "        error.backward()\n",
    "    return error\n",
    "\n",
    "# We need to set the size of our mini-batches\n",
    "batch_size = 16\n",
    "\n",
    "# Now we need an optimisation `loop` where we will take steps to minimise the error\n",
    "number_of_gd_steps = 256*batch_size//time_dataset.shape[0]\n",
    "\n",
    "# We also need to track the best solution thus far\n",
    "best_error = torch.inf\n",
    "best_frequency, best_damping = optimised_frequency.detach().clone(), optimised_damping.detach().clone()\n",
    "for step in range(number_of_gd_steps):\n",
    "    epoch_error = 0.0\n",
    "    shuffled_indices = torch.randperm(time_dataset.shape[0])\n",
    "    for batch_idx in range(0, time_dataset.shape[0], batch_size):\n",
    "        batch_dict = {\n",
    "            'times': time_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n",
    "            'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n",
    "        }\n",
    "    \n",
    "        step_error = optimiser.step(lambda: sha_closure(batch_dict))\n",
    "        epoch_error = epoch_error + step_error.item()*batch_dict['times'].shape[0]\n",
    "        print(f\"[{step+1}/{number_of_gd_steps}]/[{batch_idx}/{time_dataset.shape[0]}] Batch Error: {step_error:.6f}, Current Frequency: {optimised_frequency.item():.4f}, Current Damping: {torch.exp(log_encoded_damping).item():.4f}\", end='\\r')\n",
    "    epoch_error = epoch_error/time_dataset.shape[0]\n",
    "    if epoch_error < best_error:\n",
    "        best_error = epoch_error\n",
    "        best_frequency = optimised_frequency.detach().clone()\n",
    "        best_damping = torch.exp(log_encoded_damping.detach().clone())\n",
    "    lr_on_plateau.step(epoch_error)\n",
    "    print(\" \"*128, end=\"\\r\")\n",
    "    print(f\"[{step+1}/{number_of_gd_steps}] Epoch Error: {epoch_error:.6f}, Current Frequency: {optimised_frequency.item():.6f}, Current Damping: {torch.exp(log_encoded_damping).item():.6f}\")\n",
    "    # If the step size is too small, then we can interrupt the\n",
    "    # training as it will not lead to significant improvements\n",
    "    if lr_on_plateau.get_last_lr()[0] < 1e-6:\n",
    "        break\n",
    "\n",
    "rel_err = torch.mean(torch.abs(1 - best_frequency / frequency)).item()\n",
    "mae_err = torch.mean(torch.abs(frequency - best_frequency)).item()\n",
    "print(f\"Best frequency: {best_frequency.item():.6f}, relative error: {rel_err:.6%}, mean absolute error: {mae_err:.6f}\")\n",
    "rel_err = torch.mean(torch.abs(1 - best_damping / damping)).item()\n",
    "mae_err = torch.mean(torch.abs(damping - best_damping)).item()\n",
    "print(f\"Best damping:   {best_damping.item():.6f}, relative error: {rel_err:.6%}, mean absolute error: {mae_err:.6f}\")"
   ],
   "id": "51ab785d574fb8bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\")\n",
    "fig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")\n",
    "\n",
    "_, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(neuralode.dynamics.simple_harmonic_oscillator, initial_state, initial_time, final_time, initial_timestep, atol, rtol, best_frequency, best_damping)\n",
    "_ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\")\n",
    "_ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\")"
   ],
   "id": "24d6f35a733f9cd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This replicates the prior results and actually improves on them by a small margin.\n",
    "\n",
    "Now let's add a neural network into the mix! This will be the simplest network possible aka a matrix multiplied with the input vector. While this may seem simple, you'll see that our simple harmonic oscillator can also be expressed as a matrix multiplied by the input. It has a very specific structure that arises from the fact that it was a second order equation. I've written this matrix below:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    x^{(1)} \\\\\n",
    "    v^{(1)}\n",
    "\\end{bmatrix} = \n",
    "\\mathbf{A}\n",
    "\\begin{bmatrix}\n",
    "    x \\\\\n",
    "    v\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{A} =\n",
    "\\begin{bmatrix}\n",
    "    0 & 1 \\\\\n",
    "    -\\omega^2 & -2\\zeta\\omega\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Given that matrix multiplication underlies most neural networks, we can try to learn this $\\mathbf{A}$-matrix and at the same time introduce some of the Neural Network machinery in PyTorch. We will revisit these later when learning more interesting/complex systems."
   ],
   "id": "7502d1d3e0867df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state_dataset = sha_states_ref.clone()\n",
    "time_dataset  = sha_times_ref.clone()\n",
    "\n",
    "# we define our network as a subclass of torch.nn.Module\n",
    "# This allows PyTorch to appropriately track parameters\n",
    "class OscillatorNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # First we initialise the superclass, `torch.nn.Module`\n",
    "        super().__init__()\n",
    "        # Then we define the actual neural network\n",
    "        # Most Neural Networks operate sequentially so they can be wrapped\n",
    "        # inside a torch.nn.Sequential which takes each layer\n",
    "        # as an argument.\n",
    "        # Since we're only learning one matrix, we have\n",
    "        # one layer, the `torch.nn.Linear`.\n",
    "        # `torch.nn.Linear` stores a matrix and a bias which actually makes it\n",
    "        # an Affine transformation rather than a purely linear transformation\n",
    "        self.internal_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 2),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        # Our network only depends on x, but since it could also depend on t, we have\n",
    "        # included it for completeness\n",
    "        # Additionally, PyTorch layers and modules expect a batched tensor\n",
    "        # ie. a tensor where the first dimension is over different samples\n",
    "        # Since we don't depend on batches, we check if the input is 1-dimensional\n",
    "        # And add a batch dimension as needed for the internal module\n",
    "        if x.dim() == 1:\n",
    "            return self.internal_net(x[None])[0]\n",
    "        else:\n",
    "            return self.internal_net(x)\n",
    "\n",
    "# Here we instantiate our network.\n",
    "simple_oscillator_net = OscillatorNet()\n",
    "# And then instantiate the weights of the network itself\n",
    "def init_weights(m):\n",
    "    # For each layer type, we can define how we initialise its values\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        # A linear equation with a positive coefficient\n",
    "        # translates to exponential growth and a negative coefficient\n",
    "        # to exponential decay. In order to preserve stability we sample a matrix\n",
    "        # that is biased to be negative in its entries thus ensuring\n",
    "        # that our initial system is of exponential decay.\n",
    "        m.weight.data.normal_(0.0, 0.1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.normal_(0.0, 0.1)\n",
    "simple_oscillator_net.apply(init_weights)\n",
    "\n",
    "# `torch.autograd.Function`s track computation on all input tensors.\n",
    "# For that reason, we must pass our neural network parameters to the integrator,\n",
    "# which will pass it to the derivative function.\n",
    "# Since our network is stateful, we don't use these parameters, but define them in \n",
    "# the function signature.\n",
    "def sha_nn_fn(x, t, *nn_parameters):\n",
    "    return simple_oscillator_net(x, t)\n",
    "\n",
    "optimiser = torch.optim.Adam(simple_oscillator_net.parameters(), lr=1e-1, amsgrad=True)\n",
    "# Whenever the loss plateaus, we can reduce the learning rate to improve convergence\n",
    "lr_on_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser)\n",
    "\n",
    "def sha_closure(minibatch):\n",
    "    current_state = initial_state.detach().clone()\n",
    "    current_time  = initial_time.detach().clone()\n",
    "    optimiser.zero_grad()\n",
    "    error = 0.0\n",
    "    \n",
    "    times = minibatch['times']\n",
    "    states = minibatch['states']\n",
    "    \n",
    "    sorted_time_indices = torch.argsort(times)\n",
    "    times, states = times[sorted_time_indices], states[sorted_time_indices]\n",
    "    \n",
    "    for sample_state, sample_time in zip(states, times):\n",
    "        dt = torch.minimum(initial_timestep, sample_time - current_time).detach()\n",
    "        current_state, current_time, _, _, _ = adaptive_rk45_integrator.apply(sha_nn_fn, current_state, current_time, sample_time, dt, atol, rtol, *simple_oscillator_net.parameters())\n",
    "        error = error + torch.linalg.norm(sample_state - current_state)/times.shape[0]\n",
    "    if error.requires_grad:\n",
    "        error.backward()\n",
    "    return error\n",
    "\n",
    "ideal_matrix = neuralode.dynamics.get_simple_harmonic_oscillator_matrix(frequency, damping)\n",
    "ideal_bias = torch.zeros_like(initial_state)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Now we need an optimisation `loop` where we will take steps to minimise the error. \n",
    "# We don't know how many steps are required to \"train\" this network, so we start with a large number\n",
    "number_of_gd_steps = 1024\n",
    "\n",
    "best_error = torch.inf\n",
    "# For pytorch modules, the `state_dict` method allows us to get a copy\n",
    "# of all the parameters that define the model, thus enabling us to \n",
    "# store the state as well as restore it.\n",
    "best_parameters = simple_oscillator_net.state_dict()\n",
    "for step in range(number_of_gd_steps):\n",
    "    epoch_error = 0.0\n",
    "    shuffled_indices = torch.randperm(time_dataset.shape[0])\n",
    "    for batch_idx in range(0, time_dataset.shape[0], batch_size):\n",
    "        batch_dict = {\n",
    "            'times': time_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n",
    "            'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n",
    "        }\n",
    "    \n",
    "        step_error = optimiser.step(lambda: sha_closure(batch_dict))\n",
    "        epoch_error = epoch_error + step_error.item()*batch_dict['times'].shape[0]\n",
    "        # print(f\"[{step+1}/{number_of_gd_steps}]/[{batch_idx}/{time_dataset.shape[0]}] Batch Error: {step_error:.6f} \", end='\\r')\n",
    "    epoch_error = epoch_error/time_dataset.shape[0]\n",
    "    if epoch_error < best_error:\n",
    "        best_error = epoch_error\n",
    "        best_parameters = simple_oscillator_net.state_dict()\n",
    "    lr_on_plateau.step(epoch_error)\n",
    "    learned_matrix = simple_oscillator_net.state_dict()['internal_net.0.weight']\n",
    "    learned_bias = simple_oscillator_net.state_dict()['internal_net.0.bias']\n",
    "    # Ideally our matrix is equivalent to our simple harmonic oscillator matrix and our bias goes to zero\n",
    "    print(f\"[{step+1}/{number_of_gd_steps}] Epoch Error: {epoch_error:.6f}, \\nW={learned_matrix.cpu()}, \\nb={learned_bias.cpu()}\")\n",
    "    print()\n",
    "    # If the step size is too small, then we can interrupt the\n",
    "    # training as it will not lead to significant improvements\n",
    "    if lr_on_plateau.get_last_lr()[0] < 1e-6:\n",
    "        break\n",
    "    \n",
    "simple_oscillator_net.load_state_dict(best_parameters)\n",
    "learned_matrix = simple_oscillator_net.state_dict()['internal_net.0.weight']\n",
    "learned_bias = simple_oscillator_net.state_dict()['internal_net.0.bias']\n",
    "\n",
    "# Before we were looking at relative error, but in the case of a matrix with zeros,\n",
    "# the relative error is undefined, so we look at another common metric: mean absolute error\n",
    "print(f\"Best matrix: {learned_matrix}, mean absolute error: {torch.mean(torch.abs(ideal_matrix - learned_matrix)).item():.6f}\")\n",
    "print(f\"Best bias:   {learned_bias}, mean absolute error: {torch.mean(torch.abs(ideal_bias - learned_bias)).item():.6f}\")"
   ],
   "id": "c15be21129020cc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig_ref_position, axes_ref_position = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Position Ref.\")\n",
    "fig_ref_velocity, axes_ref_velocity = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_ref, sha_times_ref)], method_label=\"RK4(5) - SHA Velocity Ref.\")\n",
    "\n",
    "simple_oscillator_net.load_state_dict(best_parameters)\n",
    "_, _, sha_states_optimised, sha_times_optimised, _ = adaptive_rk45_integrator.apply(sha_nn_fn, initial_state, initial_time, final_time, initial_timestep, atol, rtol, best_frequency, best_damping)\n",
    "_ = neuralode.plot.trajectory.plot_trajectory([(i[0], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_position, method_label=\"RK4(5) - SHA Position Opt.\")\n",
    "_ = neuralode.plot.trajectory.plot_trajectory([(i[1], j) for i, j in zip(sha_states_optimised, sha_times_optimised)], axes=axes_ref_velocity, method_label=\"RK4(5) - SHA Velocity Opt.\")"
   ],
   "id": "cc56c1854444d2f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "And we can see that the neural network is able to effectively learn the dynamics of this system, but it would not extend to other systems with different frequency and damping as they would have a different matrix.\n",
    "\n",
    "In the coming notebooks, we will extend our network to learn the general dynamics by passing frequency and damping as a parameter. Further, we will learn more complex system dynamics and how to manipulate these systems."
   ],
   "id": "ad2af00588e44169"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
