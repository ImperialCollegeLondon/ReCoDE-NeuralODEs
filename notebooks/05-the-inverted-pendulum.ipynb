{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a43901d03f99741f",
   "metadata": {},
   "source": [
    "# The Inverted Pendulum (or the Cart-Pole System)\n",
    "\n",
    "In this notebook, we will go through implementing the inverted pendulum system for which we'll train a neural network to learn to control the pendulum state.\n",
    "\n",
    "A part of this exercise will also show you how to make animations in matplotlib!"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We restrict the number of PyTorch CPU threads to 1 as cpu training is\n",
    "# slower with multiple threads due to the small sizes of network we're dealing with\n",
    "%env OMP_NUM_THREADS = 1\n",
    "%env OPENBLAS_NUM_THREADS = 1\n",
    "%env MKL_NUM_THREADS = 1\n",
    "%env VECLIB_MAXIMUM_THREADS = 1\n",
    "%env NUMEXPR_NUM_THREADS = 1"
   ],
   "id": "6997cdf94bbe0064"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import neuralode"
   ],
   "id": "5c8fdae9a18cf750"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For convenience, we define the default tensor device and dtype here\n",
    "torch.set_default_device('cpu')\n",
    "# In neural networks, we prefer 32-bit/16-bit floats, but for precise integration, 64-bit is preferred. We will revisit this later when we need to mix integration with neural network training\n",
    "torch.set_default_dtype(torch.float64)"
   ],
   "id": "336b4bc9dd670908"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Implementing the Cart-Pole System",
   "id": "27c3a6b8ecd5adba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Dynamics\n",
    "\n",
    "The cart-pole system is a dynamical system comprising of a cart attached to a linear track with one-degree of freedom (the x-axis), and a rigid pole attach to the cart with one-degree of freedom (the rotational axis). While the equations can be complicated due to the interaction between the cart and the pole, the system itself is easy to imagine. Unlike the previous systems, we will not derive the equations of the system in this notebook as they require an understanding of force-balance equations, and instead quote them below[\\[1\\]](##references):\n",
    "\n",
    "$$\n",
    "\\theta^{(2)}=\n",
    "\\frac{g\\sin\\theta + \\cos\\theta\\left\\{\\frac{-F_p + d_N\\cos\\theta}{m_T} + g d_N\\right\\}-\\frac{\\mu_p\\theta^{(1)}}{m_p l}}{l\\left\\{\\frac{4}{3}-\\frac{m_p\\cos\\theta}{m_T}\\left[\\cos\\theta - d_N\\right]\\right\\}}\n",
    "$$\n",
    "and\n",
    "$$ x^{(2)} =\n",
    "\\frac{F_p - m_p l \\theta^{(2)}\\cos\\theta - N_c d_N}{m_T}\n",
    "$$\n",
    "where \n",
    "\n",
    "$$\n",
    "m_T = m_c + m_p\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_c=m_T g-m_p l \\left(\\theta^{(2)}\\sin\\theta +\\left[\\theta^{(1)}\\right]^2\\cos\\theta\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_N = \\mu_c\\mathrm{sgn}\\left(N_c x^{(1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_p = F + m_p l \\left[\\theta^{(1)}\\right]^2\\sin\\theta\n",
    "$$\n",
    "If we neglect friction (i.e. $\\mu_p = \\mu_c = 0$) then this simplifies to:\n",
    "\n",
    "$$\n",
    "\\theta^{(2)}=\n",
    "\\frac{g\\sin\\theta + \\cos\\theta\\frac{-F_p}{m_T}}{l\\left\\{\\frac{4}{3}-\\frac{m_p\\cos^2\\theta}{m_T}\\right\\}}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "x^{(2)} = \\frac{F_p - m_p l \\theta^{(2)}\\cos\\theta}{m_T}\n",
    "$$\n",
    "\n",
    "Using the same transformation as before, we can write this as a first-order Ordinary Differential Equation by introducing $\\omega=\\theta^{(1)}$ and $v = x^{(1)}$ as variables and denoting the state $\\vec{x}=\\left[\\theta, \\omega, x, v\\right]$, we get the dynamical equations\n",
    "$$\n",
    "\\vec{x}^{(1)}=\n",
    "\\begin{bmatrix}\n",
    "\\omega \\\\\n",
    "\\frac{g\\sin\\theta + \\cos\\theta\\left\\{\\frac{-F_p + d_N\\cos\\theta}{m_T} + g d_N\\right\\}-\\frac{\\mu_p\\theta^{(1)}}{m_p l}}{l\\left\\{\\frac{4}{3}-\\frac{m_p\\cos\\theta}{m_T}\\left[\\cos\\theta - d_N\\right]\\right\\}} \\\\\n",
    "v \\\\\n",
    "\\frac{F_p - m_p l \\theta^{(2)}\\cos\\theta - N_c d_N}{m_T}\n",
    "\\end{bmatrix}\n",
    "$$.\n",
    "\n",
    "You'll note that the right-hand side of the equation depends on $\\theta^{(2)}$ through $N_c$ which is also on the left-hand side of our equation. One way to solve this would be to use a non-linear root-finding algorithm to find the exact value of $\\theta^{(2)}$ that satisfies this equation, but this would be computationally expensive. If you look closely, $N_c$ is the normal force applied by the track on the cart and for $\\theta^{(2)}$ we only use its sign to determine the acceleration of the pole. In most cases, because the cart is attached to the track and weighed down, $N_c$ will be positive so we can assume that it is positive and compute $\\theta^{(2)}$ accordingly. Once we've computed $\\theta^{(2)}$, we can check that $N_c$ is still positive. If it isn't, we can recompute $\\theta^{(2)}$. Because in most cases it will be positive, this incurs minimal computational cost in comparison to using a root-finding scheme.\n",
    "\n",
    "The implementation itself can be found in `neuralode.dynamics.inverted_pendulum`."
   ],
   "id": "851fb0d797fabda6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### The System Parameters\n",
    "mass_cart   = torch.tensor(1.0)  # kg\n",
    "mass_pole   = torch.tensor(0.01)  # kg\n",
    "length_pole = torch.tensor(1.0)  # m\n",
    "gravity     = torch.tensor(9.81) # m/s^2\n",
    "friction_cart = torch.tensor(0.0)\n",
    "friction_pole = torch.tensor(0.0)"
   ],
   "id": "dc785af04e170b22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "initial_state = torch.tensor([0.0,-1.0,0.0,0.1])\n",
    "\n",
    "initial_time = torch.tensor(0.0)\n",
    "final_time   = torch.tensor(5.0)\n",
    "\n",
    "initial_timestep = torch.tensor(5e-4)\n",
    "\n",
    "current_integrator = neuralode.integrators.AdaptiveRKV87Integrator\n",
    "\n",
    "atol = rtol = torch.tensor(torch.finfo(initial_state.dtype).eps**0.5)\n",
    "common_integrator_parameters = [mass_cart, mass_pole, length_pole, gravity, friction_cart, friction_pole]"
   ],
   "id": "e3dc038c1aae6d8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "final_state, _, pendulum_states, pendulum_times, estimated_errors = current_integrator.apply(neuralode.dynamics.inverted_pendulum, initial_state, initial_time, final_time, initial_timestep, {'atol': atol, 'rtol': rtol}, torch.tensor(0.0), *common_integrator_parameters)",
   "id": "5badc629b9df8512"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pendulum_x = torch.sin(pendulum_states[...,0]) * length_pole + pendulum_states[...,2]\n",
    "pendulum_y = torch.cos(pendulum_states[...,0]) * length_pole\n",
    "\n",
    "cart_x, cart_y = pendulum_states[...,2], torch.zeros_like(pendulum_states[...,2])\n",
    "\n",
    "neuralode.plot.inverted_pendulum.plot_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, pendulum_times, torch.zeros_like(pendulum_times))\n",
    "HTML(neuralode.plot.inverted_pendulum.animate_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, pendulum_times, frame_time=1000/60))"
   ],
   "id": "f7422bd732a2a72a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training a Network",
   "id": "6b83a0b5aa85441c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Model\n",
    "First we define the model. We've left the definition in the module if you'd like to look at the implementation, but this is a simple Multi-Layer Perceptron (MLP) model with a default of 6 layers (4 hidden, 1 input, 1 output) using a non-linear Sinusoidal activation function. Unlike many other MLPs, we've opted to use Sine activations due to two desirable properties that assists in the training of the network:\n",
    "\n",
    "1. Sinusoids are $C^1$-continuous (the $1^{st}$ derivative is continuous everywhere)\n",
    "2. Sinusoids do not saturate and die off (i.e. there are only a discrete set of points for which $\\frac{\\mathrm{d}}{\\mathrm{d}x}\\sin(x)=0$).\n",
    "    \n",
    "The first property is desirable because, during the adjoint integration, we are integrating the gradient of the network and discontinuities in the activation function naturally lead to jumps in the adjoint that are difficult to integrate over using explicit integration techniques.\n",
    "\n",
    "The second property is desirable because our inputs have large magnitudes and have periodic properties, and while we could use normalisation, we'd be removing the periodicity. Instead, sinusoids allow the network to learn periodic control representations that are well suited for a dynamical system such as this.\n",
    "\n",
    "We have also restricted the maximum force applied by the network to $80\\mathrm{N}$ as any real controller would be limited in the amount of force it can apply."
   ],
   "id": "b3ff6660f815081d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cart_pole_net = neuralode.models.inverted_pendulum.CartPoleNet()\n",
    "cart_pole_net.apply(neuralode.models.util.init_weights)\n",
    "\n",
    "def nn_controlled_pendulum(state, time, controller, mc, mp, length, g, mu_c, mu_p, *args):\n",
    "    force = controller(state, time)[...,0]\n",
    "    return neuralode.dynamics.inverted_pendulum(state, time, force, mc, mp, length, g, mu_c, mu_p)"
   ],
   "id": "a919d3cec2dbb259"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Dataset\n",
    "We want our network to learn how to balance the pole over the cart and thus we generate a dataset of nearly balanced poles (within $22.5^\\mathrm{o}$ of the vertical position) in various angular velocity, cart position and cart velocity configurations for the network to learn from.\n",
    "\n",
    "We also add examples, equal to $3.125\\%$ of our nearly balanced examples, for which the pole is hanging below the cart so that the network can learn to swing the pole up and over the cart before balancing it."
   ],
   "id": "5d8c1443275a67a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# As the goal is balancing the pole on top of the cart, we generate initial states where the pole\n",
    "# is near vertical and moving with some random rotational velocity. This way, the neural network observes\n",
    "# multiple states near the vertical state where it needs to learn to counter-balance it.\n",
    "num_examples = 2048\n",
    "state_min = torch.tensor([-0.125*torch.pi, -1.0*torch.pi, -10.0, -1.0])\n",
    "state_max = torch.tensor([ 0.125*torch.pi,  1.0*torch.pi,  10.0,  1.0])\n",
    "state_dataset = torch.rand(num_examples, *state_max.shape) * (state_max - state_min)[None] + state_min[None]\n",
    "\n",
    "frac_below = 0.03125\n",
    "state_min = torch.tensor([-0.5*torch.pi+torch.pi, -1.0*torch.pi, 0.0, 0.0])\n",
    "state_max = torch.tensor([ 0.5*torch.pi+torch.pi,  1.0*torch.pi, 0.0, 0.0])\n",
    "state_dataset = torch.cat([\n",
    "    state_dataset, \n",
    "    torch.rand(math.ceil(num_examples*frac_below), *state_max.shape) * (state_max - state_min)[None] + state_min[None]\n",
    "], dim=0)"
   ],
   "id": "d3ab782e886655fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loss Function\n",
    "\n",
    "Ideally the network balances the pole above the cart and this is equivalent to having the pole angle equal zero at all times. To that end, we use an loss of the angular deviation from zero and integrate it alongside the dynamics. This gives us a measure of the angular deviation over time and is a quantity that we would like to minimise."
   ],
   "id": "63a40d2b7603364d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# As our network needs to minimize the angle across all timesteps,\n",
    "# we can integrate our loss function alongside the system and then minimize its value at the end.\n",
    "def loss_func(x0):\n",
    "    return x0[...,:1].square().sum(dim=-1, keepdim=True)\n",
    "\n",
    "# We append one more state variable that will track our loss.\n",
    "# Initially it is set to zero as our initial state is\n",
    "# outside the control of our neural network\n",
    "state_dataset = torch.cat([\n",
    "    state_dataset[...,:initial_state.shape[0]],\n",
    "    torch.zeros_like(state_dataset[...,:1]),\n",
    "], dim=-1)\n",
    "\n",
    "\n",
    "def loss_augmented_rhs(x0, t, *args):\n",
    "    return torch.cat([\n",
    "        nn_controlled_pendulum(x0[...,:4], t, cart_pole_net, *args),\n",
    "        loss_func(x0[...,:4])\n",
    "    ], dim=-1)\n",
    "\n",
    "training_integrator_kwargs = {\n",
    "    'atol': atol,\n",
    "    'rtol': rtol\n",
    "}\n",
    "testing_integrator_kwargs = {\n",
    "    'atol': atol,\n",
    "    'rtol': rtol\n",
    "}\n",
    "\n",
    "def batched_integrator(x0, integration_duration):\n",
    "    return current_integrator.apply(loss_augmented_rhs, x0, initial_time, initial_time+integration_duration, initial_timestep, training_integrator_kwargs, *common_integrator_parameters, *cart_pole_net.parameters())"
   ],
   "id": "19e267f5a6ea6c56"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Configuration\n",
    "We select a batch size of 32 and train for 256 epochs. Unlike our previous networks, the learning rate is set very low as a higher learning rate leads to divergence.\n",
    "\n",
    "For faster convergence, we use Adam with Nesterov momentum and for regularisation, we use weight-decay of ${10}^{-4}$.\n",
    "\n",
    "The closure function we have accepts a variable integration duration and thus we have set it so that the returned error is invariant to this. Thus the loss function we're minimising is the maximum deviation of the final state and the rate of the integrated error (as opposed to the integrated error itself)."
   ],
   "id": "42a520ae0b994845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 128\n",
    "number_of_epochs = 256\n",
    "number_of_batches = math.ceil(state_dataset.shape[0] / batch_size)\n",
    "total_steps = number_of_batches * number_of_epochs\n",
    "\n",
    "optimiser = torch.optim.AdamW(cart_pole_net.parameters(), lr=1e-6, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimiser,\n",
    "    base_lr=1e-6,\n",
    "    max_lr=4e-5,\n",
    "    step_size_up=number_of_batches//2,\n",
    "    step_size_down=number_of_batches - number_of_batches//2,\n",
    "    mode='exp_range'\n",
    ")\n",
    "\n",
    "def pendulum_closure(minibatch, integration_duration):\n",
    "    optimiser.zero_grad()\n",
    "    \n",
    "    states = minibatch['states']\n",
    "    \n",
    "    final_state, *_ = batched_integrator(states, integration_duration)\n",
    "\n",
    "    # The error is simply the integrated error over the whole trajectory\n",
    "    ## The integrated error is divided by the integration duration to give a time invariant error\n",
    "    error = final_state[...,:4].square().sum(dim=-1).mean() + final_state[...,-1].mean()/integration_duration\n",
    "    \n",
    "    if error.requires_grad:\n",
    "        error.backward()\n",
    "    return error"
   ],
   "id": "bef4d6ab3228919a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the Network Incrementally\n",
    "\n",
    "While we could have the network tackle the full problem where it tries to balance the pole over many seconds, this is a very difficult problem.\n",
    "Instead, we train the network on a set of random states to minimise the error over $0.125$ seconds. Whenever the error decreases, we increase the time interval by $0.0625$s up to a maximum of $16.125$s (if all steps decrease the error which is unlikely).\n",
    "\n",
    "The idea behind incremental training of the network is that the full problem over many seconds is simply a sequence of simpler problems divided into time increments. If the network can successfully move the pendulum closer to the desired state, then a sequence of these increments can, ideally, reach the desired state, however many increments it takes. But since some initial states may require more complex maneuvers such as swinging the pole up and over the cart, we increase the interval of integration each time the network improves on the smaller problem."
   ],
   "id": "19aa2bf88b467c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "best_error = torch.inf\n",
    "best_params = copy.deepcopy(cart_pole_net.state_dict())\n",
    "\n",
    "cart_pole_net.train()\n",
    "\n",
    "integration_time = torch.tensor(0.125)\n",
    "\n",
    "for step in range(number_of_epochs):\n",
    "    epoch_error = 0.0\n",
    "    shuffled_indices = torch.randperm(state_dataset.shape[0])\n",
    "    for batch_idx in range(0, state_dataset.shape[0], batch_size):\n",
    "        batch_dict = {\n",
    "            'states': state_dataset[shuffled_indices][batch_idx:batch_idx+batch_size],\n",
    "        }\n",
    "    \n",
    "        step_error = optimiser.step(lambda: pendulum_closure(batch_dict, integration_time))\n",
    "        scheduler.step()\n",
    "        if any(torch.any(~torch.isfinite(i)) for i in cart_pole_net.parameters()) or any(torch.any(~torch.isfinite(i.grad)) for i in cart_pole_net.parameters() if i.requires_grad):\n",
    "            raise ValueError(\"Encountered non-finite parameters/parameters gradients\")      \n",
    "        epoch_error = epoch_error + step_error.item()*batch_dict['states'].shape[0]\n",
    "        print(f\"[{step+1}/{number_of_epochs} - {optimiser.param_groups[0]['lr']:.4e} - horizon: {integration_time:.5f}]/[{batch_idx}/{state_dataset.shape[0]}] Batch Error: {step_error:.6f}\", end='\\r')\n",
    "    epoch_error = epoch_error/state_dataset.shape[0]\n",
    "    print(\" \"*128, end=\"\\r\")\n",
    "    print(f\"[{step+1}/{number_of_epochs} - {optimiser.param_groups[0]['lr']:.4e} - horizon: {integration_time:.5f}] Epoch Error: {epoch_error:.6f}\")\n",
    "    if epoch_error < best_error:\n",
    "        if best_error < torch.inf:\n",
    "            integration_time = integration_time + torch.tensor(0.0625)\n",
    "        best_error = epoch_error\n",
    "        best_params = copy.deepcopy(cart_pole_net.state_dict())\n",
    "    torch.cuda.empty_cache()"
   ],
   "id": "bf41e30785862e2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "We can see from the plots below that the network successfully learns to balance an upturned pole over $25$s despite only being trained over periods less than $1.5$s in total. While the network hasn't truly minimised the error in that the cart is offset from the origin, the pole is stable above the cart which is the primary goal of the training."
   ],
   "id": "b0146a62c71cdda1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_controller(test_state, test_duration, controller, hide_static_plot=False):\n",
    "    final_state, _, controlled_pendulum_states, controlled_pendulum_times, errors_achieved = current_integrator.apply(nn_controlled_pendulum, test_state, initial_time, initial_time+test_duration, initial_timestep, testing_integrator_kwargs, controller, *common_integrator_parameters)\n",
    "    cart_x = controlled_pendulum_states[...,2]\n",
    "    cart_y = torch.zeros_like(cart_x)\n",
    "    \n",
    "    pendulum_x = torch.sin(controlled_pendulum_states[...,0]) * length_pole + cart_x\n",
    "    pendulum_y = torch.cos(controlled_pendulum_states[...,0]) * length_pole\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        controller_force = controller(controlled_pendulum_states, controlled_pendulum_times)[...,0]\n",
    "    if not hide_static_plot:\n",
    "        neuralode.plot.inverted_pendulum.plot_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, controlled_pendulum_times, controller_force)\n",
    "    return HTML(neuralode.plot.inverted_pendulum.animate_pendulum(pendulum_x, pendulum_y, cart_x, cart_y, controlled_pendulum_times, forces=controller_force, frame_time=1000/60))"
   ],
   "id": "92420cb74f185457"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cart_pole_net.eval()\n",
    "cart_pole_net.load_state_dict(best_params)\n",
    "\n",
    "display(test_controller(initial_state, 25.0, cart_pole_net, hide_static_plot=True))\n",
    "display(test_controller(state_dataset[0,...,:-1], 25.0, cart_pole_net, hide_static_plot=True))\n",
    "display(test_controller(state_dataset[1,...,:-1], 25.0, cart_pole_net, hide_static_plot=True))\n",
    "display(test_controller(state_dataset[-1,...,:-1], 25.0, cart_pole_net, hide_static_plot=True))\n",
    "display(test_controller(state_dataset[-2,...,:-1], 25.0, cart_pole_net, hide_static_plot=True))"
   ],
   "id": "5afae848feada2db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Post-Training Notes\n",
    "\n",
    "From this, we can see that the network learns how to balance the pole vertically, but struggles to learn how to flip the pole up and over the cart when it's hanging below. Furthermore, the controller drives the cart far from the origin at high speed and this is partly due to the fact that the dataset and the controlled cart never go further than several tens of meters from the origin and thus the network has no experience with this region. This is one of the difficulties of adjoint-based methods, where, unlike reinforcement learning which explicitly controls exploitation vs exploration, we're limited to the signal provided by the gradients of the dynamics which can suffer from local minima and instability.\n",
    "\n",
    "Some solutions could be to include a different position encoding where rather than passing the position and velocity directly to the network we use a sum of sinusoids to encode the information, enforcing some kind of translational invariance. The downside of these methods is that the loss function is not translationally invariant and so these encodings would be at odds with the objective of the training. Another alternative is to cap the magnitude of the distance, $x$ passed to the network so that it remains within the domain of the dataset it was trained on.\n",
    "\n",
    "Neural networks are fickle to train, the above parameters that led to a successfully trained network took quite a lot of guided trial-and-error, especially in terms of picking an appropriate learning rate and learning schedule, and dataset sizes. It is absolutely worth exploring these parameters to see their effects on the training, and try to find ways to improve the convergence of the network. Some ideas to experiment with are weighting the integrated error term differently, dynamically adjusting the weight of the integrated error term based on the integration duration, or changing its form from $\\int_{t_0}^{t_1}\\omega\\mathrm{d}t$ to $\\int_{t_0}^{t_1}\\omega t\\mathrm{d}t$ so that deviations from the target angle of $\\omega=0$ at later times have greater weight than earlier times."
   ],
   "id": "559ad600d9db0e0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# References\n",
    "\\[1\\]: https://coneural.org/florian/papers/05_cart_pole.pdf"
   ],
   "id": "c069dc5cb048f224"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
